[
    {
        "question": "Who is the lecturer for the module discussed in the text?",
        "answer": "Eric Atwell"
    },
    {
        "question": "What is the weightage of the group project report in the overall assessment for the course?",
        "answer": "60%"
    },
    {
        "question": "What are some of the reasons mentioned for developing text analytics?",
        "answer": "Some reasons mentioned for developing text analytics include computer models of language, computerised language resources, natural communication between people and computers, assisting communication between people, and wealth creation."
    },
    {
        "question": "What are some reasons for developing text analytics?",
        "answer": "Some reasons for developing text analytics include computer models of language, computerised language resources, natural communication between people and computers, assisting communication between people, and wealth creation."
    },
    {
        "question": "What are some examples of real applications mentioned in the text?",
        "answer": "Examples include soldiers in Bosnia using a small computer to give orders, text editing tools in word processors, machine translation services on the Internet, and a flight information service called ALF by Lufthansa."
    },
    {
        "question": "What is the purpose of using leading (and trailing) wildcards in a keyword search within a traditional relational database?",
        "answer": "The purpose is to perform a 'brute force scan' to match a condition with the text data records."
    },
    {
        "question": "What is the purpose of an inverted file structure in information retrieval systems?",
        "answer": "The purpose of an inverted file structure is to store the list of terms used in the whole collection of documents and for each term point to the list of documents that are indexed by the term."
    },
    {
        "question": "What is the purpose of the 'Inverted file structure' in the given text?",
        "answer": "The purpose of the 'Inverted file structure' is to store terms along with the documents they appear in and the frequency of their occurrence."
    },
    {
        "question": "What is the purpose of an inverted file in information retrieval?",
        "answer": "The purpose of an inverted file in information retrieval is to store a list of pointers into the data file (or object-ids, or URLs) that identify the objects indexed by the dictionary term."
    },
    {
        "question": "What is the Boolean query mentioned in the text?",
        "answer": "(A or B) and C"
    },
    {
        "question": "What is the Boolean query mentioned in the text?",
        "answer": "(A or B) and C"
    },
    {
        "question": "What is the disjunctive normal form of the Boolean query (A or B) and C?",
        "answer": "(1, 0, 1) OR (0, 1, 1) OR (1, 1, 1)"
    },
    {
        "question": "How many hits were reported to the user in the given text?",
        "answer": "4"
    },
    {
        "question": "What is the purpose of using inverted file structures in a search engine system?",
        "answer": "The purpose of using inverted file structures in a search engine system is to provide suitable indexing for search on sets of index terms, allow for vector model queries by storing frequencies/weights, and enable proximity queries by storing positional information."
    },
    {
        "question": "Why are inverted file structures considered suitable for 'search-engine' type queries?",
        "answer": "Inverted file structures are purpose-made for 'search-engine' type queries because they allow for storing frequencies/weights in the dictionary and inverted file, enabling vector model queries, as well as storing positional information for proximity queries."
    },
    {
        "question": "What is the purpose of the Inverted file structure in Information Retrieval (IR)?",
        "answer": "The purpose of the Inverted file structure in Information Retrieval (IR) is to represent a document by a set of 'descriptors' or 'index terms' and to search for documents mainly in the 'space' of index terms."
    },
    {
        "question": "What is the result of the cosine similarity calculation between the query vector q = (1.0, 0.6, 0.0, 0.0, 0.8) and the document vector d1 = (0.8, 0.8, 0.0, 0.0, 0.2) representing 'Jam pud recipe'?",
        "answer": "The result of the cosine similarity calculation is 1.32."
    },
    {
        "question": "What are some issues that need to be resolved in information retrieval systems?",
        "answer": "Some issues that need to be resolved in information retrieval systems include synonyms, homonyms, local/global contexts determining 'good' terms, precoordination (proximity query) for multi-word terms, and evaluation/effectiveness measures."
    },
    {
        "question": "What are some of the evaluation measures used in information retrieval?",
        "answer": "Some evaluation measures used in information retrieval include effort required by users in formulating queries, time between receipt of user query and production of list of 'hits', presentation of the output, coverage of the collection, recall, precision, and user satisfaction with the retrieved items."
    },
    {
        "question": "What are the two methods mentioned for query broadening in the text?",
        "answer": "The two methods mentioned for query broadening are using feedback information from the user and using a thesaurus/term-bank/ontology."
    },
    {
        "question": "What is the purpose of relevance feedback in information retrieval systems?",
        "answer": "The purpose of relevance feedback is to improve search results by adjusting the query based on the relevance of retrieved documents, moving the query vector closer to relevant document vectors and further from non-relevant document vectors."
    },
    {
        "question": "What are some components that a thesaurus or ontology may contain?",
        "answer": "A thesaurus or ontology may contain controlled vocabulary of terms or phrases describing a specific restricted topic, synonym classes, hierarchy defining broader terms (hypernyms) and narrower terms (hyponyms), and classes of 'related' terms."
    },
    {
        "question": "How can precision and recall be improved in information retrieval by using a thesaurus?",
        "answer": "Precision and recall in information retrieval can be improved by replacing words from documents and query words with synonyms from a controlled language using a thesaurus."
    },
    {
        "question": "What is the NLTK Natural Language Toolkit used for?",
        "answer": "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
    },
    {
        "question": "What is the purpose of NLTK according to the text?",
        "answer": "NLTK is a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation."
    },
    {
        "question": "What is the guiding principle mentioned in the text?",
        "answer": "The guiding principle mentioned in the text is 'get others to do the work'."
    },
    {
        "question": "What was the main approach used in the 'cheat.py' program described in the text?",
        "answer": "The main approach used in the 'cheat.py' program was unsupervised learning by reading outputs of other systems, selecting majority-vote analysis, and choosing the result from the best system in case of a tie."
    },
    {
        "question": "What is the difference between Information Extraction (IE) and Information Retrieval (IR) as explained in the text?",
        "answer": "IR pulls documents from large text collections in response to specific keywords or queries, while IE pulls facts and structured information (Named Entities and Relations) from the content of large text collections."
    },
    {
        "question": "What is Named Entity Recognition (NER) and why is it important?",
        "answer": "NER is the identification of proper names in texts and their classification into predefined categories like persons, organizations, locations, etc. It is important because it provides a foundation for building more complex Information Extraction (IE) systems, helps in tracking relationships between named entities, and supports ontological information and scenario building."
    },
    {
        "question": "What are the advantages and disadvantages of the list lookup approach in Named Entity Recognition?",
        "answer": "Advantages of the list lookup approach include being simple, fast, language independent, and easy to retarget. Disadvantages include the need for collection and maintenance of lists, inability to deal with name variants, and inability to resolve ambiguity."
    },
    {
        "question": "What is the purpose of using context-based patterns in the shallow parsing approach?",
        "answer": "The purpose of using context-based patterns is to help identify ambiguous cases and correctly associate entities with their context."
    },
    {
        "question": "What is one of the challenges mentioned in adapting MUSE to different languages?",
        "answer": "One of the challenges mentioned is the extensive support needed for non-Latin scripts and text encodings, including conversion utilities."
    },
    {
        "question": "What is the main focus of the GATE Unicode Kit (GUK)?",
        "answer": "The main focus of the GATE Unicode Kit (GUK) is Information Extraction, specifically extracting Named Entities and Relations from text."
    },
    {
        "question": "What does BERT stand for and what is its purpose?",
        "answer": "BERT stands for Bidirectional Encoder Representations from Transformers and its purpose is to provide pre-trained representations from unlabeled text, which can then be fine-tuned for various natural language processing tasks."
    },
    {
        "question": "What is the approach used by BERT Transformer NN for unsupervised learning of morphology, syntax, and semantics?",
        "answer": "BERT Transformer NN maps pairs of texts onto a class by training on many examples of sentence pairs and predicting Yes or No for new sentence pairs."
    },
    {
        "question": "What task does BERT pre-train for in the context of sentence prediction?",
        "answer": "BERT pre-trains for a next sentence prediction (NSP) task."
    },
    {
        "question": "According to the text, which model significantly outperforms the other across all tasks?",
        "answer": "BERTLARGE"
    },
    {
        "question": "What is the main advantage of using BERT according to the results mentioned in the text?",
        "answer": "BERT outperforms all systems on all tasks by a substantial margin and can successfully tackle a broad set of NLP tasks."
    },
    {
        "question": "What are some of the challenges faced in Machine Translation according to the text?",
        "answer": "Some of the challenges in Machine Translation include complex orthography, lexical ambiguity, morphological complexity and variation, tokenization issues, and translation divergences."
    },
    {
        "question": "What is the purpose of a parallel corpus in machine translation?",
        "answer": "A parallel corpus is needed for machine translation to have source and target sentences aligned for training and evaluation."
    },
    {
        "question": "What is the purpose of the GIZA++ toolkit in machine translation?",
        "answer": "The GIZA++ toolkit is used to train word alignments in statistical machine translation."
    },
    {
        "question": "Who did not slap the green witch?",
        "answer": "Mary"
    },
    {
        "question": "What are some advantages of Phrase-Based SMT mentioned in the text?",
        "answer": "Some advantages of Phrase-Based SMT mentioned in the text include handling non-compositional phrases, using local context for disambiguation, learning longer phrases with more data, and sometimes even whole sentences."
    },
    {
        "question": "What is the purpose of the Bleu Metric in machine translation evaluation?",
        "answer": "The purpose of the Bleu Metric in machine translation evaluation is to compare machine translation output against several human translations to give a standardized score that correlates highly with human evaluation."
    },
    {
        "question": "What are some examples of social media communities where Artificial Intelligence experts can join to network and share knowledge?",
        "answer": "Some examples of social media communities where Artificial Intelligence experts can join include Facebook, LinkedIn, Quora, KDnuggets, Kaggle, Weka, ICAME, ACL, SemEval, and EU-JRC communities."
    },
    {
        "question": "What is the purpose of Quora according to the text?",
        "answer": "Quora is a platform to ask questions and connect with people who contribute unique insights and quality answers."
    },
    {
        "question": "What is the purpose of the ICAME conference mentioned in the text?",
        "answer": "The purpose of the ICAME conference is to bring together researchers and professionals in the field of English text corpora to discuss and exchange ideas."
    },
    {
        "question": "What is the purpose of the EU-JRC fund projects mentioned in the text?",
        "answer": "The EU-JRC fund projects to foster EU research collaboration."
    },
    {
        "question": "What is the purpose of sentiment analysis mentioned in the text?",
        "answer": "To determine whether a review is positive or negative, understand public sentiment, predict election outcomes or market trends, etc."
    },
    {
        "question": "What is the focus of sentiment analysis according to the text?",
        "answer": "The focus of sentiment analysis is the detection of attitudes."
    },
    {
        "question": "What are some examples of tasks that can be accomplished through text classification?",
        "answer": "Some examples of tasks that can be accomplished through text classification include sentiment analysis, spam detection, authorship identification, language identification, and assigning subject categories, topics, or genres."
    },
    {
        "question": "What is the main idea behind the Bag of Words representation in the Naive Bayes Classifier?",
        "answer": "The Bag of Words representation is a simple classification method based on Bayes rule that relies on a very simple representation of a document, treating it as a collection of words without considering their order."
    },
    {
        "question": "What is the approach used for parameter estimation in the Naive Bayes model?",
        "answer": "The approach used for parameter estimation in the Naive Bayes model is maximum likelihood estimates, where the frequencies in the data are simply used."
    },
    {
        "question": "What are stop words and how are they typically treated in text classification systems?",
        "answer": "Stop words are very frequent words like 'the' and 'a'. In some systems, stop words are ignored by sorting the vocabulary by word frequency in the training set and calling the top 10 or 50 words the stopword list. These stop words are then removed from both the training and test sets."
    },
    {
        "question": "What method is suggested for dealing with negation in sentiment classification?",
        "answer": "Adding NOT_ to every word between negation and following punctuation"
    },
    {
        "question": "What is the purpose of using lexicons in sentiment classification?",
        "answer": "The purpose of using lexicons in sentiment classification is to add a feature that counts whenever a word from the lexicon occurs, such as positive or negative words, to improve sentiment analysis."
    },
    {
        "question": "What are some advantages of using Naive Bayes for text classification?",
        "answer": "Some advantages of using Naive Bayes for text classification include being very fast, having low storage requirements, working well with small amounts of training data, being robust to irrelevant features, and being a good dependable baseline for text classification."
    },
    {
        "question": "Why do we use precision and recall instead of accuracy in text classification tasks?",
        "answer": "Precision and recall emphasize true positives, finding the things that we are supposed to be looking for, unlike accuracy which may be skewed by imbalanced datasets."
    },
    {
        "question": "What is the purpose of Precision and recall in text classification?",
        "answer": "Precision and recall emphasize true positives, finding the things that we are supposed to be looking for."
    },
    {
        "question": "What are some of the causes of harms in sentiment and toxicity classifiers, as mentioned in the text?",
        "answer": "Some causes of harms in sentiment and toxicity classifiers include problems in the training data, human labels, resources used, and model architecture."
    },
    {
        "question": "What is the goal of probabilistic language modeling?",
        "answer": "The goal of probabilistic language modeling is to assign a probability to a sentence."
    },
    {
        "question": "What is the Markov Assumption in the context of language modeling?",
        "answer": "The Markov Assumption is a simplifying assumption where each component in the product is approximated by considering only the previous word or a small window of previous words."
    },
    {
        "question": "What is the probability of the sentence 'I want english food' based on the given bigram estimates?",
        "answer": ".000031"
    },
    {
        "question": "What is the purpose of perplexity in language modeling?",
        "answer": "Perplexity is used to evaluate how well a language model predicts the next word in a sequence of words. It is a measure of how well the model assigns probabilities to the words in the test set, normalized by the number of words."
    },
    {
        "question": "What is the purpose of minimizing perplexity in language modeling?",
        "answer": "The purpose of minimizing perplexity is to maximize the probability of a language model and to best predict an unseen test set, ultimately leading to a better model."
    },
    {
        "question": "Why is it important to train robust models that generalize in language modeling?",
        "answer": "It is important to train robust models that generalize in language modeling because N-grams only work well for word prediction if the test corpus looks like the training corpus. In real life, this is often not the case, so robust models that can generalize are needed."
    },
    {
        "question": "What is the purpose of Add-one (Laplace) smoothing in language modeling?",
        "answer": "The purpose of Add-one (Laplace) smoothing is to steal probability mass to generalize better when dealing with sparse statistics."
    },
    {
        "question": "What is the purpose of add-1 estimation in N-gram language models?",
        "answer": "Add-1 estimation is used to smooth some NLP models and is not used for state-of-the-art N-gram language models."
    },
    {
        "question": "What technique is recommended for dealing with unknown words in language modeling tasks?",
        "answer": "Instead of creating a closed vocabulary task, it is recommended to create an unknown word token <UNK> and train its probabilities like a normal word."
    },
    {
        "question": "What are two types of chatbots mentioned in the text?",
        "answer": "Conversational agents and Task-based Dialogue Agents"
    },
    {
        "question": "What term is used to describe each contribution in a conversation?",
        "answer": "Turn"
    },
    {
        "question": "Why is grounding important in conversations?",
        "answer": "Grounding is important in conversations to acknowledge that the hearer has understood the speaker's utterances."
    },
    {
        "question": "What is the term used to describe conversations where one person controls the flow of the conversation by asking questions and the other person responds?",
        "answer": "Conversational initiative"
    },
    {
        "question": "What technique did ELIZA use to engage in conversation with users?",
        "answer": "ELIZA used the technique of reflecting the user's statements back at them, similar to a Rogerian psychologist."
    },
    {
        "question": "In the conversation between the patient and the psychologist, what approach does the psychologist take when the patient mentions going for a long boat ride?",
        "answer": "The psychologist asks the patient to tell them about boats, assuming the patient had some conversational goal rather than assuming she didn't know what a boat is."
    },
    {
        "question": "What ethical implications were raised regarding the storage of conversations with ELIZA?",
        "answer": "People pointed out the privacy implications and suggested that they were having quite private conversations with ELIZA, despite knowing that it was just software."
    },
    {
        "question": "What was the first system to pass a version of the Turing test?",
        "answer": "PARRY"
    },
    {
        "question": "What are the two architectures for corpus-based chatbots?",
        "answer": "Response by retrieval and response by generation"
    },
    {
        "question": "According to the text, what is the ongoing research problem with neural chatbots?",
        "answer": "Neural chatbots can get repetitive and boring"
    },
    {
        "question": "What are some pros and cons of chatbots mentioned in the text?",
        "answer": "Some pros of chatbots include being fun and good for narrow, scriptable applications. Some cons include not really understanding and giving the appearance of understanding, which may be problematic. Rule-based chatbots are expensive and brittle, while IR-based chatbots can only mirror training data."
    },
    {
        "question": "How are chatbots and task-based dialogue systems evaluated according to the text?",
        "answer": "Chatbots are evaluated by humans through participant evaluation and observer evaluation. Participant evaluation involves a human chatting with the model and rating dimensions of quality, while observer evaluation involves a third party reading a transcript of a conversation and assigning a score."
    },
    {
        "question": "What is one current research direction mentioned in the text for evaluating dialogue systems?",
        "answer": "Adversarial Evaluation, inspired by the Turing Test, where a 'Turing-like' classifier is trained to distinguish between human responses and machine responses."
    },
    {
        "question": "What are some of the ethical issues mentioned in the text related to artificial agents?",
        "answer": "Some of the ethical issues mentioned include safety concerns, representational harm, privacy issues, bias in training datasets, and the importance of considering user responses in the design phase."
    },
    {
        "question": "What are some examples of rule-based chatbots mentioned in the text?",
        "answer": "ELIZA, PARRY, AIML, ALICE, HUBERT"
    },
    {
        "question": "What is the purpose of regular expressions in text processing?",
        "answer": "Regular expressions are used as a formal language for specifying text strings and for searching for specific patterns or words within a text."
    },
    {
        "question": "What are the two kinds of errors discussed in the text related to regular expressions?",
        "answer": "The two kinds of errors discussed are: Matching strings that we should not have matched (there, then, other) - False positives (Type I errors) and Not matching things that we should have matched (The) - False negatives (Type II errors."
    },
    {
        "question": "What is the purpose of using capture groups in regular expressions?",
        "answer": "The purpose of using capture groups in regular expressions is to 'capture' a pattern into a numbered register, which can then be referred to using \\1 to access the contents of the register."
    },
    {
        "question": "According to the text, what is the difference between 'Type' and 'Token' in text processing?",
        "answer": "Type is an element of the vocabulary, while Token is an instance of that type in running text."
    },
    {
        "question": "What is the purpose of text normalization in natural language processing tasks?",
        "answer": "Text normalization is required for tasks such as tokenizing words, normalizing word formats, and segmenting sentences."
    },
    {
        "question": "What is the purpose of tokenization in natural language processing?",
        "answer": "The purpose of tokenization in natural language processing is to break text into smaller units, such as words or phrases, for analysis."
    },
    {
        "question": "How many characters are in the segment '姚   明        进      入       总         决         赛'?",
        "answer": "There are 7 characters in the segment."
    },
    {
        "question": "What is the purpose of word normalization in text processing?",
        "answer": "The purpose of word normalization is to put words or tokens in a standard format, such as converting U.S.A. to USA or am, is, be, are to be."
    },
    {
        "question": "What was the main difference between the map found in Billy Bones's chest and the map described in the text?",
        "answer": "The main difference was that the map found in Billy Bones's chest had red crosses and written notes, while the map described in the text was an accurate copy without these markings."
    },
    {
        "question": "What is word embedding in the context of NLP?",
        "answer": "Word embedding is a term used for the representation of word meaning for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
    },
    {
        "question": "According to the text, what is the meaning component of a word?",
        "answer": "A sense or 'concept'"
    },
    {
        "question": "What is the difference between words that are 'similar' and words that are 'related' according to the text?",
        "answer": "Words that are 'similar' have similar meanings, while words that are 'related' share some element of meaning but are not synonyms."
    },
    {
        "question": "According to the text, how is word meaning represented in modern NLP algorithms?",
        "answer": "Word meaning is represented using embeddings as the representation of word meaning."
    },
    {
        "question": "According to the text, what are the three affective dimensions for a word in Idea 2?",
        "answer": "The three affective dimensions for a word in Idea 2 are valence (pleasantness), arousal (intensity of emotion), and dominance (the degree of control exerted)."
    },
    {
        "question": "What is the problem with using raw dot-product for computing word similarity?",
        "answer": "The problem with raw dot-product is that it favors long vectors, as the dot product tends to be higher if a vector is longer (has higher values in many dimensions). This leads to an overemphasis on frequent words, as they have longer vectors due to their high occurrence."
    },
    {
        "question": "Why does the cosine for term-term matrix vectors range from 0-1?",
        "answer": "The cosine for term-term matrix vectors ranges from 0-1 because raw frequency values are non-negative."
    },
    {
        "question": "What is the purpose of using add-one smoothing in the context of computing PPMI?",
        "answer": "The purpose of using add-one smoothing in the context of computing PPMI is to address the bias of PMI towards infrequent events and to prevent very rare words from having very high PMI values."
    },
    {
        "question": "What is the main idea behind the Word2vec approach mentioned in the text?",
        "answer": "The main idea behind the Word2vec approach is to predict whether a word is likely to show up near another word, such as 'apricot', by training a classifier on a binary prediction task."
    },
    {
        "question": "What is the goal of the classifier mentioned in the text?",
        "answer": "The goal is to assign a probability to each candidate (word, context) pair, denoted as P(+|w, c) and P(−|w, c) = 1 − P(+|w, c)."
    },
    {
        "question": "What is the purpose of adjusting the word weights in stochastic gradient descent when learning word2vec embeddings?",
        "answer": "To make the positive pairs more likely and the negative pairs less likely over the entire training set."
    },
    {
        "question": "In the analogy 'apple is to tree as grape is to _____', what is the correct answer?",
        "answer": "vine"
    },
    {
        "question": "What are some examples of gender bias found in word embeddings according to the text?",
        "answer": "Some examples of gender bias found in word embeddings include the association of 'man' with 'computer programmer' and 'woman' with 'homemaker'."
    },
    {
        "question": "What is the purpose of the WebBootCat tool in SketchEngine?",
        "answer": "The purpose of the WebBootCat tool in SketchEngine is to collect a corpus from the Web."
    },
    {
        "question": "What is the purpose of ACL SIGWAC?",
        "answer": "ACL SIGWAC is the Association for Computational Linguistics Special Interest Group on Web as Corpus."
    },
    {
        "question": "What are word embeddings and what is the focus of the research papers mentioned in the text?",
        "answer": "Word embeddings are numerical vector representations of word meanings. The research papers focus on methods and software to learn word embeddings from corpora, as well as the impacts of scaling from small data to large real-world data-sets."
    },
    {
        "question": "What was the aim of the parsing expert system described in the text?",
        "answer": "The aim of the parsing expert system was to develop a discovery procedure for grammar, given a corpus."
    },
    {
        "question": "According to the text, what is the main factor that determines the 'best' classifier for natural language disambiguation?",
        "answer": "The 'best' classifier depends on the training set size, not the 'best' machine learning algorithm."
    },
    {
        "question": "What is the purpose of the new comprehensive test set mentioned in the text?",
        "answer": "The purpose of the new comprehensive test set is to measure both syntactic and semantic regularities of word representations."
    },
    {
        "question": "What is one of the conclusions drawn from the efficient estimation of word representations in vector space?",
        "answer": "One of the conclusions is that it is possible to train high quality word vectors using very simple model architectures compared to popular neural network models."
    },
    {
        "question": "What are word embeddings and what is the impact of scaling from small data to large real-world data-sets?",
        "answer": "Word embeddings are numerical vector representations of word meanings. Scaling from small data to large real-world data-sets can lead to higher accuracy for all classifiers."
    },
    {
        "question": "What are the two major classes of words discussed in the text and how do they differ?",
        "answer": "The two major classes of words are Closed class and Open class. Closed class words have relatively fixed membership and are usually function words, while Open class words are usually content words."
    },
    {
        "question": "What is the purpose of Part of Speech Tagging?",
        "answer": "Part of Speech Tagging is useful for other NLP tasks, improving syntactic parsing, reordering words in machine translation, distinguishing adjectives or other parts of speech for sentiment analysis, text-to-speech applications, and controlling for parts of speech in linguistic analysis."
    },
    {
        "question": "What is the tag accuracy for POS tagging in English?",
        "answer": "About 97%"
    },
    {
        "question": "What are the four most common tags used in Named Entity Recognition (NER) and what do they represent?",
        "answer": "The four most common tags used in Named Entity Recognition (NER) are PER (Person), LOC (Location), ORG (Organization), and GPE (Geo-Political Entity). They represent proper names that can refer to a person, location, organization, or geo-political entity."
    },
    {
        "question": "What does the 'B' tag represent in BIO Tagging?",
        "answer": "The 'B' tag represents a token that begins a span."
    }
]