[
    {
        "question": "What is the focus of Unit 1, part 1.1 in the module?",
        "answer": "The focus of Unit 1, part 1.1 is on the background and practical applications of corpus linguistics and text analytics."
    },
    {
        "question": "What topics are covered in Week 5 of the module on text analytics?",
        "answer": "Machine Translation, Information Extraction, and Python tools for text analytics"
    },
    {
        "question": "What is the title of the book by Witten, Frank, Hall, and Pal that is mentioned in the lecture?",
        "answer": "Data Mining: Practical machine learning tools and techniques"
    },
    {
        "question": "What are the six phases of the CRISP-DM modelling process in the context of Data Mining?",
        "answer": "The six phases of the CRISP-DM modelling process in Data Mining are: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment."
    },
    {
        "question": "What are some of the academic disciplines highlighted in the book regarding speech and language technology?",
        "answer": "Some of the academic disciplines highlighted in the book include Linguistics, Phonetics, Lexicography, and Syntax."
    },
    {
        "question": "What are some key topics covered in the lecture on Information Retrieval?",
        "answer": "Some key topics covered in the lecture on Information Retrieval include Google search, Inverted file for efficient match, Boolean set theoretic model, weighted vector model, Worked examples, Evaluation, Query broadening to improve matching, and the problem of finding strings in a database."
    },
    {
        "question": "What is the purpose of the given SQL query?",
        "answer": "The purpose of the SQL query is to find all modules that match the terms 'database', 'AI', or 'knowledge base'."
    },
    {
        "question": "Why will the given SQL query not match any record?",
        "answer": "The SQL query will not match any record because the condition 't.value = database AND t.value = AI AND t.value = knowledge base' cannot be simultaneously true for any record in the database."
    },
    {
        "question": "What is the main difference between Information Extraction (IE) and Information Retrieval (IR)?",
        "answer": "The main difference is that IR pulls documents from large text collections in response to specific keywords or queries, while IE pulls facts and structured information (Named Entities and Relations) from the content of large text collections."
    },
    {
        "question": "When would you use Information Extraction (IE)?",
        "answer": "Information Extraction (IE) would be used when you need to extract knowledge at a deeper level than Information Retrieval (IR), such as constructing a database through IE and linking it back to the documents to provide a valuable alternative search tool."
    },
    {
        "question": "What is the aim of the HaSIE application mentioned in the lecture segment?",
        "answer": "The aim of the HaSIE application is to find out how companies report about health and safety information."
    },
    {
        "question": "What is Named Entity Recognition?",
        "answer": "Named Entity Recognition is a process where a system identifies relevant sections of each document, pulls out sentences about specific entities, such as health and safety issues, and populates a database with this information."
    },
    {
        "question": "Why is Named Entity Recognition (NER) important?",
        "answer": "NER provides a foundation from which to build more complex Information Extraction (IE) systems. Relations between Named Entities (NEs) can provide tracking, ontological information, and scenario building."
    },
    {
        "question": "What are some examples of ambiguity in Named Entity (NE) types mentioned in the lecture?",
        "answer": "Some examples of ambiguity in NE types mentioned are: John Smith (company vs. person), June (person vs. month), Washington (person vs. location), and 1945 (date vs. time)."
    },
    {
        "question": "What are some advantages and disadvantages of the shallow parsing approach?",
        "answer": "Advantages include being simple, fast, language independent, and easy to retarget by creating new lists. Disadvantages include the collection and maintenance of lists, inability to deal with name variants, and inability to resolve ambiguity."
    },
    {
        "question": "What is BERT and what is it used for?",
        "answer": "BERT is a method and toolkit from Google Labs for understanding meaning relationships between sentences, and is used in tasks which involve measuring meaning similarity between sentences."
    },
    {
        "question": "What are some of the challenges in Machine Translation mentioned in the lecture?",
        "answer": "Some of the challenges in Machine Translation mentioned in the lecture include complex orthography, lexical ambiguity, morphological complexity and variation, tokenization issues, and translation divergences."
    },
    {
        "question": "Why is it important to have a parallel corpus with source and target sentences aligned for machine translation?",
        "answer": "It is important to have a parallel corpus with source and target sentences aligned for machine translation in order to train translation models effectively by having corresponding sentences in different languages for comparison and learning."
    },
    {
        "question": "What is GIZA++ and how is it used in statistical machine translation?",
        "answer": "GIZA++ is a statistical machine translation toolkit used to train word alignments."
    },
    {
        "question": "What are some advantages of Phrase-Based Statistical Machine Translation (SMT)?",
        "answer": "Some advantages of Phrase-Based SMT include many-to-many mappings for non-compositional phrases, the use of local context for disambiguation, and the ability to handle longer learned phrases with more data."
    },
    {
        "question": "What is the purpose of the Bleu Metric in automatic evaluation of machine translation?",
        "answer": "The purpose of the Bleu Metric is to compare machine translation output against several human translations to give a standardized score that correlates highly with human evaluation."
    },
    {
        "question": "Where can Artificial Intelligence experts join social media communities for networking, sharing knowledge, and getting ideas for research projects?",
        "answer": "Artificial Intelligence experts can join social media communities like Facebook, LinkedIn, Quora, as well as communities set up by/for academics and practitioners such as KDnuggets, Kaggle, Weka, ICAME, ACL, SemEval, and EU-JRC communities."
    },
    {
        "question": "What are some of the subsidiaries of the parent organization mentioned in the lecture?",
        "answer": "Facebook, Instagram, WhatsApp"
    },
    {
        "question": "What is a knowledge nugget according to the lecture material?",
        "answer": "A knowledge nugget is a small item or piece of useful knowledge in categories of interest to the user."
    },
    {
        "question": "What platform offers a customizable Jupyter Notebooks environment, access to free GPUs, and a large repository of community published data & code?",
        "answer": "Kaggle offers a no-setup, customizable, Jupyter Notebooks environment. Access free GPUs and a huge repository of community published data & code."
    },
    {
        "question": "What is the purpose of the WEKA software mentioned in the lecture segment?",
        "answer": "The purpose of WEKA is to provide a platform for knowledge analysis and is a hub for users to access downloads, textbooks, tutorials, and contribution add-ons in a more open and community-based environment."
    },
    {
        "question": "What areas of expertise are involved in the HUMAINT project?",
        "answer": "The areas of expertise involved in the HUMAINT project include cognitive science, machine learning, human-computer interaction, and economy."
    },
    {
        "question": "What method was used by Mosteller and Wallace to determine the authorship of the disputed letters in the Federalist papers?",
        "answer": "Bayesian methods"
    },
    {
        "question": "What is the goal of probabilistic language models discussed in the lecture?",
        "answer": "The goal is to assign a probability to a sentence."
    },
    {
        "question": "What is the goal of probabilistic language modeling?",
        "answer": "The goal is to compute the probability of a sentence or sequence of words."
    },
    {
        "question": "How can we compute the joint probability of a sequence of words using the Chain Rule of Probability?",
        "answer": "We can compute the joint probability of a sequence of words using the Chain Rule by multiplying the individual probabilities of each word given the previous words in the sequence."
    },
    {
        "question": "Why might it be challenging to accurately predict the next word in a sentence based on the given text 'No! Too many possible sentences!'?",
        "answer": "It might be challenging because there are too many possible combinations and variations of sentences that could follow the given text."
    },
    {
        "question": "What are the two kinds of chatbots discussed in the lecture?",
        "answer": "The two kinds of chatbots discussed in the lecture are Conversational agents and Task-based Dialogue Agents."
    },
    {
        "question": "Which chatbot architecture was the first system to pass the Turing Test?",
        "answer": "A mental model (PARRY) was the first system to pass the Turing Test."
    },
    {
        "question": "What is a formal language used for specifying text strings?",
        "answer": "Regular expressions"
    },
    {
        "question": "What are the two kinds of errors discussed in the lecture regarding Regular Expressions?",
        "answer": "The two kinds of errors discussed are: 1. Matching strings that should not have been matched (false positives or Type I errors) and 2. Not matching things that should have been matched (false negatives or Type II errors)."
    },
    {
        "question": "What are the two antagonistic efforts involved in reducing the error rate for an application?",
        "answer": "The two antagonistic efforts involved in reducing the error rate for an application are increasing accuracy or precision (minimizing false positives) and increasing coverage or recall (minimizing false negatives)."
    },
    {
        "question": "What is word embedding in the context of NLP?",
        "answer": "Word embedding is a term used for the representation of word meaning for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
    },
    {
        "question": "According to the lecture, what is the meaning of 'dog' in the context of Computer Vision classification?",
        "answer": "DOG"
    },
    {
        "question": "What is the meaning component of a word according to the lecture material?",
        "answer": "A sense or concept"
    },
    {
        "question": "What tool is introduced in the video for corpus linguistics?",
        "answer": "SketchEngine web tool"
    },
    {
        "question": "What are some of the functions of SketchEngine mentioned in the lecture segment?",
        "answer": "Some of the functions of SketchEngine mentioned in the lecture segment include Word sketch summary, Concordance examples, Distributional Thesaurus, Parallel corpus creation, and Part-of-Speech tagging."
    },
    {
        "question": "What is the purpose of the SketchEngine web tool mentioned in the lecture segment?",
        "answer": "The SketchEngine web tool is used for corpus linguistics."
    },
    {
        "question": "What are word embeddings and what is their significance in natural language processing?",
        "answer": "Word embeddings are numerical vector representations of word meanings. They are significant in natural language processing as they capture semantic relationships between words and are used in various NLP tasks such as language modeling, document classification, and machine translation."
    },
    {
        "question": "Where can you find word2vec code and the associated research paper?",
        "answer": "You can find the word2vec code on Google code archive at code.google.com/archive/p/word2vec/ and the research paper in the document titled Efficient Estimation of Word Representations in Vector Space by Mikolov."
    },
    {
        "question": "What are the key components of an AI research paper, as mentioned in the lecture segment?",
        "answer": "The key components of an AI research paper include research aims & objectives, background related research, methods, programme of research work, conclusions, and results."
    },
    {
        "question": "Who is attributed with the classification of 8 parts of speech in the 1st century BCE?",
        "answer": "Dionysius Thrax of Alexandria"
    }
]