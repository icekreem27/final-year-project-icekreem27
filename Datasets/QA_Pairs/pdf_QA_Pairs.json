[
    {
        "question": "What is the focus of Professor Eric Atwell's research?",
        "answer": "Professor Eric Atwell's research focuses on the use of chatbots in higher education."
    },
    {
        "question": "What misconception does the text address about chatbots?",
        "answer": "The text addresses the misconception that a chatbot is simply a back-and-forth conversation between a person and a computer."
    },
    {
        "question": "Who are the research fellows mentioned in the text?",
        "answer": "Noorhan Abbas and Tom Pickard"
    },
    {
        "question": "Who is involved in using chatbots for education according to the text?",
        "answer": "Aisha Walker"
    },
    {
        "question": "What is the process at Leeds University at the end of each course module?",
        "answer": "At the end of each course module at Leeds University, students are sent a questionnaire where they have to answer on a scale of 1 to 5 whether they agree or disagree with each question."
    },
    {
        "question": "Where have the chatbots been used?",
        "answer": "The chatbots have been used in four different universities and research centres in the UK, Spain, Croatia, and Cyprus."
    },
    {
        "question": "What is one benefit of collecting coarse feedback in the described way?",
        "answer": "Collecting coarse feedback this way gets better quality responses and potentially the students can think it's fun as well."
    },
    {
        "question": "Why does the text mention that some students may get fed up with answering questionnaires?",
        "answer": "Some students may get fed up with answering questionnaires because if they receive loads of questionnaires with 10 questions each, they may not respond at all."
    },
    {
        "question": "What is the name of the chatbot system mentioned in the text?",
        "answer": "Hubert or Hubert.ai"
    },
    {
        "question": "What are some examples of voice-driven chatbots mentioned in the text?",
        "answer": "Amazon's Alexa and Microsoft's Ask LU"
    },
    {
        "question": "What is LU an abbreviation for in the text?",
        "answer": "Lancaster University"
    },
    {
        "question": "What can the little Cora chatbot help with in online banking?",
        "answer": "The little Cora chatbot can help with questions like how to close down an account or how to make a complaint about NatWest."
    },
    {
        "question": "What is the purpose of the Differ system mentioned in the text?",
        "answer": "The Differ system helps students to get to know each other before they come to the University and promotes student engagement."
    },
    {
        "question": "What is an example of a system mentioned in the text that helps students in introductory programming courses?",
        "answer": "Coding Tutor"
    },
    {
        "question": "What is the purpose of the Coding Tutor chatbot mentioned in the text?",
        "answer": "The Coding Tutor chatbot provides feedback to students in English and suggests ways to improve their work."
    },
    {
        "question": "What is one challenge mentioned in the text when it comes to teaching?",
        "answer": "Building in the background knowledge into a chatbot can be harder for real teachers compared to a chatbot that tells you when your next lecture is."
    },
    {
        "question": "What use case did the project focus on, which was considered useful in teaching?",
        "answer": "Getting feedback from students about the course"
    },
    {
        "question": "Which institutions were partners in the project mentioned in the text?",
        "answer": "University of Granada in Spain, University of Zagreb in Croatia, Centre of Excellence Centre in Cyprus, Science Centre of Excellence, and University in the UK"
    },
    {
        "question": "What did the teachers do at the end of the courses?",
        "answer": "The teachers asked the students to give feedback on the courses."
    },
    {
        "question": "What types of courses are most of the courses in the project focused on?",
        "answer": "Most of the courses in the project are computing-oriented or business-oriented."
    },
    {
        "question": "What instructions does the text provide for the student to follow regarding their age and gender?",
        "answer": "The text instructs the student to type their age as a number and choose between male or female for their gender."
    },
    {
        "question": "What are students supposed to type about the course according to the text?",
        "answer": "Students are supposed to type more about what is actually good or bad about the course."
    },
    {
        "question": "What is the big difference between the new approach and the traditional approach of asking questions to students?",
        "answer": "The big difference is that the new approach involves going into a conversation with the student to tease out more responses, rather than just presenting questions in a box for the student to type into."
    },
    {
        "question": "What is being asked about the conversations mentioned in the text?",
        "answer": "The number of interactions"
    },
    {
        "question": "What does the chatbot do if a student responds with 'nothing' when asked what was wrong with the course?",
        "answer": "The chatbot would try to prompt the student to think about it more by asking 'Are you sure? Can you tell it think about it a bit more?'"
    },
    {
        "question": "How many turns did the conversations go up to?",
        "answer": "About 30 turns"
    },
    {
        "question": "What does Hubert ask the user in the conversation?",
        "answer": "Hubert asks the user, 'What could the teachers start doing that would improve it?'"
    },
    {
        "question": "What is the user being asked to remember in the text?",
        "answer": "The user is being asked to remember what the teachers could start doing to improve something."
    },
    {
        "question": "What is Hubert trying to do when interacting with the student?",
        "answer": "Hubert is trying to not just simply accept that the student is very happy but try to find something more."
    },
    {
        "question": "When does the lecturer typically receive feedback and use it for improvement?",
        "answer": "The lecturer typically receives feedback and uses it for improvement for the next year."
    },
    {
        "question": "What did the students get to find out about in terms of the modules being taught?",
        "answer": "The students get to find out something about what really works and what doesn't work for them in terms of the modules being taught."
    },
    {
        "question": "What scale is used to rate Hubert in the text?",
        "answer": "On a scale of 1 to 10"
    },
    {
        "question": "What is being discussed in the conversation mentioned in the text?",
        "answer": "The European Union funding a project and wanting to know if it is good."
    },
    {
        "question": "What is the user's opinion on the new way of interaction compared to filling a form?",
        "answer": "The user finds the new way of interaction interesting and better than filling a form."
    },
    {
        "question": "What unexpected side effect did the computing students experience while interacting with the chatbot?",
        "answer": "Some computing students started to say that they just wanted to see if they could break the chatbot and test its intelligence by asking difficult questions."
    },
    {
        "question": "What does Hubert say in response to the user saying 'bitch'?",
        "answer": "A poor choice of words never improved anything."
    },
    {
        "question": "How does the user feel about Hubert's joke?",
        "answer": "The user doesn't think it's a very good joke, calling it a 'sick joke.'"
    },
    {
        "question": "What does Hubert think coronavirus causes?",
        "answer": "Hubert thinks coronavirus causes the SARS-CoV-2 virus."
    },
    {
        "question": "Who is the American surrealist film written, edited, co-produced, and directed by?",
        "answer": "Hubert"
    },
    {
        "question": "What method did Hubert use to try to find an answer to a question?",
        "answer": "Hubert did a Google search of 'what is it'"
    },
    {
        "question": "What were some of the positive perceptions that students had of using the chatbot named Hubert?",
        "answer": "Students volunteered positive perceptions of the use of Hubert, finding it good to use rather than just having a questionnaire. They engaged with the chatbot, gave friendly responses, and found Hubert to be friendly in return."
    },
    {
        "question": "How did Hubert typically respond to challenging questions from the students?",
        "answer": "Hubert tended to either just give a silly reply or treat it as a joke, or ask them to modify their questions to see if it worked."
    },
    {
        "question": "What is the topic of Professor Eric Atwell's lecture?",
        "answer": "The topic of Professor Eric Atwell's lecture is chatbots, also known as dialogue systems."
    },
    {
        "question": "What are some examples of chatbots mentioned in the text?",
        "answer": "ELIZA, PARRY, AIML, ALICE, and Hubert"
    },
    {
        "question": "What is one method mentioned for training chatbots in the text?",
        "answer": "Using a corpus to train via some sort of machine learning"
    },
    {
        "question": "What are some other names used to refer to conversational agents?",
        "answer": "Dialogue systems, dialogue agents, chatbots"
    },
    {
        "question": "What are some examples of personal assistants mentioned in the text?",
        "answer": "SIRI, Alexa, Cortana, Google Assistant"
    },
    {
        "question": "What feature allows you to have a verbal conversation with someone in addition to typing?",
        "answer": "The feature allows you to have a verbal conversation with someone in addition to typing is mentioned in the text."
    },
    {
        "question": "What are these smart systems particularly good for?",
        "answer": "Playing music"
    },
    {
        "question": "What can you ask Alexa to do if you want to hear your favorite Sex Pistols song?",
        "answer": "You can say, 'Alexa, play me my favourite Sex Pistols song.'"
    },
    {
        "question": "What are some tasks that can be done using Alexa according to the text?",
        "answer": "Booking travel reservations, ordering, shopping via Amazon, and answering general knowledge questions"
    },
    {
        "question": "Who are some of the professionals that individuals with mental health problems can talk to?",
        "answer": "Individuals with mental health problems can talk to a clinician, a mental health practitioner, or a chatbot."
    },
    {
        "question": "What are the two kinds of chatbots mentioned in the text?",
        "answer": "The two kinds of chatbots mentioned are chatting conversational agents and targeted task-based dialogue agents."
    },
    {
        "question": "What are some examples of tasks that personal assistants could help with, as mentioned in the text?",
        "answer": "Some examples of tasks that personal assistants could help with include reminding about lectures, coursework deadlines, controlling car features, giving robot instructions, and providing washing machine instructions."
    },
    {
        "question": "What are the two types of chatbots mentioned in the text?",
        "answer": "The two types of chatbots mentioned in the text are conversational agents and task-based agents."
    },
    {
        "question": "What are the two types of chatbot architectures mentioned in the text?",
        "answer": "The two types of chatbot architectures mentioned are rule-based systems and another type."
    },
    {
        "question": "What is the process described for machine learning from a corpus in the text?",
        "answer": "The process involves having a training corpus appropriately annotated, where the machine learning algorithm extracts rules similar to those created by humans in the first version."
    },
    {
        "question": "What is one advantage of using a question type as the pattern and an answer as the action in question answering systems?",
        "answer": "It fits very well for question answering systems."
    },
    {
        "question": "What test did PARRY pass according to the psychologists?",
        "answer": "The Turing test"
    },
    {
        "question": "What are the two approaches mentioned for finding the best match in information retrieval?",
        "answer": "The two approaches mentioned are encoding the input into a vector and finding the closest vector in the corpus using information retrieval techniques, and using deep learning neural models to find the best match."
    },
    {
        "question": "What can the BlenderBot do in response to a request to sing a song?",
        "answer": "The BlenderBot can find an appropriate match in the training corpus and output that."
    },
    {
        "question": "What is the Chinese system developed by Microsoft China mentioned in the text?",
        "answer": "The Chinese system developed by Microsoft China"
    },
    {
        "question": "What is the name of the chatbot mentioned in the text?",
        "answer": "Xiaoice"
    },
    {
        "question": "What are users advised when they start the app?",
        "answer": "Users are advised that they are chatting to an AI agent, not a real person."
    },
    {
        "question": "What is usually the first focus of a model like Alexa?",
        "answer": "Setting a timer or making a travel reservation"
    },
    {
        "question": "What is the key factor that determines how many skills you can power up at the same time?",
        "answer": "Each skill is very much based on a goal or task, so the key factor is the number of components or widgets available for each skill."
    },
    {
        "question": "What is the architecture for a particular skill described in the text?",
        "answer": "The architecture for a particular skill is to have a frame with slots and values, representing user intentions rather than just recording words and phrases."
    },
    {
        "question": "What are some key components that a travel reservation system needs to have?",
        "answer": "An origin, a destination, departure time, departure date, and an airline if it's an airline booking system."
    },
    {
        "question": "What types of questions can the system ask to fill the slots in a reservation?",
        "answer": "The system can ask typical questions to fill each of the slots in a reservation."
    },
    {
        "question": "What are the two types of architectures for conversational chatbots or task-based systems mentioned in the text?",
        "answer": "The two types of architectures mentioned are rule-based systems and corpus trained machine learning systems."
    },
    {
        "question": "What is the difference in user expectations between interacting with a graphical user interface and a chatbot?",
        "answer": "Users expect a graphical user interface to be designed by a human, while they expect a chatbot to communicate in a natural human way."
    },
    {
        "question": "What is the purpose of pointing out examples in the telephone conversation between the travel agent and the client?",
        "answer": "The purpose is to show that the conversation is not just a simple exchange of question and answer, but includes strange extra things happening."
    },
    {
        "question": "What is the nature of the turns in a conversation according to the text?",
        "answer": "The turns in a conversation are not simply question followed by answer, but rather a game where two or more people take turns speaking, with a turn typically being a sentence but can also be a single word or a longer sentence."
    },
    {
        "question": "What does the client ask the agent in the text?",
        "answer": "The client asks 'what are they?'"
    },
    {
        "question": "How can individuals determine when it is their turn to speak in a conversation, according to the text?",
        "answer": "Individuals can determine when it is their turn to speak in a conversation by recognizing pauses, intonation changes, and interruptions."
    },
    {
        "question": "What behavior does the human agent exhibit when the client interrupts?",
        "answer": "The human agent stops talking when the client interrupts."
    },
    {
        "question": "What does the hash symbol indicate in the text?",
        "answer": "The hash symbol indicates that two events are happening simultaneously."
    },
    {
        "question": "What is the end-pointing task for a speech system mentioned in the text?",
        "answer": "Deciding whether the user has stopped talking"
    },
    {
        "question": "According to the text, what are constatives in the context of speech acts or dialogue acts?",
        "answer": "Constatives are speech acts that commit the speaker to agreeing to something without introducing any new information."
    },
    {
        "question": "What are some examples of commissives according to the text?",
        "answer": "Examples of commissives include 'I promise to do this.'"
    },
    {
        "question": "What is the analogy drawn between annotating dialogue corpus and part of speech tagging or named entity tagging?",
        "answer": "The analogy is that in both cases, a whole piece of speech is labelled or tagged as being a directive or a constative or acknowledgment."
    },
    {
        "question": "What are some of the principles mentioned in the text related to conversations and actions?",
        "answer": "The principles mentioned are the grounding principle and the principle of closure."
    },
    {
        "question": "Why is it important for speakers to acknowledge that the hearer has understood during a speech act?",
        "answer": "It is important for speakers to acknowledge that the hearer has understood during a speech act to ensure mutual understanding and effective communication."
    },
    {
        "question": "Why do elevator buttons light up when pressed?",
        "answer": "To show and acknowledge that the elevator knows it has been called."
    },
    {
        "question": "What action do you know to take when the light comes on?",
        "answer": "Stop pressing the button"
    },
    {
        "question": "What is the important bit in the utterance 'I will take the 5ish flight on the night before on the 11th'?",
        "answer": "The important bit is the 11th."
    },
    {
        "question": "What are some examples of pairs mentioned in the text?",
        "answer": "Some examples of pairs mentioned in the text are question and answer, proposal followed by acceptance or rejection, and compliment followed by a downplay."
    },
    {
        "question": "What can interactions in the text be compared to?",
        "answer": "Interactions in the text can be compared to bigram models or Markov models."
    },
    {
        "question": "What is the purpose of a correction sub-dialogue in the text?",
        "answer": "The purpose of a correction sub-dialogue is to make a change to a previous statement, specifically altering the day parameter."
    },
    {
        "question": "What destination were the individuals discussing in the text?",
        "answer": "Hong Kong"
    },
    {
        "question": "What did the user ask the system before inquiring about making train reservations?",
        "answer": "The user asked, 'can you make train reservations?'"
    },
    {
        "question": "Why are natural language processing systems quite difficult to implement in human conversations?",
        "answer": "Because they have to allow for the human to take charge and then for the system to take charge in a mixed initiative conversation."
    },
    {
        "question": "What is the user initiative system mentioned in the text?",
        "answer": "The user initiative system is where the assumption is the user is going to ask a question and the system responds."
    },
    {
        "question": "What is Hubert chatbot trying to do with the students?",
        "answer": "Hubert chatbot is trying to get students to give feedback on the course and ask specific questions."
    },
    {
        "question": "What inference does the system have to make based on the client's statement about the meeting dates?",
        "answer": "The system has to infer that the client needs to travel before the 12th."
    },
    {
        "question": "Why is it not good to travel on the 12th according to the text?",
        "answer": "It's not good to travel on the 12th because he won't be there for the meeting on the 12th."
    },
    {
        "question": "Who developed the chatbot ELIZA?",
        "answer": "Weizenbaum"
    },
    {
        "question": "What is the main goal of the Rogerian approach in therapy?",
        "answer": "The main goal of the Rogerian approach in therapy is to engage the client in conversation and help them understand their own problems by talking about their problems themselves."
    },
    {
        "question": "What does the psychologist do to encourage the patient to talk more about their experience?",
        "answer": "The psychologist asks the patient to 'tell me more about boats'."
    },
    {
        "question": "What is the Loebner prize contest and how is it related to the Turing Test?",
        "answer": "The Loebner prize contest is a contest organized by Hugh Loebner where chatbots are evaluated by humans to see if they can pass as human. It is related to the Turing Test as it is a version of the Turing Test."
    },
    {
        "question": "What is ELIZA based on and how does it work?",
        "answer": "ELIZA is based on a large set of rules devised by Weizenbaum, which are essentially patterns. The user input is matched against these patterns until a matching pattern is found."
    },
    {
        "question": "What does the pattern 'zero you, zero me' mean in the text?",
        "answer": "The pattern 'zero you, zero me' means anything followed by 'you' followed by anything followed by 'me'."
    },
    {
        "question": "What is the significance of the third word or constituent in the text?",
        "answer": "The third word or constituent in the text is used to organize rules by keywords, each with a pattern and a list of possible transforms."
    },
    {
        "question": "What is the key word and pattern discussed in the text?",
        "answer": "The key word is 'you' and the pattern involving 'you' is something, you, something, me."
    },
    {
        "question": "According to the text, why is the keyword 'I' ranked near the end of the list?",
        "answer": "The keyword 'I' is ranked near the end of the list because it is very general, and if the input doesn't match a very specific keyword, it should match at least some of the things on the list."
    },
    {
        "question": "What does the author mean by 'I star' in the text?",
        "answer": "Any sentence starting with 'I'"
    },
    {
        "question": "What is the rank of 'Everybody' and 'I' in the text?",
        "answer": "'Everybody' has a rank of five and 'I' has a rank of zero."
    },
    {
        "question": "According to the text, why is it important to store away what the user had input earlier on in the memory?",
        "answer": "To be able to go back to it and reference it later"
    },
    {
        "question": "What feature does the human-like thing described in the text have that sets it apart from a Markov or bigram model?",
        "answer": "The human-like thing described in the text can go back to an earlier point at various interesting points, maybe even at random."
    },
    {
        "question": "What can a chatbot do if it cannot find a rule that matches a user's input?",
        "answer": "The chatbot can either try general prompts like 'tell me more' or go back to the memory phrase."
    },
    {
        "question": "Why did one of Weizenbaum's staff ask him to leave the room while using the AI system?",
        "answer": "She thought the conversation was personal and private."
    },
    {
        "question": "Why did people point out that storing ELIZA conversations without permission is unethical?",
        "answer": "People pointed out that storing ELIZA conversations without permission is unethical because it violates privacy and requires obtaining permission in advance."
    },
    {
        "question": "What is ELIZA according to the text?",
        "answer": "ELIZA is not really intelligent, it's just a set of rules."
    },
    {
        "question": "What is the idea behind PARRY and what is it used to study?",
        "answer": "The idea behind PARRY is to have an overall architecture in addition to patterns, and it is used to study schizophrenia."
    },
    {
        "question": "What are the global variables mentioned in the text and what do they represent?",
        "answer": "The global variables mentioned are anger, fear, and mistrust. They represent mental states and are internally represented as numbers starting with low values."
    },
    {
        "question": "What factors influence PARRY's responses according to the text?",
        "answer": "Factors such as insults, compliments, and the analysis of the user's statements by PARRY influence PARRY's responses."
    },
    {
        "question": "In what ways do different emotions like fear and anger influence responses according to the text?",
        "answer": "Fear leads to responses involving running away, while anger leads to more hostile responses."
    },
    {
        "question": "What is the purpose of the Turing test?",
        "answer": "The purpose of the Turing test is for a human to judge whether the output of a system is a chatbot or a real human."
    },
    {
        "question": "What is the artificial intelligence markup language used for building systems like ELIZA?",
        "answer": "AIML (artificial intelligence markup language)"
    },
    {
        "question": "Who developed a version of ELIZA called ALICE and put it online for people to interact with?",
        "answer": "Richard Wallace"
    },
    {
        "question": "What did Wallace do if ALICE gave a poor or implausible response?",
        "answer": "Wallace added some extra rules to fix the response so that next time it would be more plausible."
    },
    {
        "question": "What is the purpose of the Loebner prize competition mentioned in the text?",
        "answer": "The purpose of the Loebner prize competition is to have humans interact with chatbots and try to judge which ones are human and which ones are not."
    },
    {
        "question": "How many times did Alice win the prize?",
        "answer": "Alice won the prize three times."
    },
    {
        "question": "What is one example of a chatbot mentioned in the text?",
        "answer": "FAQchat"
    },
    {
        "question": "What is the purpose of the FAQchat chatbot mentioned in the text?",
        "answer": "The purpose of the FAQchat chatbot is to find the most similar question in the FAQ and provide the corresponding answer when asked questions."
    },
    {
        "question": "In real conversations, what do both partners do in leading the conversation according to the text?",
        "answer": "In real conversations, both partners take turns in leading the conversation."
    },
    {
        "question": "What are the two simplifying assumptions mentioned for chatbots in the text?",
        "answer": "The two simplifying assumptions mentioned for chatbots are: either the user is asking questions and the chatbot has to answer them, or the system is asking questions and the user simply has to answer them."
    },
    {
        "question": "What type of chatbots are particularly good for general conversations?",
        "answer": "Corpus-based chatbots"
    },
    {
        "question": "What can you do if you have a very specific domain according to the text?",
        "answer": "You can build a set of rules from an FAQ website or from some other standard source."
    },
    {
        "question": "What are some ways to generate a response using information retrieval and a language model?",
        "answer": "Using information retrieval to grab a response from the corpus or training a language model on the corpus to generate a response."
    },
    {
        "question": "Where did the corpuses come from?",
        "answer": "The corpuses can be retrieved from the user input or used in advance to train a neural network."
    },
    {
        "question": "What types of linguists collected corpora of language mentioned in the text?",
        "answer": "Computational linguists and corpus linguists"
    },
    {
        "question": "What is the international Corpus of English (ICE) and what does it include?",
        "answer": "The international Corpus of English (ICE) includes a million words for British, American, Australian, New Zealand, Canadian, and various other variants of English, including a significant portion of spoken dialect transcribed."
    },
    {
        "question": "What was the Afrikaans chatbot trained on?",
        "answer": "The Afrikaans chatbot was trained on the Korpus Gesproke Afrikaans, a corpus collection collected at Potchefstroom University in South Africa of Afrikaners speaking to each other."
    },
    {
        "question": "What is the purpose of the call home corpora mentioned in the text?",
        "answer": "The purpose of the call home corpora is to provide a free one-hour call to volunteers' parents or family back in their home country."
    },
    {
        "question": "What type of data sets are mentioned in the text for training a chatbot?",
        "answer": "Frequently asked questions and a large data set of typical IT user questions and their answers"
    },
    {
        "question": "What can you extract from movie subtitles to get the transcript of what was said?",
        "answer": "You can extract the subtitles from movie subtitles to get the transcript of what was said."
    },
    {
        "question": "Where can you find pseudo conversations about how to get into university?",
        "answer": "On social media platforms like Twitter, Reddit, Chinese Weibo, etc."
    },
    {
        "question": "What is one ethical issue mentioned in the text?",
        "answer": "One ethical issue mentioned is the need to remove personally identifiable information."
    },
    {
        "question": "What does the speaker emphasize people should be careful about during telephone conversations?",
        "answer": "The speaker emphasizes that people should be careful about mentioning other people's names, telephone numbers, credit card numbers, or any personal information during telephone conversations."
    },
    {
        "question": "How is the response determined when a user asks a question in a large corpus?",
        "answer": "The response is determined by finding the turn in the corpus that is most similar to the question using a measure like tf-isf cosine measure, and then responding with the corresponding answer."
    },
    {
        "question": "What method is described as not being much more complicated than the simple model mentioned?",
        "answer": "The neural method"
    },
    {
        "question": "What is the main difference in using BERT compared to cosine similarity for finding the most similar turn in a corpus?",
        "answer": "The main difference is that BERT is used instead of cosine similarity."
    },
    {
        "question": "What is one way you could use a neural network according to the text?",
        "answer": "One way you could use a neural network is by encoding the corpus and training it for every sentence, question, and response."
    },
    {
        "question": "How is the response generated when a user types in a question?",
        "answer": "The response is generated by conditioning the encoding of the query and the response so far."
    },
    {
        "question": "Who wrote a series of books about Jeeves very early on, about a hundred years ago?",
        "answer": "PG Wodehouse"
    },
    {
        "question": "What is the significance of the phrase 'What ho' in the text?",
        "answer": "The phrase 'What ho' is a standard response to anything, but if you reply with 'What ho' in return, then the conversation becomes difficult to continue."
    },
    {
        "question": "What is the consequence of saying 'see you later' in response to 'see you later'?",
        "answer": "You're stuck"
    },
    {
        "question": "What sources does Xiaoice look into to find information about Beijing?",
        "answer": "Xiaoice looks into news articles, public lectures, and sources like Wikipedia to find information about Beijing."
    },
    {
        "question": "What is one limitation of chatbots mentioned in the text?",
        "answer": "One limitation of chatbots mentioned in the text is that they are not really people and do not truly understand."
    },
    {
        "question": "What is a potential drawback of information retrieval-based chatbots?",
        "answer": "They can give bad responses if there is something bad in the training data."
    },
    {
        "question": "What is the purpose of integrating a chatbot into a knowledge-based frame-based agent?",
        "answer": "The purpose is to create a dialogue state or belief state architecture."
    },
    {
        "question": "What components are involved in the architecture diagram described in the text?",
        "answer": "The components involved are automatic speech recognition, spoken language understanding, dialogue state tracker, corpus of likely responses, and state model of frames."
    },
    {
        "question": "What is the purpose of the dialogue policy model mentioned in the text?",
        "answer": "The purpose of the dialogue policy model is to determine an appropriate response and generate natural language text replies."
    },
    {
        "question": "What is the function of the dialogue state tracker in data mining and text analytics?",
        "answer": "The dialogue state tracker maintains the current state of the dialogue."
    },
    {
        "question": "What criteria is the user looking for when searching for a restaurant?",
        "answer": "The user is looking for a cheaper restaurant, specifically for Thai food in the downtown or center of the city."
    },
    {
        "question": "What additional information does the system need to provide in response to the user's question 'where is it?'",
        "answer": "The system needs to provide an address in addition to the information about cheap Thai food."
    },
    {
        "question": "What is one method mentioned for evaluating dialogue systems?",
        "answer": "Coming up with an accuracy score or maybe a precision and a recall score."
    },
    {
        "question": "According to the text, why is it difficult to determine if a response in a conversation is correct or not correct?",
        "answer": "It is difficult to determine if a response in a conversation is correct or not correct because there are usually several plausible answers and variations of answers, rather than just one correct answer."
    },
    {
        "question": "How are conversational chatbots evaluated according to the text?",
        "answer": "Conversational chatbots are evaluated by humans based on whether the conversation is liked and feels reasonable."
    },
    {
        "question": "What are the two main methods mentioned in the text for evaluating a chatbot's performance?",
        "answer": "The two main methods mentioned are the Turing test and observer evaluation."
    },
    {
        "question": "How were psychologists involved in the evaluation of PARRY's conversations?",
        "answer": "Psychologists were asked to rate whether PARRY was having a conversation or a real psychopath or psychotic person."
    },
    {
        "question": "What are the eight dimensions of quality that are used to rate the conversation between a human and AI models?",
        "answer": "The eight dimensions of quality are: avoiding repetition, being interesting, making sense, being fluent, listening to the human, being inquisitive, etc."
    },
    {
        "question": "What are some criteria used in the evaluation of human-like and engaging behavior?",
        "answer": "Some criteria used in the evaluation are repeating themselves over and over, saying the same thing twice, and always saying something new."
    },
    {
        "question": "How do you rate the different dimensions or features of a chatbot?",
        "answer": "You rate the different dimensions or features of a chatbot on a scale of one to three or one to five."
    },
    {
        "question": "What is the most scientific way of evaluating conversations with chatbots according to the text?",
        "answer": "The most scientific way is to have people converse with chatbots, have annotators analyze the responses, and compare two conversations to decide which one is better."
    },
    {
        "question": "According to the text, what criteria should be considered when deciding which conversation to prefer in a long conversation?",
        "answer": "Engagingness"
    },
    {
        "question": "What is one of the challenges mentioned in evaluating chatbots?",
        "answer": "One of the challenges mentioned is that there is no real equivalent for chatbot evaluation."
    },
    {
        "question": "What is the purpose of adversarial evaluation in the context of training a Turing-like classifier?",
        "answer": "The purpose of adversarial evaluation is to distinguish between human responses and machine responses by training a classifier on examples of chatbots and human conversations."
    },
    {
        "question": "What method can be used to evaluate an IT system according to the text?",
        "answer": "A user satisfaction survey"
    },
    {
        "question": "What are some questions that can be asked to the user after a chatbot conversation?",
        "answer": "Some questions that can be asked to the user after a chatbot conversation include: was the system easy to understand? Did the system understand what you said? and did you like the chatbot?"
    },
    {
        "question": "What are some of the questions that can be asked to evaluate the effectiveness of a chatbot?",
        "answer": "Some questions that can be asked to evaluate the effectiveness of a chatbot include: How good was it to use? How easy was it to use? Would you use this system in the future?"
    },
    {
        "question": "What are some examples of metrics mentioned in the text for evaluating chatbots?",
        "answer": "Examples of metrics mentioned in the text for evaluating chatbots include efficiency cost, quality cost, and the number of times the user had to intervene."
    },
    {
        "question": "How does the text suggest evaluating chatbots?",
        "answer": "The text suggests evaluating chatbots in the same ways as other IT systems and IT interfaces."
    },
    {
        "question": "What is a Wizard of Oz study based on?",
        "answer": "A famous story of Oz where the wizard of Oz appeared to be all-powerful but turned out to be just an American salesman."
    },
    {
        "question": "What is the purpose of a Wizard of Oz study mentioned in the text?",
        "answer": "The purpose of a Wizard of Oz study is to have a real person pretend to be the chatbot and enter into conversations to gather data for iterative tests on users."
    },
    {
        "question": "Why is it important to distinguish between humans and chatbots in artificial agents?",
        "answer": "It is important to distinguish between humans and chatbots in artificial agents to make clear that they are not simply built to see if it's possible to build."
    },
    {
        "question": "Who wrote the book Frankenstein?",
        "answer": "Mary Shelley"
    },
    {
        "question": "What were the consequences of the humans succeeding in a certain task in the story?",
        "answer": "The human was very upset because they didn't feel normal, didn't feel right, and lots of things happened in the story which weren't very good."
    },
    {
        "question": "What are some potential dangers mentioned in the text regarding the behavior of systems like chatbots or psychotherapy systems?",
        "answer": "The potential dangers mentioned include distracting the driver, giving bad medical advice, being nasty or abusive towards users, and being biased or nasty towards particular social groups."
    },
    {
        "question": "What is the problem of information leakage mentioned in the text?",
        "answer": "The problem of information leakage is that the chatbot, if trained on a corpus, may know things about the people in the corpus, such as what they said, which may leak out."
    },
    {
        "question": "Why is it important to communicate with a chatbot or in-vehicle conversational agent when using it for mental health purposes?",
        "answer": "It is important to communicate with a chatbot or in-vehicle conversational agent when using it for mental health purposes because not saying anything can be bad for mental health."
    },
    {
        "question": "What is the purpose of the driving simulator at Leeds University?",
        "answer": "The driving simulator at Leeds University is used for testing out interfaces to different computer control systems."
    },
    {
        "question": "What does the speaker mention as a potential distraction while using the virtual system?",
        "answer": "The speaker mentions that the chat thing talking to them is distracting them from driving, causing them to crash into things."
    },
    {
        "question": "What personality did Microsoft give to the Twitter chatbot they developed?",
        "answer": "Microsoft gave the Twitter chatbot the personality of a young 18 to 24-year-old American woman."
    },
    {
        "question": "Why did Tay become offensive and abusive according to the text?",
        "answer": "Tay became offensive and abusive because it started reflecting what was already in Twitter, including conspiracy theories, Nazi propaganda, harassment towards women, racism, and misogyny."
    },
    {
        "question": "Why did Microsoft have to take Tay down after only 16 hours?",
        "answer": "Microsoft had to take Tay down after only 16 hours because it was learning and spouting misogynistic, racist, and inflammatory tweets posted by users on Twitter."
    },
    {
        "question": "Where did researchers find biases and hate speech when applying hate speech and bias detectors on training data sets for dialogue systems?",
        "answer": "Researchers found biases and hate speech in Twitter, Reddit, and other dialogue data sets."
    },
    {
        "question": "What issue arises when dialogue models are trained on data that includes hate speech and bias?",
        "answer": "The dialogue models trained on this data also include hate speech and bias."
    },
    {
        "question": "What is the most frequent content word in the British National corpus that is not a function word?",
        "answer": "fuck"
    },
    {
        "question": "Why might using a certain word frequently be unacceptable in certain applications like conversational agents or chatbots?",
        "answer": "Using a certain word frequently may be unacceptable in certain applications like conversational agents or chatbots because it is a very frequently used word in real British English, which may not be suitable for all contexts."
    },
    {
        "question": "What does the speaker mention about the transcripts of telephone conversations?",
        "answer": "The speaker mentions that they have read transcripts of conversations that include instances of people giving out their credit card numbers."
    },
    {
        "question": "Why is it important to preserve privacy in the training data for dialogue systems?",
        "answer": "It is important to preserve privacy in the training data for dialogue systems to prevent intentional information leakage, such as asking users for their credit card numbers."
    },
    {
        "question": "What are some of the differences between human conversations and interactions with chatbots?",
        "answer": "Human conversations are much more complicated than interactions with chatbots. In human conversations, there are turn-taking dynamics, various types of turns, and dialogue acts."
    },
    {
        "question": "What are the two general sorts of architectures mentioned in the text?",
        "answer": "Rule-based systems or machine learning systems"
    },
    {
        "question": "What are the two main ways in which a corpus-based chatbot can learn from a dialogue corpus?",
        "answer": "The two main ways are instance-based learning and building a model using a neural network."
    },
    {
        "question": "What is a key consideration when evaluating chatbots according to the text?",
        "answer": "Ethical issues"
    },
    {
        "question": "What caution does the text mention about the behavior of chatbots?",
        "answer": "The text mentions being careful that chatbots don't start spouting Nazi propaganda or giving out personal information."
    },
    {
        "question": "What is the speaker expressing gratitude for?",
        "answer": "The speaker is expressing gratitude for the audience listening."
    },
    {
        "question": "Who is the lecturer mentioned in the text?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "What is the standard term for a numerical vector representation of word meaning, derived from a corpus?",
        "answer": "Word embedding"
    },
    {
        "question": "What type of data sets were a lot of early research focused on?",
        "answer": "Small data sets"
    },
    {
        "question": "Who conducted experiments on different machine learning algorithms to see how they scaled from very small to very large corpora?",
        "answer": "Michelle Banko and Eric Brill from Microsoft Research"
    },
    {
        "question": "Who authored the groundbreaking paper on efficient estimation of word representations in vector space in 2013?",
        "answer": "Thomas McAuliffe and his group from Google Research"
    },
    {
        "question": "Where do researchers write up their findings after conducting experiments in the field of AI research?",
        "answer": "Researchers write up their findings in a paper after conducting experiments in the field of AI research."
    },
    {
        "question": "Where is a research paper usually presented if it is accepted by experts?",
        "answer": "A research paper is usually presented at a conference if it is accepted by experts."
    },
    {
        "question": "What is the purpose of a journal in relation to a conference?",
        "answer": "A journal is a separate publication that contains longer papers with more details, unlike a conference."
    },
    {
        "question": "What are some events mentioned in the text that one can attend either online or in person?",
        "answer": "Conferences"
    },
    {
        "question": "What is the ACL Anthology and where can it be found?",
        "answer": "The ACL Anthology is a collection of papers that can be searched and found online."
    },
    {
        "question": "What is the plural form of the word 'corpus' in Latin?",
        "answer": "The plural form of the word 'corpus' in Latin is 'corpora' or 'core-pore-ah'."
    },
    {
        "question": "What is the English plural form of the word 'corpus'?",
        "answer": "corpuses"
    },
    {
        "question": "Why has the speaker been asked to read AI research papers?",
        "answer": "One reason is to learn about AI research."
    },
    {
        "question": "What is the difference between finding out about current research and finding the latest research?",
        "answer": "Finding out about current research can be done by googling topics like word embeddings, while finding the latest research involves looking for peer-reviewed articles that have been checked by other experts before publication."
    },
    {
        "question": "What type of assignment has the speaker been asking the listener to complete?",
        "answer": "The speaker has been asking the listener to write a research proposal for coursework."
    },
    {
        "question": "What is one suggestion mentioned in the text for what to do if you find a research paper that you like?",
        "answer": "One suggestion mentioned is to write a proposal based on the research paper, but maybe using a different data set, for a different company, or for a different user group."
    },
    {
        "question": "What are some common ways in which research can be related to previous research?",
        "answer": "Changing some parameters such as using it for a different language, a different user group, or a different but similar task."
    },
    {
        "question": "What is typically missing from a paper that is included in a proposal?",
        "answer": "Durations of work packages and results"
    },
    {
        "question": "What is the main purpose of presenting interesting, new results and conclusions in a research paper?",
        "answer": "The main purpose is to contribute to knowledge and explain why the findings are important and worth reading by others."
    },
    {
        "question": "What is one way of adding to a research proposal mentioned in the text?",
        "answer": "One way of adding to a research proposal is to do some pilot study or proof of concept study."
    },
    {
        "question": "Who is the author of the paper 'A Parsing Expert System which learns from Corpus Analysis'?",
        "answer": "Eric Atwell"
    },
    {
        "question": "Where was the paper presented and in what year?",
        "answer": "The paper was presented at the ICAME conference in 1986."
    },
    {
        "question": "What was the size of the small corpus used in the study?",
        "answer": "200,000 words or 1/5 of the Lancaster-Oslo-Bergen corpus of a million words"
    },
    {
        "question": "What is compared in order to determine if two words belong to the same word class?",
        "answer": "The context lists of the words"
    },
    {
        "question": "What is the main idea behind merging word embeddings into one joint word class?",
        "answer": "The main idea is to merge word embeddings that are very similar into one joint word class."
    },
    {
        "question": "Why could only the most frequent words be dealt with in the text analytics process described?",
        "answer": "Only the most frequent words could be dealt with because the processes at the time could only cope with 200,000 words and only 175 words occurred more than 100 times."
    },
    {
        "question": "What was the threshold set at for merging similar words together?",
        "answer": "0.8"
    },
    {
        "question": "What words were merged into a single word class based on the immediate lexical context in the text?",
        "answer": "\"will,\" \"should,\" \"could,\" \"must,\" \"may,\" and \"might\""
    },
    {
        "question": "What type of words were the majority of the 175 words mentioned in the text?",
        "answer": "Functional words"
    },
    {
        "question": "By what year was it possible to deal with much bigger data sets?",
        "answer": "By the year 2000, 2001"
    },
    {
        "question": "Where did Michelle Banko and Eric Brill work, and what advantage did they have in terms of computers?",
        "answer": "Michelle Banko and Eric Brill worked at Microsoft Research Labs and had very powerful computers, much better than what was available at universities."
    },
    {
        "question": "What was the main goal of Banko and Brill's evaluation?",
        "answer": "To evaluate the performance of different learning methods trained on orders of magnitude more label data."
    },
    {
        "question": "What are the two different spellings of the word 'weather' mentioned in the text?",
        "answer": "W-E-A-T-H-E-R and W-H-E-T-H-E-R"
    },
    {
        "question": "What is the benefit of labelled training data in the context of the problem described?",
        "answer": "The benefit is that labelled training data is essentially free, as you can go through a corpus and replace certain words with alternatives."
    },
    {
        "question": "What is the writer's criteria for determining the correct class in the text?",
        "answer": "The writer's criteria for determining the correct class is to assume that the actual text found is the correct class."
    },
    {
        "question": "What was the overall conclusion drawn from evaluating a range of different classifiers on different data set sizes?",
        "answer": "The more data you get, the better the systems are, the higher the accuracy."
    },
    {
        "question": "What is the practical conclusion for text analytics researchers mentioned in the text?",
        "answer": "The practical conclusion is that text analytics researchers should reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development."
    },
    {
        "question": "What is suggested as a more effective approach than developing new machine learning algorithms?",
        "answer": "Giving more training data is suggested as a more effective approach than tweaking the algorithm."
    },
    {
        "question": "What suggestion is given to researchers based on the results mentioned in the text?",
        "answer": "Researchers should spend more time collecting a corpus and labelling the corpus."
    },
    {
        "question": "What happened to the accuracy of the classifiers as the number of words increased?",
        "answer": "The accuracy of the classifiers increased as the number of words increased."
    },
    {
        "question": "Which classifier was ranked as the worst for a small amount of data but became the best for a much larger amount of data?",
        "answer": "Winnow"
    },
    {
        "question": "What factor determines which classifier is the best according to the text?",
        "answer": "The amount of training data you have."
    },
    {
        "question": "What does the text suggest is more important than tweaking algorithms for machine learning researchers in computational linguistics?",
        "answer": "Having a huge corpus and piling up more and more training data"
    },
    {
        "question": "What is one of the problems mentioned in the text regarding training data for the task?",
        "answer": "The larger the data set, the larger the model that you're learning."
    },
    {
        "question": "What does the size of the internal model refer to in the context of the Winnow algorithm and the memory-based algorithm?",
        "answer": "The size of the internal model refers to how much processor and memory you need to store the model and to compute with the model."
    },
    {
        "question": "What is the general trend in terms of memory and processor requirements as the number of words in a corpus increases?",
        "answer": "The general trend is that as the number of words in a corpus increases, more memory and processor power are required."
    },
    {
        "question": "What limitation do university researchers face when using Sketch Engine?",
        "answer": "University researchers are limited to having a million-word corpus when using Sketch Engine."
    },
    {
        "question": "What are students restricted to when it comes to coursework exercises in the text?",
        "answer": "Students are restricted to memory and processor for coursework exercises."
    },
    {
        "question": "What is an ensemble in machine learning?",
        "answer": "An ensemble in machine learning is when multiple classifiers or algorithms are used together, and the final prediction is made based on a 'voting' system."
    },
    {
        "question": "In what scenario does the voting classifier work well according to the text?",
        "answer": "For small amounts of data and for medium sizes of data"
    },
    {
        "question": "According to the text, when is an ensemble generally better than an individual classifier?",
        "answer": "An ensemble is generally better than an individual classifier, unless you have a very large training set and you know in advance which one is best for that set."
    },
    {
        "question": "What strategy can be used to generate training data for a nice problem or task?",
        "answer": "Replacing every ambiguous word with its other possibilities"
    },
    {
        "question": "What is one possible solution or strategy mentioned in the text for dealing with cases where several different classifiers are trained on a small amount of data?",
        "answer": "One possible solution mentioned is active learning, where the ensemble of classifiers is used to label a large amount of data and identify the most uncertain instances."
    },
    {
        "question": "What does active learning involve in terms of choosing instances to label?",
        "answer": "Active learning involves choosing carefully which instances to label, specifically focusing on instances that the classifiers are not sure about."
    },
    {
        "question": "What does the author suggest as a method for automatic labelling in instances with the highest probability of being correct?",
        "answer": "The author suggests using semi-supervised learning."
    },
    {
        "question": "What happens if you have an automatic classifier trained on a small data set and then continue to use it on a growing data set?",
        "answer": "It will choose most classifiers and give a probability that the label is correct."
    },
    {
        "question": "What does it mean when all the different classifiers in an ensemble vote for the same option?",
        "answer": "It means that you're quite sure about that option."
    },
    {
        "question": "What is one of the conclusions the researchers came to regarding training collections and learning techniques?",
        "answer": "One conclusion the researchers came to is that efforts should be directed towards increasing the size of annotated training collections and de-emphasizing the focus on comparing different learning techniques trained only on a small training corpus."
    },
    {
        "question": "What is one of the recommended steps for training a classifier according to the text?",
        "answer": "Collecting a large training corpus and annotating it for the particular task"
    },
    {
        "question": "What was the main challenge in learning very large word representations in vector space?",
        "answer": "Dealing with very large corpora of 1,000 million words made it difficult to learn very large word representations in vector space."
    },
    {
        "question": "What were the researchers trying to come up with in addition to the model?",
        "answer": "A comprehensive test set for measuring syntactic and semantic regularities"
    },
    {
        "question": "What are the two methods mentioned for training and giving each word a vector representing its meaning from the corpus?",
        "answer": "Continuous Bag of Words (CBOW) and continuous SKIP-GRAM (or just SKIP-GRAM)"
    },
    {
        "question": "Why were the researchers able to analyze 1,000 million words instead of just a million words?",
        "answer": "The researchers were able to analyze 1,000 million words because the models they used had much lower computational complexity than feedforward or recurrent neural networks."
    },
    {
        "question": "What did the researchers demonstrate about their methods compared to other people's methods in the test set?",
        "answer": "The researchers demonstrated that their methods had much better scores compared to other people's methods."
    },
    {
        "question": "What does the CBOW model predict based on the previous contexts?",
        "answer": "The CBOW model predicts the current word based on the previous contexts, including the word before, two spaces before, the word after, and two words after."
    },
    {
        "question": "What does the current word in the text aim to predict?",
        "answer": "The current word aims to predict the surrounding words or its context."
    },
    {
        "question": "What are the models mentioned in the text used for?",
        "answer": "The models mentioned in the text are used for generating vector representations of the meanings of words."
    },
    {
        "question": "How do previous papers typically evaluate the quality of different versions of word vectors?",
        "answer": "Previous papers typically use a table showing some example words and their most similar words."
    },
    {
        "question": "How many types of semantic relations were included in the comprehensive test set?",
        "answer": "Five types of semantic relations were included in the comprehensive test set."
    },
    {
        "question": "What is the condition for a question to be assumed correctly answered in the tests mentioned in the text?",
        "answer": "The question is assumed to be correctly answered only if the closest word to the vector computed using this method is exactly the same as the correct word in the question."
    },
    {
        "question": "What is an example of a relation mentioned in the text?",
        "answer": "Athens is related to Greece in terms of being a capital city, or Oslo is related to Norway in terms of being a capital city."
    },
    {
        "question": "What are the two types of relationships mentioned in the text?",
        "answer": "The two types of relationships mentioned in the text are semantic relations and syntactic relations."
    },
    {
        "question": "What type of relationships are discussed in the text?",
        "answer": "Syntactic relationships"
    },
    {
        "question": "What was the accuracy score of Mikolov's own recursive neural network system on the semantics?",
        "answer": "8.6%"
    },
    {
        "question": "What type of models did the researchers try after testing other systems?",
        "answer": "Their own CBOW and SKIP-GRAM models"
    },
    {
        "question": "What is one advantage of using a very large training set for a neural network language model?",
        "answer": "Using a very large training set for a neural network model gives better results compared to smaller training sets."
    },
    {
        "question": "What analogy was being tested with the example of France to Paris and Italy?",
        "answer": "France is to Paris as Italy is to Rome"
    },
    {
        "question": "According to the text, what is the result of subtracting the vector for France from the vector for Paris and then adding the vector for Italy?",
        "answer": "The result is a vector that is most similar to the vector for Rome."
    },
    {
        "question": "What is the relationship between Microsoft and Windows similar to the relationship between Google and?",
        "answer": "Android"
    },
    {
        "question": "What comparison does the speaker make between Japan and sushi, and France and tapas?",
        "answer": "The speaker compares Japan to sushi and France to tapas, but points out that tapas is a food from Spain, not France."
    },
    {
        "question": "What is the conclusion regarding training high quality word vectors using simple model architectures?",
        "answer": "The conclusion is that it is possible to train high quality word vectors using simple model architectures."
    },
    {
        "question": "According to the text, why was SKIP-GRAM considered better?",
        "answer": "SKIP-GRAM was considered better because it can cope with much larger data sets."
    },
    {
        "question": "Why is it important to have a comprehensive test set with lots of examples for evaluating systems?",
        "answer": "It is important to have a comprehensive test set with lots of examples for evaluating systems because it ensures that systems are tested and scored accurately, rather than just assuming they are good based on a few examples."
    },
    {
        "question": "What type of representations do NLP systems work on, according to the text?",
        "answer": "NLP systems work on vector representations of words extracted using something like word2vec."
    },
    {
        "question": "What is the purpose of constructing a vocabulary and learning vector representations for words in the training text data?",
        "answer": "The resulting word vector file can be used as features in natural language processing and machine learning applications."
    },
    {
        "question": "What is one way to investigate the learned representations in word2vec?",
        "answer": "A simple way to investigate the learned representations is to find the closest words for a user specified word."
    },
    {
        "question": "What can you find on the Google code archive website besides Google's own training corpora?",
        "answer": "Pointers to where you can get large training corpora from lots of different sources"
    },
    {
        "question": "What is the purpose of learning about word embeddings according to the text?",
        "answer": "To learn about numerical vector representations of word meanings"
    },
    {
        "question": "What does the author recommend the reader to do after sharing the main findings?",
        "answer": "The author recommends the reader to go away and read the findings for themselves to get a better understanding and an idea of what a research paper is like."
    },
    {
        "question": "What did Banko and Brill at Microsoft Research Labs show in their experiments in 2001?",
        "answer": "They showed that the more data you have, the better your classifier will perform, leading to higher accuracy for all classifiers."
    },
    {
        "question": "What are the two different algorithms displayed by Mikolov and his colleagues at Google Research Labs for efficient estimation of word representations?",
        "answer": "Continuous Bag of Words and SKIP-GRAM models"
    },
    {
        "question": "Who is the researcher at the Natural Language Processing Group at University of Sheffield mentioned in the text?",
        "answer": "Diana Maynard"
    },
    {
        "question": "What is the topic of discussion in the text?",
        "answer": "Information extraction and some applications of information extraction"
    },
    {
        "question": "What are the two different approaches mentioned for building information extraction systems?",
        "answer": "Machine learning approach and knowledge engineering approach"
    },
    {
        "question": "What is the example of a rule-based named entity recognition system mentioned in the text?",
        "answer": "MUSE, MUlti-Source Entity recognition system"
    },
    {
        "question": "What is an example of information retrieval mentioned in the text?",
        "answer": "A Google search"
    },
    {
        "question": "What is the difference between information retrieval theory and information extraction?",
        "answer": "Information retrieval theory focuses on reading or analyzing documents to find answers, while information extraction pulls facts and structured information from documents."
    },
    {
        "question": "Why would you want to extract named entities and relations from a large text collection?",
        "answer": "To analyze the facts without having to read all the documents and to have the ability to check back on the original documents."
    },
    {
        "question": "What can be difficult with traditional information retrieval query engines?",
        "answer": "Getting out the facts"
    },
    {
        "question": "What are some examples of search terms mentioned in the text for finding out where the queen has visited or places with cases of West Nile virus?",
        "answer": "Some examples of search terms mentioned in the text are 'queen, visiting, 2021' and 'East Coast of the United States, West Nile virus.'"
    },
    {
        "question": "What is another name for the queen of England mentioned in the text?",
        "answer": "Elizabeth or Queen Elizabeth"
    },
    {
        "question": "What is one advantage of using information extraction over information retrieval?",
        "answer": "Information extraction returns knowledge at a deeper level and can pull out entities and relations in a more structured way."
    },
    {
        "question": "Why is it mentioned that even if the results of information extraction aren't always accurate, they can still be valuable?",
        "answer": "Because you can still check them in the original text."
    },
    {
        "question": "What is one example of how the text suggests the usefulness of identifying major events and relationships in news stories?",
        "answer": "Finding out what companies are buying which other company"
    },
    {
        "question": "In what scientific areas is natural language processing particularly useful for research?",
        "answer": "Medicine, pharmacology, and genomics"
    },
    {
        "question": "What is the name of the Health and Safety Information Extraction System developed at Sheffield?",
        "answer": "HSE"
    },
    {
        "question": "What type of questions might the health and safety inspector ask when looking at documents at Leeds University?",
        "answer": "The health and safety inspector may ask about how many members of staff have died or had accidents in the last year."
    },
    {
        "question": "Why might information retrieval not be particularly helpful according to the text?",
        "answer": "Information retrieval might not be particularly helpful because even if it returns documents with information, the documents could still be quite large and require reading."
    },
    {
        "question": "What does HSE do in relation to health and safety issues?",
        "answer": "HSE identifies sentences about health and safety issues or entities, extracts them, and populates a database with the entities and the relationships between them."
    },
    {
        "question": "What is a kibbutz and where is it typically found?",
        "answer": "A kibbutz is a collective cooperative farm commune found in Israel."
    },
    {
        "question": "How did they populate the database with information about kibbutzes?",
        "answer": "They extracted anything related to kibbutzes from newspaper stories and put them into the database."
    },
    {
        "question": "What type of patterns can you search for in the text?",
        "answer": "Patterns where x is a person involved in an event related to a kibbutz attack in Israel."
    },
    {
        "question": "What type of individuals does the speaker want to find examples of in the text?",
        "answer": "The speaker wants to find persons involved in kibbutz attacks in Israel."
    },
    {
        "question": "What is the purpose of the query language in the context of information extraction?",
        "answer": "The query language is the interface to the information extraction database."
    },
    {
        "question": "What did Alias-i develop during the time of the Americans invading Iraq?",
        "answer": "Alias-i developed something called the Threat Tracker."
    },
    {
        "question": "What is the main focus of a threat tracker as described in the text?",
        "answer": "The main focus of a threat tracker is tracking threats within text, not using vision or image processing."
    },
    {
        "question": "What method did the Americans use to help soldiers recognize the terrorists they wanted to catch in Iraq?",
        "answer": "The Americans gave every soldier a pack of cards with a photo of a terrorist on the back of each card."
    },
    {
        "question": "What was Huda Salih Mahdi Ammash also known as?",
        "answer": "Mrs. Anthrax"
    },
    {
        "question": "What is named entity recognition and what does it involve?",
        "answer": "Named entity recognition is the identification of proper names and other references to entities in text, and their classification into a set of categories of interest."
    },
    {
        "question": "Why is named entity recognition important?",
        "answer": "Named entity recognition is important as a foundation to build more complex information extraction."
    },
    {
        "question": "What is the first step in finding out relations between named entities?",
        "answer": "Identifying the entities"
    },
    {
        "question": "What are some examples of ontology relations mentioned in the text?",
        "answer": "Persons, events, and countries are all types of entities within an ontology and they're related in various ways."
    },
    {
        "question": "What is one requirement for machine learning according to the text?",
        "answer": "It requires a large amount of training data annotated with the targets."
    },
    {
        "question": "What type of models require a lot of computing processor and memory according to the text?",
        "answer": "Machine learning models, particularly deep learning models"
    },
    {
        "question": "What are some of the disadvantages of machine learning mentioned in the text?",
        "answer": "Some of the disadvantages of machine learning mentioned in the text are the need for a lot of compute power, as well as a significant effort required to collect and annotate data."
    },
    {
        "question": "What approach was more popular before powerful machine learning became feasible?",
        "answer": "The knowledge engineering approach"
    },
    {
        "question": "What was mentioned as a possible project for a PhD at Helsinki University related to part of speech tagging?",
        "answer": "Developing a rule-based system for part of speech tagging in a language of choice"
    },
    {
        "question": "What is emphasized as a requirement for understanding the task mentioned in the text?",
        "answer": "Being experienced in the topic area and having human intuition are emphasized as requirements for understanding the task."
    },
    {
        "question": "What is mentioned as a potential drawback of information extraction that may occur even when high-performance computing is not required?",
        "answer": "Development time can be time-consuming."
    },
    {
        "question": "What is one example of a situation where you may need to change and check all the rules you've established so far?",
        "answer": "If you want to separate out industry companies from non-profit companies"
    },
    {
        "question": "What is the real problem in all language analysis according to the text?",
        "answer": "The real problem in all language analysis is ambiguity."
    },
    {
        "question": "What are the two types of ambiguity mentioned in the text?",
        "answer": "The two types of ambiguity mentioned are having different ways of referring to the same entity and one name referring to several different entities."
    },
    {
        "question": "What is the name of the company in Yorkshire mentioned in the text?",
        "answer": "John Smith Brewery"
    },
    {
        "question": "Who is the famous American mentioned in the text?",
        "answer": "Denzel Washington"
    },
    {
        "question": "What examples of ambiguity are mentioned in the text?",
        "answer": "The examples of ambiguity mentioned in the text are the year 1945, which could refer to the end of the Second World War or quarter to 8:00 in the evening, and the word 'location' or 'person'."
    },
    {
        "question": "What does the author suggest about the importance of capitalization in understanding the meaning of words?",
        "answer": "The author suggests that capitalization can provide additional information about the meaning of words, as lowercase and uppercase letters can change the interpretation of a word."
    },
    {
        "question": "What are some of the issues to take into account when it comes to text?",
        "answer": "Punctuation, spelling, spacing, and formatting"
    },
    {
        "question": "What does the named entity system have to recognize in the given text?",
        "answer": "The named entity system has to recognize that the address of the computing department is a single entity, hierarchical, and part of Manchester Metropolitan University within Manchester, which is part of the United Kingdom."
    },
    {
        "question": "What clues can help a human determine that a greater-than sign in a text may indicate a repeat of a previous email?",
        "answer": "Additional information from the punctuation or the layout can help determine that a greater-than sign in a text may indicate a repeat of a previous email."
    },
    {
        "question": "What is the simplest baseline approach mentioned in the text for recognizing entities?",
        "answer": "The simplest baseline approach mentioned in the text is to have lists, where the system recognizes entities stored in its lists, called gazetteers."
    },
    {
        "question": "What is one disadvantage of using a new list for a new topic, language, or domain?",
        "answer": "One disadvantage is that you have to collect the lists in the first place and maintain them."
    },
    {
        "question": "What can you do on top of just having lists or gazetteers of pharmaceutical entities for pharmaceutical named entity recognition?",
        "answer": "You can address the ambiguity problem by considering context or using additional techniques."
    },
    {
        "question": "What is a possible indicator that a word is a named entity and a geographical location?",
        "answer": "If the word starts with a capitalized letter followed by City, Forest, Centre, or River."
    },
    {
        "question": "What is the significance of Nottingham Forest in the text?",
        "answer": "Nottingham Forest is the name of a football club, although there is no actual forest left near Nottingham."
    },
    {
        "question": "What is Quality Street and when is it commonly eaten?",
        "answer": "Quality Street is a type of chocolate sweets and it is commonly eaten around Christmas."
    },
    {
        "question": "Which languages mentioned in the text do not have capital letters?",
        "answer": "Arabic and Hindi"
    },
    {
        "question": "Why is it not always the case that a word starting with a capital letter is significant as a name in English?",
        "answer": "Because the first word of every sentence has to start with a capital letter."
    },
    {
        "question": "What is the difference between a named entity like 'All-American bank' and a non-named entity like 'All state police'?",
        "answer": "A named entity like 'All-American bank' is a specific entity, while a non-named entity like 'All state police' refers to a general entity."
    },
    {
        "question": "What is an example of a company name that includes words like 'and'?",
        "answer": "Cable and Wireless"
    },
    {
        "question": "According to the text, what is the difference between 'City Hospital for John Smith' and 'Center for Computational Linguistics'?",
        "answer": "The difference is that 'City Hospital for John Smith' consists of two entities ('City Hospital' and 'John Smith'), while 'Center for Computational Linguistics' is considered as one entity."
    },
    {
        "question": "What does the phrase 'David Walton of Goldman Sachs' imply?",
        "answer": "It implies that David Walton belongs to the company Goldman Sachs."
    },
    {
        "question": "What tool can be used to identify semantic patterns according to the text?",
        "answer": "A quick concordance"
    },
    {
        "question": "What type of patterns can be extracted by searching using a concordance according to the text?",
        "answer": "The text suggests that patterns such as a person followed by the word 'owns' followed by another entity can be extracted by searching using a concordance."
    },
    {
        "question": "What does the text suggest about the relationship between 'x' and 'y' in the phrase 'x joined y'?",
        "answer": "The text suggests that 'x' is likely to be a person and 'y' is likely to be an organization."
    },
    {
        "question": "What does the semantic pattern 'person earns money' help identify?",
        "answer": "The semantic pattern helps identify 'John Smith' as a person, '15 pounds' as a money entity, and the relationship between them as earning."
    },
    {
        "question": "What is one difference between the knowledge engineering approach and the machine learning approach mentioned in the text?",
        "answer": "One difference is that in the knowledge engineering approach, you do not need hundreds of examples of each type, whereas in machine learning you do."
    },
    {
        "question": "What is one of the tasks that needs to be done before learning from a corpus?",
        "answer": "Manually marking up the named entities, the types of the entities, and the relationship between the entities."
    },
    {
        "question": "What system was built at Sheffield University using GATE?",
        "answer": "The multi-source entity recognition system was built at Sheffield University using GATE."
    },
    {
        "question": "What approach does the MUSE system use for named entity recognition and code reference?",
        "answer": "The MUSE system uses a knowledge engineering approach with hand-crafted rules."
    },
    {
        "question": "What are some of the advantages of hand-crafted systems mentioned in the text?",
        "answer": "One advantage is the ability to build rule-based systems for different modules separately, such as formatting, tokenization, and sentence-splitting."
    },
    {
        "question": "What is the purpose of a gazetteer lookup in natural language processing?",
        "answer": "The purpose of a gazetteer lookup is to look up each word in a dictionary to see if it's a named entity or not."
    },
    {
        "question": "What is coreference and how does it work with names and pronouns?",
        "answer": "Coreference is when two or more words or phrases refer to the same entity. In the text, 'Mr. Smith' and 'John' are examples of coreference with names, while 'he' and 'John' are examples of coreference with pronouns."
    },
    {
        "question": "Which languages were initially developed for the adaptation mentioned in the text?",
        "answer": "French, German, Romanian, Bulgarian, and Russian"
    },
    {
        "question": "What is mentioned as an example of a language in the text that the speaker does not know?",
        "answer": "Cebuano"
    },
    {
        "question": "What alphabet is used for the language mentioned in the text?",
        "answer": "The Latin or Roman alphabet, the same as English, French, or German."
    },
    {
        "question": "What challenges did the developers face when trying to develop Hindi as an example?",
        "answer": "The challenges included having a completely different alphabet, different character encodings, absence of capital letters, and words having spaces between them."
    },
    {
        "question": "What makes Hindi a medium difficulty language for adapting MUSE to other languages?",
        "answer": "The availability of lots of resources due to India being a big country with lots of computing research going on."
    },
    {
        "question": "What are some challenges mentioned in the text regarding the development of scripts and character sets?",
        "answer": "The challenges mentioned include the need for a lot of support for other scripts, other character sets, and the time-consuming task of dealing with different character strings."
    },
    {
        "question": "What resources does the text suggest obtaining for language learning and evaluation?",
        "answer": "The text suggests obtaining an English to the other language dictionary, a bilingual dictionary comparing the language to a major European language, and a corpus for evaluation."
    },
    {
        "question": "Why is it necessary to annotate at least a small test corpus for evaluation in machine learning?",
        "answer": "It is necessary to annotate a small test corpus for evaluation in order to compare the results of the machine learning model against the actual truth provided by human annotations."
    },
    {
        "question": "What are some resources mentioned in the text that can be found on the internet?",
        "answer": "Phone books, Yellow Pages, and other resources"
    },
    {
        "question": "What did the developers create on top of the core Java resources to help with dealing with different character sets?",
        "answer": "The developers created a GATE Unicode kit."
    },
    {
        "question": "What colors are used to represent different entities in the output?",
        "answer": "Green for location, blue for persons, and purple for organization"
    },
    {
        "question": "What is the importance of having a common, very large, standard data test set in information extraction?",
        "answer": "The importance of having a common, very large, standard data test set in information extraction is to allow other people to use it for testing and evaluation purposes."
    },
    {
        "question": "What is one very important thing mentioned in the text related to information extraction?",
        "answer": "Extraction evaluation sets"
    },
    {
        "question": "What are some areas where there has been a lot of funding for information extraction?",
        "answer": "COVID sources, COVID cures, finance industry"
    },
    {
        "question": "What is one thing apart from finding entities that is mentioned as important in order to answer questions?",
        "answer": "Finding the relations between the entities"
    },
    {
        "question": "What topic does the Jurafsky and Martin textbook chapter mentioned in the text cover?",
        "answer": "The chapter covers information extraction and relation extraction algorithms."
    },
    {
        "question": "What is the difference between information extraction and information retrieval?",
        "answer": "Information extraction involves extracting named entities and relations from text, while information retrieval involves finding the documents."
    },
    {
        "question": "What is the MUSE system and where was it developed?",
        "answer": "The MUSE system is the MUlti-Source Entity Recognition System developed at Sheffield University by the Natural Language Processing Research Group."
    },
    {
        "question": "Where can you find more information about ongoing research and machine learning research for information extraction?",
        "answer": "In the Jurafsky and Martin textbook chapter"
    },
    {
        "question": "What is commonly thought of as information retrieval according to Professor Eric Atwell?",
        "answer": "Google search or web search"
    },
    {
        "question": "What will the lecture compare information retrieval with?",
        "answer": "The lecture will compare information retrieval with database querying using something like SQL, or structured query language."
    },
    {
        "question": "What are the two models mentioned in the text for evaluating search tools?",
        "answer": "The two models mentioned are a Boolean set theoretic model and a weighted vector model."
    },
    {
        "question": "What is one way to improve matching and get more of what you want in database querying?",
        "answer": "One way to improve matching and get more of what you want in database querying is to do query broadening."
    },
    {
        "question": "What is a simple approach to searching for a particular keyword or string of characters?",
        "answer": "A simple approach is to look for the string with wild cards before and after the keyword."
    },
    {
        "question": "Why is brute force scanning through all the text data records considered inefficient?",
        "answer": "Brute force scanning through all the text data records is considered inefficient because it means going through all the records until finding the desired information, which can be time-consuming and resource-intensive."
    },
    {
        "question": "What is the suggested approach for handling information from the University of Leeds module catalogue?",
        "answer": "The suggested approach is to extract named entities and keywords effectively, and hold them in a relational database."
    },
    {
        "question": "What information would the modules have according to the text?",
        "answer": "The modules would have information like the module code, the title, and semester."
    },
    {
        "question": "What does the speaker suggest to do if you want a course that includes database, AI, and knowledge base?",
        "answer": "Change the OR's to AND's in the query"
    },
    {
        "question": "According to the text, why doesn't the SQL statement work?",
        "answer": "The SQL statement doesn't work because it requires the T-value to be both 'database' and 'AI' at the same time, which is not possible as T-value can only have one value."
    },
    {
        "question": "What is the process described in the text for finding records containing multiple keywords?",
        "answer": "The process involves having two T's, T1 and T2, where T1 represents 'database' and T2 represents 'AI'. Then, one needs to find all the records or fields containing both T1 and T2."
    },
    {
        "question": "Why does the text suggest that database querying using SQL isn't a sensible way for information retrieval?",
        "answer": "The text suggests that database querying using SQL isn't a sensible way for information retrieval because it gets much more complicated for 'and' than it is for 'or', and in information retrieval, you typically have several keywords and want to find all the documents which have all of them, not just a document which is any one of them."
    },
    {
        "question": "What is the main difference between the non-database structure mentioned in the text and a standard SQL database?",
        "answer": "The main difference is that the non-database structure does not store data in records and fields, making it unsuitable for standard SQL."
    },
    {
        "question": "What is the basic idea of an inverted file in information retrieval systems?",
        "answer": "The basic idea of an inverted file is to associate terms with the documents they appear in."
    },
    {
        "question": "What is the notion of inverting in the context of the text?",
        "answer": "The notion of inverting refers to wanting a list of documents that each term contains."
    },
    {
        "question": "What is the purpose of pre-processing the set of documents mentioned in the text?",
        "answer": "The purpose of pre-processing the set of documents is to extract a dictionary of all the words that are in all the documents and determine which documents each word came from."
    },
    {
        "question": "What term is being discussed in the text and how many examples of it are mentioned?",
        "answer": "The term being discussed is 'a' and there are two examples of it mentioned."
    },
    {
        "question": "What term is being searched for in document 2?",
        "answer": "\"and\""
    },
    {
        "question": "How many times does the word 'and' appear in documents 1, 2, and 3?",
        "answer": "The word 'and' appears three times in documents 1, 2, and 3."
    },
    {
        "question": "In which document does the term 'aardvark' appear according to the text?",
        "answer": "Document number 2"
    },
    {
        "question": "How many times did the term 'aardvark' occur in the text?",
        "answer": "Once"
    },
    {
        "question": "What additional information can be included in the inverted files besides pointers to objects?",
        "answer": "The inverted files may also contain other information like positional information and term frequency."
    },
    {
        "question": "What kind of pudding does the speaker want to make on a cold day?",
        "answer": "The speaker wants to make jam pudding or treacle pudding."
    },
    {
        "question": "What does the speaker want to find in the documents mentioned in the text?",
        "answer": "The speaker wants to find documents that have jam or treacle and pudding in them."
    },
    {
        "question": "What are the three options that are matching in the given text?",
        "answer": "A and C, B and C, A and B and C"
    },
    {
        "question": "What is the process described for finding a match in the text?",
        "answer": "To find a match, one must try the disjunctive normal form against a list of three lists: one containing 'a', one containing 'b', and one containing 'c'. The match is found if the criteria are met, such as being in one document and not in another."
    },
    {
        "question": "In which document is the profile 110 found?",
        "answer": "Document 3"
    },
    {
        "question": "What is the profile of document 7?",
        "answer": "101"
    },
    {
        "question": "According to the text, which documents match the pattern 101?",
        "answer": "Document 1 and Document 4"
    },
    {
        "question": "How many documents have jam pudding and treacle pudding according to the text?",
        "answer": "Documents 1 and 4 have jam pudding, while documents 2 and 12 have treacle pudding."
    },
    {
        "question": "What is emphasized as important in the text regarding documents?",
        "answer": "Weighted weights are emphasized as important in the text regarding documents."
    },
    {
        "question": "What analogy does the speaker use to explain the concept of weighted words in a query?",
        "answer": "The speaker uses the analogy of expressing preferences for food items like jam, treacle, and pudding to explain the concept of weighted words in a query."
    },
    {
        "question": "What is the range of weights for all the words mentioned in the text?",
        "answer": "The range of weights for all the words mentioned in the text is from 0 to 1."
    },
    {
        "question": "What does the weight of a word in a document indicate in the context of data mining and text analytics?",
        "answer": "The weight of a word in a document indicates the importance of that word within the document. A higher weight suggests that the word is more important in that particular document."
    },
    {
        "question": "What is the concept mentioned in the text that involves the frequency of a word in a document compared to the frequency of the word in other documents?",
        "answer": "TFIDF (Term Frequency-Inverse Document Frequency)"
    },
    {
        "question": "What does the Jurafsky and Martin textbook chapter on question answering have a section on?",
        "answer": "information retrieval"
    },
    {
        "question": "What function is used to calculate the similarity between the query and document vectors in the given text?",
        "answer": "Cosine function"
    },
    {
        "question": "According to the text, which document is slightly better for the query, document 1 or document 2?",
        "answer": "Document 2"
    },
    {
        "question": "How does the system present the ranked list to the user?",
        "answer": "The system presents the ranked list to the user in rank order."
    },
    {
        "question": "What information is stored in the inverted file mentioned in the text?",
        "answer": "For each word, the inverted file stores a list of document URLs and the weights of the words within each of those documents."
    },
    {
        "question": "What information can help you search for 'Venetian blind' rather than 'blind Venetian'?",
        "answer": "Carrying information in the listing in the file or index about where the positions of the words are."
    },
    {
        "question": "What type of query does the author want to favor when searching for the phrase 'Venetian blind'?",
        "answer": "The author wants a query for 'Venetian blind' to favor documents which actually have the phrase 'Venetian blind' in it."
    },
    {
        "question": "What are some ways in which the inverted file can be used?",
        "answer": "The inverted file can be used for Booleans, weighted queries, and even positional queries."
    },
    {
        "question": "What is one of the tasks that Google has to do before launching a query?",
        "answer": "Google has to set up a huge index of all the documents in the world for all the vocabulary that appears in those documents."
    },
    {
        "question": "Why can it be expensive to update if the information objects change in the context of query processing?",
        "answer": "It can be expensive to update if the information objects change because if web pages change, then the entire index has to be recomputed all over again."
    },
    {
        "question": "What is the purpose of an inverted file index in information retrieval?",
        "answer": "An inverted file index is a standard system for information retrieval that is used to store the dictionary and the inverted file, which are necessary for extracting information from web pages."
    },
    {
        "question": "Why is the inverted file structure considered more efficient than relational databases for storing and querying data with multiple keywords?",
        "answer": "The inverted file structure is considered more efficient because it can store frequencies or weights in the dictionary to allow ordering of results, unlike SQL which struggles with expressing queries for finding records with multiple keywords."
    },
    {
        "question": "What does the text suggest about the ability to find subtle differences between similar terms using search engines?",
        "answer": "The text suggests that search engines like Google can help find subtle differences between similar terms, such as 'knowledge management' and 'management knowledge', by ranking search results in a certain order."
    },
    {
        "question": "What does the Google robot do when it goes out on the web?",
        "answer": "The Google robot goes out on the web finding new web pages, storing a copy of a document and a cut-down version of the document."
    },
    {
        "question": "What is used to keep the words in the document and a stored, sorted list of the words appearing in a document with links back to the full document?",
        "answer": "Dictionary and inverted file"
    },
    {
        "question": "What is the difference between the expectations of a database management system and information retrieval system?",
        "answer": "For a database management system, you expect to issue an SQL query and get exactly what you asked for. With information retrieval, you type some query into Google, and you get a partial or best match, and a rank ordering of matches."
    },
    {
        "question": "What is the inference mechanism in database management systems?",
        "answer": "Deduction"
    },
    {
        "question": "How does information retrieval differ from querying a database in terms of the consistency of results?",
        "answer": "Information retrieval is more probabilistic and may not give back the same query or responses each time, while querying a database gives back the same information every time."
    },
    {
        "question": "What is the internal structure of documents in information retrieval?",
        "answer": "We don't know anything about the internal structure of documents except that they contain text."
    },
    {
        "question": "How does Google handle questions in information retrieval systems?",
        "answer": "Google treats questions as questions rather than queries in information retrieval systems."
    },
    {
        "question": "In what way does the query specification differ between SQL and information retrieval?",
        "answer": "The query specification for SQL has to be exact and complete, while for information retrieval, it can be based on some keywords and may not need to be complete."
    },
    {
        "question": "What is the main difference between information retrieval and keyword matching?",
        "answer": "The main difference is that for information retrieval, the goal is to find something relevant to what's in your mind, not just a match to the keywords."
    },
    {
        "question": "What does the author suggest is important when searching for information online?",
        "answer": "The author suggests that relevance is dependent on what you actually want rather than what the exact match is."
    },
    {
        "question": "What is the difference between the sensitivity to errors in SQL queries and information retrieval queries?",
        "answer": "SQL queries are sensitive to errors, while information retrieval queries are relatively insensitive to errors."
    },
    {
        "question": "What is the main idea behind representing a document in data mining and text analytics?",
        "answer": "The main idea is to represent a document as a set of descriptors or index terms or words, and then search for the document in the space of these index terms or words."
    },
    {
        "question": "What is the general architecture described for matching queries with document descriptors or keywords?",
        "answer": "The general architecture involves the user typing in a query of keywords, the query matching looking at the object base (database of web pages and documents), finding matches, and returning hits."
    },
    {
        "question": "What is the author discussing when mentioning 'query broadening'?",
        "answer": "The author is discussing a learning component that allows users to give better hits and more of what they want and less of what they don't want."
    },
    {
        "question": "What are the three documents mentioned in the text?",
        "answer": "D1, D2, and D3"
    },
    {
        "question": "What are the interesting words mentioned in the text?",
        "answer": "The interesting words mentioned in the text are 'pudding', 'jam', 'traffic', 'lane', and 'treacle'."
    },
    {
        "question": "What does the first document contain and not contain according to the text?",
        "answer": "The first document contains pudding and jam, and does not contain traffic, lane, and treacle."
    },
    {
        "question": "What is the significance of Pudding Lane in the text?",
        "answer": "Pudding Lane is a road in London where the Great Fire of London started."
    },
    {
        "question": "What does the author point out about the representation of bigrams in the text?",
        "answer": "The author mentions that the simple representation in the text only captures unigrams and does not capture the idea that 'jam' should go with 'pudding' and not with 'traffic'."
    },
    {
        "question": "What is the goal in the Boolean model described in the text?",
        "answer": "The goal is to convert the query into disjunctive normal form."
    },
    {
        "question": "What does the binary code 11001 represent in the text?",
        "answer": "Pudding and jam"
    },
    {
        "question": "What is the score assigned to a document if it matches one of the query vectors 11000, 10001, or 11001?",
        "answer": "1"
    },
    {
        "question": "According to the text, does document 1 match the first component of the query?",
        "answer": "Yes, document 1 matches the first component of the query."
    },
    {
        "question": "What is the content of the first document mentioned in the text?",
        "answer": "It's a jam pudding recipe."
    },
    {
        "question": "What document number contains the jam pudding recipe?",
        "answer": "Document 1, 11000, contains the jam pudding recipe."
    },
    {
        "question": "What weight is assigned to the term 'pudding' in the vector described in the text?",
        "answer": "1"
    },
    {
        "question": "What is the weight assigned to treacle pudding in the text?",
        "answer": "0.8"
    },
    {
        "question": "How many terms are involved in the cosine coefficient mentioned in the text?",
        "answer": "Two terms"
    },
    {
        "question": "What does a cosine coefficient of 0 indicate in the context of comparing a document and a query?",
        "answer": "A cosine coefficient of 0 indicates that the weights of the words are in the same proportions for the query and the document, resulting in a perfect match."
    },
    {
        "question": "What angle is formed when a document has only 'T1' and the query has only 'T2' in it?",
        "answer": "90 degrees"
    },
    {
        "question": "What ingredients are mentioned in the jam pudding recipe?",
        "answer": "Pudding and jam"
    },
    {
        "question": "What ingredient can be used as a substitute for jam in the jam pudding recipe?",
        "answer": "Treacle"
    },
    {
        "question": "What is the sum of the multiplications mentioned in the text?",
        "answer": "1.44"
    },
    {
        "question": "What is the overall answer after calculating the sum of the query weight squared and putting the terms into the given equation?",
        "answer": "0.89"
    },
    {
        "question": "What is the score given to the jam pudding recipe in the text?",
        "answer": "0.89"
    },
    {
        "question": "What is the result of dividing 0 by the square root of blah, blah, blah, according to the text?",
        "answer": "0"
    },
    {
        "question": "What was the topic of the radio report mentioned in the text?",
        "answer": "Traffic jams in Pudding Lane"
    },
    {
        "question": "What is the result of the calculation described in the text?",
        "answer": "2.53"
    },
    {
        "question": "What score is obtained by dividing 1.14 by the square root of 2.53 and then multiplying by 2?",
        "answer": "0.51"
    },
    {
        "question": "What is the best hit according to the results collected?",
        "answer": "The best hit is the jam pudding recipe."
    },
    {
        "question": "What is a limitation of the Boolean model mentioned in the text?",
        "answer": "The Boolean model doesn't work well for very large web resources."
    },
    {
        "question": "Why do users find Boolean queries hard to formulate?",
        "answer": "Users find Boolean queries hard to formulate because they have to put AND's and OR's in, which makes it difficult."
    },
    {
        "question": "Why is the process described in the text popular with search engines?",
        "answer": "The process is popular with search engines because it allows ranking of output based on scores, even for imperfect documents."
    },
    {
        "question": "What are the only words mentioned in the very simple model discussed in the text?",
        "answer": "pudding, jam, traffic, lane, and treacle"
    },
    {
        "question": "What does Google have to allow for in real life when it comes to search terms?",
        "answer": "Google has to allow for all the words in the documents as being possible search terms."
    },
    {
        "question": "What is the difference between Google and Yahoo and Bing in terms of their weighting formulae?",
        "answer": "Google, Yahoo, and Bing have very slightly different weighting formulae."
    },
    {
        "question": "What is the author's suggestion for extending a successful experiment?",
        "answer": "If the people of California like it, then it can be extended to the rest of the United States and then the rest of the world."
    },
    {
        "question": "How does Google recognize a question when you type it into the search bar?",
        "answer": "Google recognizes a question when you type it into the search bar by looking into Wikipedia or other data sources to find answers."
    },
    {
        "question": "What is the American word for football?",
        "answer": "Soccer"
    },
    {
        "question": "What does the word 'tap' mean in American English?",
        "answer": "In American English, 'tap' can refer to a faucet or a type of dancing where you tap on the floor."
    },
    {
        "question": "What are the different meanings of the word 'lead' mentioned in the text?",
        "answer": "The different meanings of the word 'lead' mentioned in the text are a metal, lead for taking a dog for a walk, and a particular sense that can have more than one keyword."
    },
    {
        "question": "What is an example of a situation where the word 'football' may not be explicitly mentioned in a document about football?",
        "answer": "When the document mentions names of players, clubs, and related terms, assuming that the topic is football."
    },
    {
        "question": "What is the difference between a 'Venetian blind' and a 'blind Venetian'?",
        "answer": "A 'Venetian blind' is a particular sort of window covering, while a 'blind Venetian' is someone from Venice who can't see."
    },
    {
        "question": "What are some metrics used to evaluate information retrieval systems according to the text?",
        "answer": "Metrics such as effort, in addition to accuracy, are used to evaluate information retrieval systems."
    },
    {
        "question": "What does Google pride itself in when presenting its search results?",
        "answer": "Google prides itself in presenting its results very straightforwardly, very clearly without much graphics."
    },
    {
        "question": "What are some metrics for measuring the results that are given?",
        "answer": "Accuracy, recall, precision, and user satisfaction are metrics for measuring the results."
    },
    {
        "question": "What is query broadening and how does it help users in information retrieval?",
        "answer": "Query broadening is an approach that takes a user's initial query, which may not be the best possible query, and tries to find a better one by either finding new index terms or adjusting term weights to retrieve more relevant information."
    },
    {
        "question": "What are the two general approaches mentioned for improving the system's performance?",
        "answer": "The two general approaches mentioned are asking the user for feedback and offering the user a thesaurus or a term bank."
    },
    {
        "question": "What does the user do with the hits in the system according to the text?",
        "answer": "The user decides which hits are relevant and which ones are not relevant."
    },
    {
        "question": "What is the process described for calculating relevance in the text?",
        "answer": "The process involves adding a constant beta to the sum of all the relevant hits, taking a fraction of the original query, and adding a sum of all the good documents while taking a fraction of those."
    },
    {
        "question": "What does the process described in the text do to the query vector?",
        "answer": "The process moves the query vector closer to the centroid of the relevant, retrieved document vectors and further away from the centroid of non-relevant, retrieved documents."
    },
    {
        "question": "How does the system create a replacement query based on the initial query?",
        "answer": "The system creates a replacement query by adding index terms and weights that have been used to index the relevant documents, increasing their weights and reducing the weights of terms found in non-relevant documents."
    },
    {
        "question": "What term turns out to be a new term that wasn't used in the original query?",
        "answer": "jelly"
    },
    {
        "question": "How does the system handle documents that are marked as not relevant in the context of the text?",
        "answer": "The system downgrades all the other words that appear in the documents marked as not relevant."
    },
    {
        "question": "What is the purpose of setting gamma to 0 in the context of data mining and text analytics?",
        "answer": "The purpose of setting gamma to 0 is to ignore non-relevant hits and only focus on positive feedback."
    },
    {
        "question": "Why is the feedback formula described in the text important for high-use systems?",
        "answer": "The feedback formula is important for high-use systems because it can be applied repeatedly to improve search results by adding extra weights or increasing weights on queries and word terms in relevant documents."
    },
    {
        "question": "What was the job of the PhD student mentioned in the text?",
        "answer": "The job of the PhD student was to work part-time in the British Library finding documents and doing human information retrieval."
    },
    {
        "question": "What is included in the first document, which is a recipe for jam pudding?",
        "answer": "The first document includes jam, pudding, and a little bit of treacle."
    },
    {
        "question": "What does the fourth document in the text mention?",
        "answer": "The fourth document in the text mentions a radio item on traffic jam in Pudding Lane."
    },
    {
        "question": "What are the query weights mentioned in the text for pudding, jam, and everything else?",
        "answer": "1 for pudding, 0.6 for jam, and 0 for everything else"
    },
    {
        "question": "Why does the third document receive a non-zero score?",
        "answer": "The third document receives a non-zero score because it contains pudding, even though it's treacle pudding."
    },
    {
        "question": "According to the user in the text, which document is considered relevant and which one is considered not relevant?",
        "answer": "The user considered the recipe for jam pudding as relevant and the radio item on a traffic jam in Pudding Lane as not relevant."
    },
    {
        "question": "What values are being set for alpha, beta, and gamma in the new query?",
        "answer": "Alpha is set to 0.5, beta is set to 0.5, and gamma is set to 0.2."
    },
    {
        "question": "What was the 'good document' mentioned in the text and what did it contain?",
        "answer": "The 'good document' was the jam pudding recipe, which had 0.8 for pudding and 0.8 for jam. It also mentioned treacle as an alternative to jam."
    },
    {
        "question": "What items are being taken away in the text?",
        "answer": "A small amount of a non-relevant document, treacle, traffic, and lane"
    },
    {
        "question": "What is the new query described in the text?",
        "answer": "The new query has a very small positive value for treacle."
    },
    {
        "question": "What are the new scores assigned to the documents mentioned in the text?",
        "answer": "0.96 for document, 0 for the traffic lane thing, 0.86 for the treacle pudding recipe"
    },
    {
        "question": "What is the top hit when asking for the top two hits in the text?",
        "answer": "The top hit is the jam pudding recipe."
    },
    {
        "question": "What are two methods mentioned in the text for query broadening?",
        "answer": "Two methods mentioned are using user feedback and using a thesaurus or ontology."
    },
    {
        "question": "What is a controlled vocabulary and what are some examples mentioned in the text?",
        "answer": "A controlled vocabulary is a set of terms or phrases in a specific topic, including synonyms and hierarchical relationships. Examples mentioned in the text are Word Net, Roget Thesaurus, and SNOMED."
    },
    {
        "question": "How can replacing words with documents and query words with synonyms from a controlled language improve precision and recall?",
        "answer": "Replacing words with documents and query words with synonyms from a controlled language can improve precision and recall."
    },
    {
        "question": "What is the benefit of using a normalised query in a search engine?",
        "answer": "The normalised query has a better chance of matching the normalised index terms in the content."
    },
    {
        "question": "How can replacing terms in a document and query with a controlled language improve scores, recall, and precision?",
        "answer": "Replacing terms in a document and query with a controlled language results in narrower terms, which can lead to better scores, increased recall, and precision."
    },
    {
        "question": "Why is a traditional database unsuited to retrieval of unstructured information?",
        "answer": "A traditional database is unsuited to retrieval of unstructured information because SQL doesn't allow you to ask for documents which contain multiple conditions like A or B or C directly. Instead, you have to convert the query into disjunctive normal form."
    },
    {
        "question": "What is the range of values for the matching coefficient used for weighted similarity function?",
        "answer": "The matching coefficient used for weighted similarity function always has a value between 0 and 1."
    },
    {
        "question": "What is the cosine of similarity between something and itself?",
        "answer": "The cosine of similarity between something and itself is equivalent to an angle of 0, which results in a cosine value of 1."
    },
    {
        "question": "What are some limitations mentioned in the text regarding information retrieval?",
        "answer": "The limitations mentioned are the inability to allow ranked ordering of results and the lack of support for query broadening to improve the results."
    },
    {
        "question": "What is one advantage of the weighted vector model mentioned in the text?",
        "answer": "It allows you to rank order the results and give feedback about which results are good."
    },
    {
        "question": "What are some of the metrics mentioned in the text for evaluating results?",
        "answer": "Precision and recall are mentioned as metrics for evaluating results."
    },
    {
        "question": "What is the main thrust of Tom Pickard's research at the University of Leeds?",
        "answer": "The main thrust of Tom Pickard's research at the University of Leeds is a project called the EDUBOTS project, which is an EU-funded project looking at chatbots and conversational AI in higher education."
    },
    {
        "question": "What is the main focus of the discussion regarding chatbots and Multi Word Expressions in the text?",
        "answer": "The main focus is on how chatbots can be used to help teachers, students, and support staff in universities deliver quality teaching and experiences, as well as the consequences of Multi Word Expressions for natural language processing with computers."
    },
    {
        "question": "What is the definition of a Multi Word Expression (MWE) according to the text?",
        "answer": "A group of more than one word which functions linguistically as a single unit of meaning."
    },
    {
        "question": "What is an example of a phenomenon that is common to most natural languages, as well as constructed languages like Klingon and Dothraki?",
        "answer": "Multi Word Expressions"
    },
    {
        "question": "What percentage of entries in WordNet consist of more than one word?",
        "answer": "Up to about half, depending on who you ask and when they go and look."
    },
    {
        "question": "What did Schneider's study find regarding the use of Multi Word Expressions in reviews or comments from the internet?",
        "answer": "Schneider's study found that well over half of sentences used in the corpus contained at least one Multi Word Expression."
    },
    {
        "question": "What is one property of Multi Word Expressions mentioned in the text?",
        "answer": "Non-compositionality"
    },
    {
        "question": "According to the text, are all Multi Word Expressions equally easy to understand?",
        "answer": "No, some Multi Word Expressions are easier to understand than others."
    },
    {
        "question": "What is the author's comparison between phrases like 'Iron Curtain' and 'swimming pool'?",
        "answer": "The author explains that phrases like 'Iron Curtain' are very opaque and require extra context to understand, while phrases like 'swimming pool' are easier to comprehend based on the individual meanings of the words."
    },
    {
        "question": "What might a foreign visitor, alien, or robot with perfect knowledge of individual words in a dictionary struggle with when interacting with users of a language?",
        "answer": "They might struggle with Multi Word Expressions."
    },
    {
        "question": "Why do Multi Word Expressions tend to cause problems for language learners?",
        "answer": "Multi Word Expressions tend to cause problems for language learners because they are difficult for natural language processing."
    },
    {
        "question": "What is the challenge when processing language with a computer according to the text?",
        "answer": "The challenge is that breaking up the text into individual words and looking at them separately may not always construct the meaning, especially with Multi Word Expressions."
    },
    {
        "question": "What makes machine translation particularly difficult?",
        "answer": "The same Multi Word Expression may not have an equivalent in the target language as there is in the source language."
    },
    {
        "question": "Why can't you just create a list of all Multi Word Expressions in a given language?",
        "answer": "Because it is an ongoing problem of needing to identify or discover new Multi Word Expressions."
    },
    {
        "question": "Where do people frequently coin Multi Word Expressions, according to the text?",
        "answer": "People frequently coin Multi Word Expressions, especially if you're looking at social media corpuses, and Twitter."
    },
    {
        "question": "What is one method used to identify words that occur together more often than expected in a text?",
        "answer": "Collocation is a method used to identify words that occur together more often than expected in a text."
    },
    {
        "question": "Why is it important for a statistical measure to account for the fact that some words are more common than others?",
        "answer": "It is important for a statistical measure to account for the fact that some words are more common than others because conjunctions and words are very common in general."
    },
    {
        "question": "What is Pointwise Mutual Information (PMI) used as a measure for?",
        "answer": "Pointwise Mutual Information (PMI) is used as a statistical measure for adjusting for words that are not encountered frequently in text."
    },
    {
        "question": "What is the main idea behind analyzing the frequency of word pairs like 'fake news' in a large body of text?",
        "answer": "The main idea is to compare how often the pair of words 'fake news' occur together with how often they occur individually in the text."
    },
    {
        "question": "What term is mentioned to be more frequent than expected when looking at the frequency of the words 'fake' and 'news' separately?",
        "answer": "fake news"
    },
    {
        "question": "What does a big number in the Pointwise Mutual Information measure imply about the co-occurrence of words?",
        "answer": "A big number in the Pointwise Mutual Information measure implies that the words are co-occurring more often than expected."
    },
    {
        "question": "What are some examples of pairs of words mentioned in the text that occur together frequently but don't make up a single unit of meaning?",
        "answer": "Some examples mentioned in the text are 'strong and coffee' and 'black coffee'."
    },
    {
        "question": "What did the author suggest is needed to find Multi Word Expressions beyond just finding interesting co-occurrences?",
        "answer": "The author suggested using semantic embeddings or word vector embeddings."
    },
    {
        "question": "What are some tools mentioned in the text that can be used to capture the meaning of a word?",
        "answer": "word2vec and GloVe"
    },
    {
        "question": "What are non-compositional Multi Word Expressions?",
        "answer": "Non-compositional Multi Word Expressions are expressions where the meaning of the expression as a whole differs from the meanings of the individual component words."
    },
    {
        "question": "According to the text, why shouldn't you expect to get the meaning of the phrase 'green thumbs' by adding the meanings of the words 'green' and 'thumbs' together?",
        "answer": "The meaning of the phrase 'green thumbs' is very different from the meanings of the individual words 'green' and 'thumbs'."
    },
    {
        "question": "What process is described in the text to create word vector embedding models?",
        "answer": "Joining two separate words into a single word or token before building the models"
    },
    {
        "question": "What does the similarity between the vectors for the words 'green' and 'thumbs' indicate in the context of the text?",
        "answer": "The similarity between the vectors for the words 'green' and 'thumbs' indicates the compositional nature of the phrase, suggesting that it should be treated as a Multi Word Expression."
    },
    {
        "question": "What source did the author use to gather a relatively large corpus of text for their study?",
        "answer": "The author used the text from the English Wikipedia."
    },
    {
        "question": "Why did the author take a 10% sample of sentences in Wikipedia?",
        "answer": "To reduce the size of the space and make the project more manageable."
    },
    {
        "question": "What methods did the author use to calculate the co-location measure and train vector embedding models?",
        "answer": "The author used a word2vec model and GloVe to calculate the co-location measure and train vector embedding models."
    },
    {
        "question": "Who developed the methodology that was used for comparing the vectors of potential Multi Word Expressions?",
        "answer": "Will Roberts and Markus Egg"
    },
    {
        "question": "What was the author's purpose in comparing word2vec and GloVe?",
        "answer": "The author's purpose was to see which one was more useful or more effective for the task."
    },
    {
        "question": "According to the text, how did the results of using GloVe for word embeddings compare to the author's expectations?",
        "answer": "The results of using GloVe for word embeddings were lower quality and did not correlate well with what humans thought of the expressions."
    },
    {
        "question": "What were some of the considerations mentioned when comparing GloVe to word2vec?",
        "answer": "The computational costs of GloVe were higher, requiring longer run times and more memory on the computer."
    },
    {
        "question": "What percentage of sentences from English Wikipedia did the author sample to yield reasonable and effective results?",
        "answer": "10%"
    },
    {
        "question": "What are some ways in which the author suggests the topic could be further explored?",
        "answer": "The author suggests that the topic could be expanded upon or looked into further in order to understand it better or push the idea further."
    },
    {
        "question": "What are Multi Word Expressions according to the text?",
        "answer": "Multi Word Expressions are sets of groups of two or three words which occur next to each other in the text."
    },
    {
        "question": "In English, how are words typically separated?",
        "answer": "Words in English are typically separated with spaces."
    },
    {
        "question": "In which languages do nouns tend to be combined together into single compound nouns?",
        "answer": "German and other Germanic languages"
    },
    {
        "question": "What are some challenges mentioned in applying the method to a different language?",
        "answer": "The challenges mentioned include potential inefficiency and high cost."
    },
    {
        "question": "What is the author particularly interested in when it comes to Multi Word Expressions?",
        "answer": "The author is particularly interested in looking at the way in which people use Multi Word Expressions in particular domains."
    },
    {
        "question": "Where can you grab things from to get a general sense of how people use English in conversation?",
        "answer": "You can grab things from the internet, Wikipedia, or social media."
    },
    {
        "question": "What type of text is mentioned as an example of specialised domains in the given text?",
        "answer": "Academia, data science modules, or medicine"
    },
    {
        "question": "What is a big question for a lot of word embedding models regarding their transferability?",
        "answer": "The big question is how well do word embedding models transfer and apply to more specific applications."
    },
    {
        "question": "What kind of neural network is BERT and what is its purpose?",
        "answer": "BERT is a different kind of neural network used for learning word embeddings."
    },
    {
        "question": "What is the author suggesting about BERT's capability in handling Multi Word Expressions?",
        "answer": "The author suggests that BERT may already have some capability in handling Multi Word Expressions to some extent."
    },
    {
        "question": "What potential area for research is mentioned in the text regarding the structure of BERT?",
        "answer": "The potential area for research mentioned is that 'green plus thumbs together' means something different from 'green and thumbs'."
    },
    {
        "question": "What does the author recommend as the first thing to do?",
        "answer": "Viewing"
    },
    {
        "question": "What is the name of the episode from Star Trek-- The Next Generation that is mentioned in the text?",
        "answer": "Darmok"
    },
    {
        "question": "Who wrote the paper 'Multi Word Expressions: A Pain In The Neck for NLP'?",
        "answer": "Sag and his colleagues"
    },
    {
        "question": "Who is recommended as an author for learning more about Multi Word Expressions?",
        "answer": "Carlos Ramish"
    },
    {
        "question": "What did the author include in the text as a way to share their paper with the community?",
        "answer": "The author included a link to their paper that they wrote last year."
    },
    {
        "question": "What is the Sketch Engine web tool used for?",
        "answer": "The Sketch Engine web tool is used for text analytics and corpus linguistics research."
    },
    {
        "question": "What tool within Sketch Engine allows you to collect a corpus of your own from the web?",
        "answer": "WebBootCaT tool"
    },
    {
        "question": "What is Sketch Engine?",
        "answer": "Sketch Engine is a tool founded by researcher Adam Kilgarriff and is mentioned in the text as something to read about in the paper on the Association for Computational Linguistics Special Interest Group on Web As Corpus."
    },
    {
        "question": "What is the benefit of using the online tool for Leeds University researchers?",
        "answer": "Leeds University researchers have free access to the tool, even though it is a commercial service."
    },
    {
        "question": "What is the purpose of a distributional thesaurus?",
        "answer": "To compare the concordance of a word with the concordance of other words to determine if they appear in similar contexts and therefore have similar meanings."
    },
    {
        "question": "What is WebBootCaT and what is its purpose?",
        "answer": "WebBootCaT is a special tool for creating a specialised corpus of your own choice from the web."
    },
    {
        "question": "What can terminology extraction be used for in relation to a corpus?",
        "answer": "Terminology extraction can be used to extract words and multi-word terms which are characteristic of the corpus and are likely specialist terms."
    },
    {
        "question": "What is the purpose of the word sketch for the word 'catch' mentioned in the text?",
        "answer": "To show what sorts of words appear before and after the word 'catch' and to understand its collocational behavior and meanings."
    },
    {
        "question": "What type of corpus does Sketch Engine have for Chinese and Arabic?",
        "answer": "Sketch Engine has a Gigaword corpus for Chinese and Arabic."
    },
    {
        "question": "What can you obtain from a concordance in order to figure out the meaning of a particular word or phrase?",
        "answer": "Lots of examples of the word or phrase"
    },
    {
        "question": "What does the idiom 'caught with your pants down' mean?",
        "answer": "Being caught in a compromising situation"
    },
    {
        "question": "According to the text, why are tea and coffee likely to mean similar things?",
        "answer": "Tea and coffee are likely to mean similar things because they have similar distributional patterns."
    },
    {
        "question": "Why does the author mention that 'vegetable apparently means the same as tea'?",
        "answer": "Because you can have 'tea cup' and 'vegetable cup' and so on, showing that the words are less similar as you go down the list."
    },
    {
        "question": "What can you find in Opus 2 within Sketch Engine?",
        "answer": "For a whole lot of documents, the English and the translated Chinese and the translation is in other languages too."
    },
    {
        "question": "What tool allows you to create a corpus from a web page?",
        "answer": "WebBootCaT"
    },
    {
        "question": "What are seed words and how are they used in the context of the text?",
        "answer": "Seed words are words or phrases that are typical of the topic area. In the text, seed words are used to help identify URLs related to the topic by providing a list of parameters."
    },
    {
        "question": "What does the tool mentioned in the text do?",
        "answer": "It compares the words in a given corpus against the words in a standard English corpus to find words that are more frequent in the given corpus."
    },
    {
        "question": "What can you do in Sketch Engine to extract word lists?",
        "answer": "You can extract word lists in Sketch Engine."
    },
    {
        "question": "What can you compare against the frequency lists to analyze terminology?",
        "answer": "You have to compare the frequency lists against the frequency lists of the British National Corpus."
    },
    {
        "question": "What is one example of a difference between the collocations of the words 'little' and 'small'?",
        "answer": "There are certain collocations with 'little' which don't come with 'small' and vice versa."
    },
    {
        "question": "What can you do within Sketch Engine according to the text?",
        "answer": "You can do part-of-speech tagging and lemmatization within Sketch Engine."
    },
    {
        "question": "What tool can be used to collect and analyze text data for corpus research?",
        "answer": "Sketch Engine"
    },
    {
        "question": "What web browser is the speaker using on their MacBook?",
        "answer": "Chrome"
    },
    {
        "question": "What does the author mention about connecting via VPN in order to access some videos?",
        "answer": "The author mentions that in order to access some videos, you have to make sure that you connect via the VPN."
    },
    {
        "question": "What languages does Sketch Engine have very large corpora for?",
        "answer": "Arabic, Chinese, Japanese, and most languages you can think of"
    },
    {
        "question": "What is mentioned about the alphabet used in Arabic and Chinese compared to English in the text?",
        "answer": "Arabic and Chinese do not use the same alphabet as English."
    },
    {
        "question": "What options are available for using Sketch Engine according to the text?",
        "answer": "There is a 30-day free trial license or the option to have your company pay for it."
    },
    {
        "question": "What type of log in does the speaker want to use?",
        "answer": "The speaker wants to use the institutional log in."
    },
    {
        "question": "What date is the institution expected to close down?",
        "answer": "June 25, 2022"
    },
    {
        "question": "What is the name of the new corpus being created in the field of data mining?",
        "answer": "The new corpus being created in the field of data mining is called Datamining."
    },
    {
        "question": "What is the purpose of the text mining trial corpus mentioned in the text?",
        "answer": "The purpose of the text mining trial corpus is to test if it works."
    },
    {
        "question": "What folder name did the speaker give to the text they found on the web?",
        "answer": "Web1"
    },
    {
        "question": "What does the speaker plan to do in order to fulfill the requirement of typing at least three words or phrases?",
        "answer": "The speaker plans to use web search and give search terms related to data mining."
    },
    {
        "question": "What technique is the speaker suggesting they try out?",
        "answer": "Data mining"
    },
    {
        "question": "What type of web pages did the search engine match the search terms to?",
        "answer": "Web pages related to data mining"
    },
    {
        "question": "What is the purpose of the WEKA toolkit mentioned in the text?",
        "answer": "The purpose of the WEKA toolkit is data mining."
    },
    {
        "question": "What does the process described in the text involve?",
        "answer": "The process involves downloading web pages, extracting text from each page, and discarding images, tables, and boilerplate content such as headings, contact information, and links to other pages."
    },
    {
        "question": "What is the main topic discussed in the text?",
        "answer": "The main topic discussed in the text is data mining."
    },
    {
        "question": "How many tokens and words were collected in the corpus?",
        "answer": "29,000 tokens or 24,000 words"
    },
    {
        "question": "What is the purpose of pressing the Go button in the dashboard?",
        "answer": "To extract key words from the data mining corpus"
    },
    {
        "question": "What is the default reference corpus used for comparison in the text?",
        "answer": "The default reference corpus used for comparison is the English Web 2020, or enTenTen."
    },
    {
        "question": "What time periods does the Brown Family represent in terms of English language?",
        "answer": "The Brown Family represents the 1960s, the 1990s, and the 2010s in terms of English language."
    },
    {
        "question": "What type of words are mentioned in the text and why does the author suggest looking them up?",
        "answer": "The text mentions words like 'mining' and 'doi' and the author suggests looking them up because they are not very common, even in the data mining corpus, and may only occur once in the Brown corpus."
    },
    {
        "question": "Why is it important to choose a large corpus as your reference corpus?",
        "answer": "Choosing a large corpus as your reference corpus is important because it provides statistically significant meanings and better examples."
    },
    {
        "question": "What type of terms are considered good indicators of a specialist domain according to the text?",
        "answer": "Multi-word terms"
    },
    {
        "question": "What can you do with Leeds Internet Corpora according to the text?",
        "answer": "You can query the corpora in much the same way as Sketch Engine and get a concordance out."
    },
    {
        "question": "Why does the web page mentioned in the text contain information on open source development of large corpora?",
        "answer": "The web page contains information on open source development of large corpora for collecting a very large corpus of words from the web, typically around 500 keywords representing the English language as a whole."
    },
    {
        "question": "What type of words are mentioned in the text as examples for English?",
        "answer": "Typical words like picture, extent, raised, and so on"
    },
    {
        "question": "How does WebBootCaT use the list of 500 words to produce queries?",
        "answer": "WebBootCaT takes combinations of four words from the list of 500 words to create queries, sends them to the search engine, and retrieves the results."
    },
    {
        "question": "What does WebBootCaT do after extracting the text from web pages?",
        "answer": "WebBootCaT does some post-processing, gets rid of unwanted text, and generates frequency lists from the corpora."
    },
    {
        "question": "What is the author trying to emphasize in the text?",
        "answer": "The author is emphasizing the process of using WebBootCaT to collect a billion word corpus rather than just a small sample corpus."
    },
    {
        "question": "What makes the source mentioned in the text a good source of information?",
        "answer": "The source is a good source of information because they have meetings or conferences pretty much every year or every other year, and the proceedings contain research papers from all that were presented on Web As Corpus research."
    },
    {
        "question": "What are some ways mentioned in the text to find information about a topic area?",
        "answer": "Some ways mentioned in the text to find information about a topic area are using Google, using Google Scholar, and looking at the Association of Computational Linguistics Special Interest Group."
    },
    {
        "question": "What web tool was introduced in the video for corpus linguistics?",
        "answer": "Sketch Engine"
    },
    {
        "question": "What can you do with the corpus you collect yourself?",
        "answer": "You can extract a list of terminology from the corpus."
    },
    {
        "question": "What paper does the text suggest the reader should read regarding the search engine?",
        "answer": "Adam Kilgarriff's paper"
    },
    {
        "question": "What is BERT and what is its purpose according to Professor Eric Atwell?",
        "answer": "BERT stands for Bidirectional Encoder Representations from Transformers. Its purpose is to understand meaning relationships between sentences and measure the similarity in meaning between two sentences."
    },
    {
        "question": "What paper received the best paper award at the 2019 conference NAACL?",
        "answer": "BERT, pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
        "question": "Which company's AI Language Research group do Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova belong to?",
        "answer": "Google"
    },
    {
        "question": "Why are computational linguistics resources from Google freely available for other people to use?",
        "answer": "Because Google has the aim to make things useful to everybody."
    },
    {
        "question": "What is the general idea of BERT according to the text?",
        "answer": "The general idea of BERT is to have pre-trained neural network representations learned from unlabelled text, which can then be fine-tuned for specific tasks."
    },
    {
        "question": "What types of learning does BERT do a lot of, according to the text?",
        "answer": "BERT does a lot of unsupervised learning of morphology, syntax or grammar, and semantics or meaning."
    },
    {
        "question": "What method is used for learning word segmentation in the text?",
        "answer": "WordPiece"
    },
    {
        "question": "What basic assumption allows the researchers to take pairs of sentences with related meanings?",
        "answer": "The basic assumption that in running text, two sentences next to each other should be related in meaning."
    },
    {
        "question": "What is the general model used for measuring the distance between two sentences?",
        "answer": "Labelling"
    },
    {
        "question": "According to the text, which neural networks outperform all systems on all tasks by a substantial margin?",
        "answer": "BERT Base and BERT Large"
    },
    {
        "question": "What is the purpose of ablation studies mentioned in the text?",
        "answer": "The purpose of ablation studies is to deliberately break bits of BERT and see how well it works in disabled mode."
    },
    {
        "question": "What is the main idea behind BERT according to the text?",
        "answer": "The main idea behind BERT is learning pre-trained representations from large amounts of unlabelled text, determining if two sentences are related in meaning, and fine-tuning it for specific tasks."
    },
    {
        "question": "What does BERT stand for and what does it represent?",
        "answer": "BERT stands for Bidirectional Encoder Representations from Transformers, and it represents the neural networks and the data learned from them."
    },
    {
        "question": "Where do they get the 800 million words corpus for pre-training representations from unlabelled text?",
        "answer": "They get the 800 million words corpus from the Google Books corpus, which includes books scanned in from Oxford library, Oxford University library, and many other University libraries."
    },
    {
        "question": "What are some sources mentioned in the text that can be used to create models for a wide range of tasks?",
        "answer": "The sources mentioned are a very large collection of learned texts, anything in university libraries, and English Wikipedia."
    },
    {
        "question": "Where can you find the code and pre-trained models for Google Research BERT?",
        "answer": "The code and pre-trained models for Google Research BERT are available on the GitHub website."
    },
    {
        "question": "What is the core concept discussed in the text regarding machine learning?",
        "answer": "The core concept discussed in the text is unsupervised machine learning."
    },
    {
        "question": "What type of neural network is mentioned in the text?",
        "answer": "Transformer neural network"
    },
    {
        "question": "What is the main difference between a classifier and the approach described for BERT in the text?",
        "answer": "The main difference is that a classifier maps a sentence into a class, while BERT maps a pair of sentences and a class."
    },
    {
        "question": "What type of machine learning models are used for morphology, syntax, and semantics in BERT?",
        "answer": "Unsupervised or semi-supervised machine learning models"
    },
    {
        "question": "What was the Morpho Challenge focused on?",
        "answer": "The Morpho Challenge was focused on unsupervised learning of morphological analysis, segmenting words into morphemes."
    },
    {
        "question": "What is the concept described as an unsupervised measure of learning semantic vectors for meanings of words or morphemes?",
        "answer": "The concept is described as an unsupervised measure of learning semantic vectors for meanings of words or morphemes."
    },
    {
        "question": "How does unsupervised learning help in distinguishing between similar and not similar meanings of sentences?",
        "answer": "Unsupervised learning helps in distinguishing between similar and not similar meanings of sentences by providing lots of pairs of sentences which are similar, and then taking random pairs of sentences which are not similar, allowing the system to learn to distinguish between the two."
    },
    {
        "question": "What is the first stage in the process described in the text?",
        "answer": "The first stage is WordPiece, unsupervised machine learning of text segmentation into words and morphemes."
    },
    {
        "question": "What is the purpose of the Morpho Challenge mentioned in the text?",
        "answer": "The purpose of the Morpho Challenge was to work on languages like Finnish and Turkish, which have limited linguistic resources available."
    },
    {
        "question": "What is one issue mentioned in the text regarding dealing with new words in a corpus?",
        "answer": "One issue mentioned is that there will always be more and more new words you haven't seen before."
    },
    {
        "question": "What inspired the development of WordPiece?",
        "answer": "WordPiece was inspired by some other Google researchers working on machine translation."
    },
    {
        "question": "What is the purpose of WordPiece, as described in the text?",
        "answer": "WordPiece is an unsupervised machine learning morphological analyzer that computes frequencies of words and pieces in a corpus."
    },
    {
        "question": "How does the author suggest handling less common words?",
        "answer": "The author suggests chopping up less common words into pieces that are common."
    },
    {
        "question": "What process does the text describe in order to handle rare words?",
        "answer": "The text describes chopping rare words into smaller parts and trying various permutations and combinations to find the best entropy combination."
    },
    {
        "question": "What method is described for building a vector representing the concordance context for each WordPiece?",
        "answer": "Dividing the text into WordPieces and building a vector representing the concordance context for each WordPiece"
    },
    {
        "question": "What is the default assumption about the number of WordPieces per sentence in the text?",
        "answer": "The default assumption is about 25 WordPieces per sentence."
    },
    {
        "question": "According to the text, what happens if there are more than 25 words in a sentence?",
        "answer": "The default is to ignore the rest of the sentence."
    },
    {
        "question": "How does BERT pre-train its language models?",
        "answer": "BERT pre-trains its language models by masking some percentage of the input tokens at random and then predicting those masked tokens."
    },
    {
        "question": "What is a cloze task in the context of a Masked Language Model?",
        "answer": "A cloze task is a test of English language ability where a sentence is given with a blanked out word that needs to be predicted."
    },
    {
        "question": "What is the purpose of optimising the vector in the context of the text?",
        "answer": "The purpose of optimising the vector is to predict the missing words in a sentence."
    },
    {
        "question": "Where can you find a better explanation of the details of a neural network?",
        "answer": "In the Jurafsky and Martin textbook"
    },
    {
        "question": "What is the next task mentioned in the text after representing the meaning of a sentence as a combination of WordPieces?",
        "answer": "The next task is to represent the relationship between two sentences."
    },
    {
        "question": "What assumption can you make about sentences in a coherent text according to the passage?",
        "answer": "You can assume that in any coherent text, each sentence follows on from the previous sentence."
    },
    {
        "question": "According to the text, what is the structure of a Wikipedia entry or a book?",
        "answer": "Each document is structured with the first sentence followed by the second sentence, and so on, with a meaning relationship between them."
    },
    {
        "question": "What is the bigram model mentioned in the text used for?",
        "answer": "The bigram model is used for sentence meaning, where each sentence is related in meaning to the next sentence."
    },
    {
        "question": "What is the purpose of pre-training a Next Sentence Prediction task?",
        "answer": "The purpose is to establish an implicit label between sentences to indicate their relatedness."
    },
    {
        "question": "What percentage of the time is B the actual next sentence that follows from A in the training sentences?",
        "answer": "50%"
    },
    {
        "question": "What relationship is described between sentence A and sentence B in the text?",
        "answer": "The relationship described is that sentence B is related to sentence A by IsNext, paraphrasing, or entitlement."
    },
    {
        "question": "How are questions and answers related in question-passages pairing?",
        "answer": "Questions and answers in question-passages pairing are related in a similar way as sentence one is related to sentence two."
    },
    {
        "question": "What approach did the researchers take to fine-tune the general model for the specific tasks?",
        "answer": "The researchers fine-tuned the general model by giving it an extra training set for the particular task."
    },
    {
        "question": "What are some common activities that take place at computational linguistics conferences?",
        "answer": "People presenting their own papers and participating in shared tasks with standard training and test sets."
    },
    {
        "question": "What is the purpose of the GLUE competition mentioned in the text?",
        "answer": "The purpose of the GLUE competition is to evaluate general language understanding by combining existing datasets into a set of tasks for evaluation."
    },
    {
        "question": "What two tasks have just got one sentence in BERT terms?",
        "answer": "Giving a sentence and a null or blank for the second sentence, and answering yes or no to whether the sentence is linguistically acceptable."
    },
    {
        "question": "What is the task in the STS-B and MRPC datasets?",
        "answer": "To determine if pairs of sentences are similar in meaning"
    },
    {
        "question": "What is the purpose of the classifier mentioned in the text?",
        "answer": "To check if a question has already been posted before posting it on Quora"
    },
    {
        "question": "What is the purpose of the tasks MNLI and RTE mentioned in the text?",
        "answer": "To determine if there is an entailment or contradiction between two given sentences"
    },
    {
        "question": "What did the Google researchers try in addition to the standard test sets within GLUE?",
        "answer": "The Google researchers also tried other quite large datasets."
    },
    {
        "question": "What is the SQuAD dataset and what does it consist of?",
        "answer": "The SQuAD dataset stands for Stanford Question Answering Dataset and consists of pairs of questions and answers."
    },
    {
        "question": "What is the main difference between the tasks in SQuAD and SWAG?",
        "answer": "In SQuAD, the second piece of text is a larger piece of text where the answer is located, while in SWAG, the second bit is four possible continuations and the task is to choose one of them."
    },
    {
        "question": "Which models outperformed all of the existing systems on all of the tasks?",
        "answer": "BERT Base and BERT Large"
    },
    {
        "question": "How does BERT Large compare to BERT Base in terms of performance and processing requirements?",
        "answer": "BERT Large significantly outperforms BERT Base across all tasks, but it requires more processing power and time due to having 24 layers compared to BERT Base's 12 layers."
    },
    {
        "question": "What is the typical length requirement for papers submitted to conferences mentioned in the text?",
        "answer": "Short papers of four pages or long papers of eight pages"
    },
    {
        "question": "What is the author's advice regarding page limits for coursework?",
        "answer": "The author advises to stick to the page limits for coursework."
    },
    {
        "question": "What are ablation studies mentioned in the text and what is their purpose?",
        "answer": "Ablation studies are mentioned in the text as a method similar to cutting out bits of the brain in psychology to see what happens. Their purpose is to understand how the model works by removing certain components and observing the results."
    },
    {
        "question": "How do researchers study brain behavior without cutting out bits of the brain?",
        "answer": "Researchers study people who've had accidents or other reasons causing parts of their brain to not work anymore, and observe their behavior to gain insights on normal brain behavior."
    },
    {
        "question": "What type of neural network architecture is mentioned in the text?",
        "answer": "Deep bi-directional architectures"
    },
    {
        "question": "What type of research did the authors build on in the text?",
        "answer": "They built on lots of other research, including the WordPiece research."
    },
    {
        "question": "What can you find by reading all the papers in the references mentioned in the text?",
        "answer": "The current state-of-the-art in natural language processing research."
    },
    {
        "question": "What are some issues that the author mentions have not been discussed in detail in the text?",
        "answer": "The author mentions that implementation details have not been gone into detail."
    },
    {
        "question": "What is a big issue in theoretical linguistics according to the text?",
        "answer": "Long sentences don't mean the same as short sentences."
    },
    {
        "question": "What type of computing resources are required to learn the underlying BERT model, especially the Bert Large model?",
        "answer": "Huge computing resources are required to learn the underlying BERT model, especially the Bert Large model."
    },
    {
        "question": "What downstream tasks can the pre-trained model be plugged into?",
        "answer": "Question-answering"
    },
    {
        "question": "What do linguists and corpus linguists like to see in terms of examples in the BERTology paper?",
        "answer": "They like to see specific examples of sentences or pairs of sentences which work and pairs of sentences which don't work, along with explanations or analysis of why they didn't work."
    },
    {
        "question": "What is a common issue in research papers, as mentioned in the text?",
        "answer": "Researchers often present results without specifying tasks that are not suitable for BERT."
    },
    {
        "question": "What is BERT based on and what does it assume about sentences?",
        "answer": "BERT is based on sentences from Books and Wikipedia pages, and it assumes that one sentence is linked to the next sentence in some sort of meaning relationship."
    },
    {
        "question": "What is an example of a similar software being used on other languages?",
        "answer": "Arab BERT is an example of a similar software being used on other languages."
    },
    {
        "question": "What tokenisation method does the text assume?",
        "answer": "The text assumes the WordPiece tokenisation method."
    },
    {
        "question": "What is one concern mentioned regarding the application of BERT to languages other than English?",
        "answer": "The concern is that the WordPiece tokenization used by BERT may not be the most appropriate for languages with richer morphology, such as Arabic."
    },
    {
        "question": "What does the author find counterintuitive about BERT's ability to learn mapping between sentences and classes?",
        "answer": "The author finds it counterintuitive that BERT can learn this mapping without any linguistic theory, rules of morphology, or dictionary of words and morphemes in English."
    },
    {
        "question": "According to the text, what is not explicitly mentioned in the grammar of English?",
        "answer": "The text mentions that the grammar of English does not explicitly mention that words can have more than one meaning, words have senses, or that two words can mean the same thing."
    },
    {
        "question": "What are the suggestions given regarding using the questions about BERT?",
        "answer": "The suggestions include using the questions as part of a research proposal or for a project in the future."
    },
    {
        "question": "What is the assumption about the relationship between sentences in the BERT model?",
        "answer": "The assumption is that there is an inherent, implicit label between each sentence, where sentence one is related to sentence two, sentence two is related to sentence three, and sentence one is not related to any other sentence at random."
    },
    {
        "question": "What is the process described for training the model to understand relationships between sentences?",
        "answer": "The model is trained to learn that the two sentences are related and can be fine-tuned on specific relationships."
    },
    {
        "question": "What method is used for unsupervised learning of morphology in the text?",
        "answer": "WordPiece"
    },
    {
        "question": "What is one of the reasons why everyone is using BERT?",
        "answer": "The results of using BERT are quite astounding."
    },
    {
        "question": "According to the text, how do BERT Base and BERT Large perform compared to other systems on the tasks they were tested on?",
        "answer": "BERT Base and BERT Large outperform all the other systems they tried on all of the tasks by a substantial margin."
    },
    {
        "question": "What is mentioned in the appendices of the text?",
        "answer": "Detailed explanations of the experiments"
    },
    {
        "question": "What is the advantage of using BERT according to the text?",
        "answer": "The advantage of using BERT is that you don't necessarily have to understand the neural network model unless you want to become a machine learning researcher."
    },
    {
        "question": "What course does the speaker advise to take in order to build new neural network architectures?",
        "answer": "The speaker advises to take on the deep learning course to build new neural network architectures."
    },
    {
        "question": "What will Professor Eric Atwell be talking about in the lecture?",
        "answer": "Professor Eric Atwell will be talking about n-gram language modeling."
    },
    {
        "question": "How can you evaluate a language model with n-gram probability estimates?",
        "answer": "One way to evaluate a language model with n-gram probability estimates is to measure perplexity of the language model against a corpus."
    },
    {
        "question": "What is one way to deal with the situation when there are no examples of a word in the training set?",
        "answer": "One way to deal with that is what's called smoothing."
    },
    {
        "question": "Why do mathematicians prefer probabilities over just scores of some sort of number?",
        "answer": "Probabilities are nice because they are in the range 0 to 1, which mathematicians prefer compared to just scores of some sort of number."
    },
    {
        "question": "According to the text, which phrase is more commonly used in English: 'high winds' or 'large winds'?",
        "answer": "'High winds' is more commonly used in English."
    },
    {
        "question": "What does Microsoft Word do when a word is not in the dictionary?",
        "answer": "Microsoft Word underlines words that are not in the dictionary."
    },
    {
        "question": "What is the author discussing in the text regarding the probability of certain words?",
        "answer": "The author is discussing how certain words like 'minutes' have a higher probability than 'minuets' in certain contexts."
    },
    {
        "question": "According to the text, which sequence is more likely: 'I saw a van' or 'eyes awe of an'?",
        "answer": "'I saw a van' is more likely than 'eyes awe of an'."
    },
    {
        "question": "What do we want to compute when comparing two or more possible word sequences?",
        "answer": "We want to compute the probability of a sentence or more generally of a sequence of words."
    },
    {
        "question": "What is a language model?",
        "answer": "A language model is a model that computes either the probability of a complete sequence of words or the probability of the next word given the words up to a certain point."
    },
    {
        "question": "What is a Language Model (LM) in computational linguistics?",
        "answer": "A Language Model (LM) is a standard thing in computational linguistics used to compute the probability of a word sequence."
    },
    {
        "question": "What does the author suggest about computing the probability of the parts in the given sequence?",
        "answer": "The author suggests that computing the probability of the parts involves multiplying them together."
    },
    {
        "question": "What is the relationship between the probability of B given A and the probability of A followed by B?",
        "answer": "The relationship is that the probability of B given A is equal to the probability of A followed by B divided by the probability of A."
    },
    {
        "question": "How many words are in the word sequence described in the text?",
        "answer": "More than two words"
    },
    {
        "question": "According to the text, how is the probability of A,B,C,D calculated?",
        "answer": "The probability of A,B,C,D is calculated as the probability of A times the probability of B, given A, times the probability of C, given A and B, times the probability of D, given A,B,C, and so on."
    },
    {
        "question": "How can you compute the probability of the words in a sentence according to the text?",
        "answer": "By multiplying the probabilities of each word in the sequence, given the previous words."
    },
    {
        "question": "What is the range of probabilities mentioned in the text?",
        "answer": "The range of probabilities mentioned in the text is from 1 to 0 or 0 to 1."
    },
    {
        "question": "What method is suggested for combining sequences or subsequences in the text?",
        "answer": "To combine them, we multiply them together."
    },
    {
        "question": "What method is being described in the text to calculate probability?",
        "answer": "Counting occurrences and dividing by the predecessor"
    },
    {
        "question": "Why is it difficult to get accurate counts of long sequences in the given text?",
        "answer": "It is difficult to get accurate counts of long sequences in the text because long sequences do not occur significantly and there is not enough data for estimating even the subsets."
    },
    {
        "question": "Who came up with a simplifying assumption about a probability over 100 years ago?",
        "answer": "Andrei Markov"
    },
    {
        "question": "What did Andrew Markov count up in a Russian newspaper?",
        "answer": "individual letters and pairs of letters and sequences of three letters"
    },
    {
        "question": "According to the text, what is the Markov assumption?",
        "answer": "The Markov assumption states that the probability of a long sequence of words can be computed in terms of shorter subsequences."
    },
    {
        "question": "According to the text, how is the probability of a sequence of words calculated in the unigram model?",
        "answer": "By multiplying together the probabilities of each individual word, regardless of the context"
    },
    {
        "question": "What kind of random generator is being described in the text?",
        "answer": "A random generator that generates words at random but takes into account the probabilities of individual words."
    },
    {
        "question": "What is a bigram and how does it work in analyzing word sequences?",
        "answer": "A bigram looks at pairs of words in a text to analyze the probability of a word sequence. It considers the word that came before to determine the likelihood of the next word in the sequence."
    },
    {
        "question": "What method does the author suggest for generating more English-like sentences?",
        "answer": "Choosing the next word depending on the previous word"
    },
    {
        "question": "What is an example of a long distance dependency in language, as mentioned in the text?",
        "answer": "The example given is 'the computer, which I had just put into the machine room on the fifth floor crashed.'"
    },
    {
        "question": "What is being discussed in the text?",
        "answer": "The text is discussing n-gram modeling and how to estimate the probabilities of n-grams."
    },
    {
        "question": "What is the purpose of counting up in a large corpus when trying to calculate the probability of a bigram model?",
        "answer": "The purpose is to get the probability of a word given the previous word by counting up the pairs of those words and the individual words."
    },
    {
        "question": "Why do we see pseudowords at the beginning and end of sentences in the text?",
        "answer": "The pseudowords at the beginning and end of sentences are used to demonstrate that the first word 'I' is conditional on it being the beginning of a sentence, and 'Sam' is followed by the end of a sentence."
    },
    {
        "question": "How many bigrams involve the start of a sentence followed by the word 'I'?",
        "answer": "Three bigrams involve the start of a sentence followed by the word 'I'."
    },
    {
        "question": "What is the bigram probability of 'I' given the start of a sentence in the text?",
        "answer": ".67"
    },
    {
        "question": "What can you figure out probabilities of, given the previous word, according to the text?",
        "answer": "Probabilities of any word"
    },
    {
        "question": "What corpus is mentioned in the text for counting up more reasonably?",
        "answer": "Berkeley restaurant project corpus"
    },
    {
        "question": "What can be inferred from the text regarding the process of normalizing the text?",
        "answer": "The text has been converted to lowercase."
    },
    {
        "question": "How many times did the word 'I' occur in the small corpus mentioned in the text?",
        "answer": "Five times"
    },
    {
        "question": "How many times did the phrase 'I want to eat Chinese food' occur in the text?",
        "answer": "Twice"
    },
    {
        "question": "What are the three types of frequencies mentioned in the text?",
        "answer": "The three types of frequencies mentioned are frequencies of expected things, frequencies of unexpected things, and things which never occur."
    },
    {
        "question": "How many times does the word 'I' occur in the text?",
        "answer": "The word 'I' occurs five times in the text."
    },
    {
        "question": "How does normalizing by unigrams affect the hesitations and other words in the text?",
        "answer": "Normalizing by unigrams helps to remove some of the hesitations and other words that shouldn't be included."
    },
    {
        "question": "What is the estimated probability of the sentence 'I want English food' based on the given information?",
        "answer": "0.000031"
    },
    {
        "question": "What are we trying to collect based on knowledge of probability in the text?",
        "answer": "The probability of a word, given the previous word."
    },
    {
        "question": "Why is converting probabilities into logarithms a common practice in computation?",
        "answer": "Converting probabilities into logarithms is a common practice in computation to avoid underflow when dealing with very small fractions."
    },
    {
        "question": "Why is it faster to add logarithms together rather than multiplying them?",
        "answer": "Adding logarithms together is faster than multiplying them because it helps avoid underflow and allows for easier combination of logarithms."
    },
    {
        "question": "What kind of toolkits can be used within Python for language modeling?",
        "answer": "Quite sophisticated toolkits"
    },
    {
        "question": "What did a group of people do in 2006 with the Google n-gram?",
        "answer": "They counted all the 1,000 million five-word sequences that occurred at least 40 times."
    },
    {
        "question": "How many sequences of 5 words occur at least 40 times according to the text?",
        "answer": "1,000 million or a billion"
    },
    {
        "question": "What warning is given about the file mentioned in the text?",
        "answer": "The warning is that the file is very big, so one should beware before downloading it."
    },
    {
        "question": "Where can you go to have a look at the n-grams mentioned in the text?",
        "answer": "You can go to ngramsgooglelabs.com"
    },
    {
        "question": "What is the main goal when evaluating a language model's performance?",
        "answer": "The main goal is to determine if the language model assigns higher probability to real sentences rather than ungrammatical or bad sentences."
    },
    {
        "question": "Why does the speaker mention that they don't want to use Google n-grams for their language model?",
        "answer": "The speaker doesn't want to use Google n-grams because it includes everything in English, not just text about text analytics."
    },
    {
        "question": "What religious texts does the research group at Leeds focus on in their research?",
        "answer": "The research group at Leeds focuses on the Quran and Hadith, which are the religious texts of Islam."
    },
    {
        "question": "What is the purpose of having a separate test set when training a machine learning model?",
        "answer": "The purpose of having a separate test set is to see how good the machine learning model is after training on the training set."
    },
    {
        "question": "What is being compared using the evaluation models mentioned in the text?",
        "answer": "The accuracy of a spelling corrector when given language model A and language model B."
    },
    {
        "question": "What is being compared when evaluating the accuracy of the machine translation system for language model A and language model B?",
        "answer": "The accuracy of the machine translation system is being compared for language model A and language model B."
    },
    {
        "question": "What is the difference between intrinsic evaluation and extrinsic evaluation?",
        "answer": "Intrinsic evaluation focuses on looking at only the model itself, while extrinsic evaluation involves testing the model on something else outside of the model."
    },
    {
        "question": "Where do the words 'in' and 'x' come from?",
        "answer": "The words 'in' and 'x' come from the Latin language."
    },
    {
        "question": "Who was Shannon mentioned in the text and what was his profession?",
        "answer": "Shannon was an information theorist."
    },
    {
        "question": "What is the purpose of the game mentioned in the text?",
        "answer": "The purpose of the game is to predict the next word based on a limited window of words."
    },
    {
        "question": "What is the difference between the unigram, bigram, and trigram models in language processing?",
        "answer": "The unigram model looks at individual words, the bigram model looks at pairs of words, and the trigram model looks at sequences of three words."
    },
    {
        "question": "Why are unigrams not very good for predicting according to the text?",
        "answer": "Unigrams are not very good for predicting because you have to look at some context, at least the word before and preferably two words before, to get a good prediction."
    },
    {
        "question": "What is perplexity in the context of language models?",
        "answer": "Perplexity is the inverse probability of the test set, normalized by the number of words."
    },
    {
        "question": "What does the perplexity measure in the context of the text?",
        "answer": "The perplexity measures the probability of a word given the word before, calculated by taking the inverse of the probability of every possible bigram and then taking the n-th root of the product."
    },
    {
        "question": "Why is it not necessary to know the exact mathematics behind data mining for most purposes?",
        "answer": "For data mining purposes, unless you're a theoretician or a mathematician wanting to develop more complicated complexity models, you're not going to need to know the exact maths behind it."
    },
    {
        "question": "According to the text, do you need to understand the mathematics for data mining research?",
        "answer": "No, you don't need to understand the mathematics for data mining research."
    },
    {
        "question": "What is the point of minimizing perplexity according to the text?",
        "answer": "The point of minimizing perplexity is the same as maximizing probability."
    },
    {
        "question": "What can be measured by training on the Wall Street Journal corpus and having a test set of 1 and 1/2 million words?",
        "answer": "The perplexity of a unigram model, of a bigram model, and of a trigram model."
    },
    {
        "question": "According to the text, which n-gram model is typically considered the best based on perplexity as a measure?",
        "answer": "trigram model"
    },
    {
        "question": "What does the Shannon visualization method suggest in terms of predicting the next word in a sentence?",
        "answer": "The Shannon visualization method suggests taking a random bigram, generating its probability, and using that probability to predict the next word."
    },
    {
        "question": "What is the purpose of having at least one example of every bigram in the training set?",
        "answer": "The purpose is to calculate the probability of the overall string by stringing the words together."
    },
    {
        "question": "Who is giving the lecture on machine translation in the text?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "Who is the professor mentioned in the text and where does he work?",
        "answer": "Professor Nizar Habash works as a professor of computer science at New York University, Abu Dhabi."
    },
    {
        "question": "What service is being advertised on British television with speech-to-speech translation?",
        "answer": "Google Translate"
    },
    {
        "question": "What kind of language models is Google interested in developing?",
        "answer": "Google is interested in developing language models which work in general with text of many different alphabets, not just the Roman alphabet."
    },
    {
        "question": "What does World Mapper do with maps of the world?",
        "answer": "World Mapper draws maps of the world and transforms them by using figures or numbers representing features in each country."
    },
    {
        "question": "Which country is likely represented by the big lump in the middle according to the text?",
        "answer": "Egypt"
    },
    {
        "question": "What is the first step in the road map for machine translation discussed in the text?",
        "answer": "The first step is to look at some challenges for machine translation."
    },
    {
        "question": "What is one immediate problem when dealing with different languages according to the text?",
        "answer": "One immediate problem when dealing with different languages is that they have different character sets."
    },
    {
        "question": "What is one example of a language-related challenge mentioned in the text?",
        "answer": "The problem of word boundaries in Chinese"
    },
    {
        "question": "What is an example given in the text to illustrate how a single word can have different translations in another language?",
        "answer": "The example given is the word 'eat', which has two different senses in German."
    },
    {
        "question": "What is the difference between 'essen' and 'fressen' in the German language?",
        "answer": "Essen is a human eating, while fressen is an animal eating."
    },
    {
        "question": "What is the past tense of the verb 'write' according to the text?",
        "answer": "'written'"
    },
    {
        "question": "What can an affix in English become in Arabic according to the text?",
        "answer": "An affix in English might become an infix or even two bits of things in Arabic."
    },
    {
        "question": "How is the plural form of 'cars' indicated in French according to the text?",
        "answer": "The plural form of 'cars' is indicated in French by adding an 's' at the end of 'les'."
    },
    {
        "question": "What does the word 'voiture' mean in French?",
        "answer": "car"
    },
    {
        "question": "How is the phrase 'I am not here' translated in Arabic and French?",
        "answer": "In Arabic, the phrase is equivalent to two words, with the first word meaning 'I am not' and the second word meaning 'here'. In French, the phrase is 'je ne suis pas ici', where 'ne pas' means 'not' and 'am' is in between the two."
    },
    {
        "question": "How does the translation of the English sentence 'John swam across the river quickly' into Spanish differ in terms of the verb used?",
        "answer": "The verb used in the Spanish translation is 'crossing' instead of 'swimming'."
    },
    {
        "question": "What linguistic problem is highlighted in the text regarding the translation of a sentence into different languages?",
        "answer": "The verb in a sentence can vary in different languages when translated, leading to linguistic challenges."
    },
    {
        "question": "What resources are needed to train machine learning models for machine translation?",
        "answer": "A corpus (data set) and a dictionary (or list of words in the language) are needed."
    },
    {
        "question": "Why is it important to have a parallel corpus with a source for text analytics?",
        "answer": "It is important to have a parallel corpus with a source for text analytics in order to align English and French sentences, allowing for translation and analysis."
    },
    {
        "question": "Why is it a problem to find French translations for English words according to the text?",
        "answer": "It is a problem because it is not as easy to find web pages in English with their French translations compared to finding English web pages."
    },
    {
        "question": "What are some of the challenges for machine translation mentioned in the text?",
        "answer": "Some of the challenges for machine translation include the lack of parallel corpora for certain language pairs, such as English Bengali and Bengali Amharic."
    },
    {
        "question": "What is the concept of a machine translation mountain or pyramid?",
        "answer": "The concept involves starting with a source language, translating it into a target language by considering the grammar and meanings of both languages."
    },
    {
        "question": "What is the author's approach to translating from English to French according to the text?",
        "answer": "The author's approach is to do word-for-word gisting, taking each word from the English source sentence and generating the target French sentence."
    },
    {
        "question": "What is the general idea conveyed by the poorly translated sentence 'Envelope her basis out speak experiences then settle at 1988 one methodology'?",
        "answer": "The general idea conveyed by the poorly translated sentence is to 'consider and analyze past experiences before settling on a methodology in 1988'."
    },
    {
        "question": "According to a human translator, what does the phrase 'on the basis of his experiences, a methodology was arrived at in 1988' mean?",
        "answer": "It means that in 1988, a methodology was developed based on someone's experiences."
    },
    {
        "question": "What suggestion is given for transferring the grammatical structures from the source to the target?",
        "answer": "Taking the grammatical structures of the source and transferring them into the grammatical structures of the target."
    },
    {
        "question": "What is the concept described as a phrase in Spanish translating into a single word or verb in English?",
        "answer": "Transfer lexicon"
    },
    {
        "question": "What is suggested as a helpful method for understanding and translating sentences from English to French?",
        "answer": "Having lots of handwritten examples and working out the meaning of the English sentence before converting it into a French sentence."
    },
    {
        "question": "Why did trying to work out hand-drawn graph structures for individual meanings of sentences never work very well?",
        "answer": "Because it gets fairly complicated and doesn't really work out."
    },
    {
        "question": "What type of corpus is being referred to in the text?",
        "answer": "A parallel corpus with lots of English sentences and their French equivalents."
    },
    {
        "question": "What is another word for dictionary mentioned in the text?",
        "answer": "Lexicon"
    },
    {
        "question": "What statistical machine translation toolkit is mentioned in the text and what method does it use?",
        "answer": "The statistical machine translation toolkit mentioned is used to train word alignments and it uses expectation maximization."
    },
    {
        "question": "What is the Spanish translation for the word 'witch'?",
        "answer": "bruja"
    },
    {
        "question": "What did IBM develop a model for in the text?",
        "answer": "IBM developed a model for gisting word for word translation."
    },
    {
        "question": "According to the text, what is the fertility rule for the word 'slap'?",
        "answer": "The fertility rule states that the word 'slap' must be converted into longer phrases, like 'Mary not slap, slap, slap the green witch'."
    },
    {
        "question": "What is the result of applying the distortion rule to the phrase 'verde bruja'?",
        "answer": "The result is 'bruja verde'."
    },
    {
        "question": "What type of languages are more likely to have a reasonable translation using word for word translation?",
        "answer": "Languages that are quite similar, such as Spanish and English, German, or French."
    },
    {
        "question": "What is another way around the challenge of translating languages that are very different from each other?",
        "answer": "Phrase based statistical machine translation"
    },
    {
        "question": "What does the German phrase 'Morgen fliege ich nach Kanada zur konferenz' translate to in English?",
        "answer": "Tomorrow I will fly to the conference in Canada."
    },
    {
        "question": "According to the text, what needs to be swapped around when translating phrases in a machine translation system?",
        "answer": "Whole phrases need to be swapped around, not just individual words."
    },
    {
        "question": "What is the Spanish translation for the word 'slap' in the example provided?",
        "answer": "bofetada"
    },
    {
        "question": "What should the phrase 'green witch' be translated to?",
        "answer": "'green witch' should be translated to 'bruja verde'."
    },
    {
        "question": "What is one way of doing phrase-based machine translation?",
        "answer": "Translating the whole phrase in English into a whole phrase in Spanish."
    },
    {
        "question": "What advantage does the ability to translate a whole complicated phrase or sentence in one go provide?",
        "answer": "The advantage is that you don't have to worry about moving things around."
    },
    {
        "question": "What is the difference between 'interest rate' and 'interest in' according to the text?",
        "answer": "The text states that 'interest rate' and 'interest in' are no longer a single spelling with two different senses, but rather they have two different spellings with two different senses."
    },
    {
        "question": "How does having more data help in improving the translation capabilities of systems like Google Translate?",
        "answer": "Having more data allows systems to learn longer phrases and sentences, eventually enabling them to translate whole phrases and sentences without relying on word-for-word mapping."
    },
    {
        "question": "What does Google Translate invite users to do when they visit the platform?",
        "answer": "Google Translate invites users to say if a translation is good and suggest a better one."
    },
    {
        "question": "What are some suggestions given for exploring machine translation algorithms?",
        "answer": "Some suggestions given include trying out the Giza system or other approaches mentioned in the Jurafsky and Martin textbook."
    },
    {
        "question": "What is the speaker referring to when they mention companies coming to them with questions about machine translation systems?",
        "answer": "The speaker is referring to companies seeking advice on whether a machine translation system is good or which option is better."
    },
    {
        "question": "What are some factors that can be measured to evaluate an IT system?",
        "answer": "Factors that can be measured to evaluate an IT system include the interface, scalability, interaction with existing systems, speed, and correctness of translation."
    },
    {
        "question": "What is the focus of most research in machine translation evaluation?",
        "answer": "Most research in machine translation evaluation focuses on how good the translation is."
    },
    {
        "question": "How is the machine translation system typically applied by humans?",
        "answer": "The machine translation system is typically applied to a test set, which then produces output."
    },
    {
        "question": "What do the standards ranging from 5 to 1 represent when human translators rank the French sentences translated from English?",
        "answer": "The standards ranging from 5 to 1 represent the fidelity or accuracy of the translations. A score of 5 means it's almost perfect with minor corrections needed, while a score of 1 means the translation completely missed the original content."
    },
    {
        "question": "What are the two things mentioned in the text that can be measured when translating a sentence from English to French?",
        "answer": "The two things mentioned are meaning accuracy and fluency/intelligibility."
    },
    {
        "question": "What are the two measures mentioned in the text when it comes to translating a sentence?",
        "answer": "The two measures mentioned are fidelity (or accuracy) and fluency (or intelligibility)."
    },
    {
        "question": "What are the two scores that are typically measured for machine translation output?",
        "answer": "One for accuracy and one for intelligibility"
    },
    {
        "question": "What does the metric BLEU stand for?",
        "answer": "BLEU stands for the bilingual evaluation understudy."
    },
    {
        "question": "How does the process of translating English sentences into French work according to the text?",
        "answer": "The process involves getting three or several translators to translate each sentence, creating a bank of translations for each input, and then calculating a score using the BLEU metric."
    },
    {
        "question": "What is one advantage of the BLEU score mentioned in the text?",
        "answer": "It is particularly inexpensive and language independent."
    },
    {
        "question": "What does BLEU compare in machine translation?",
        "answer": "BLEU compares the machine translation against several human translations to give a standardized score."
    },
    {
        "question": "Who is the famous linguistics researcher mentioned in the text?",
        "answer": "Noam Chomsky"
    },
    {
        "question": "How did the first human translate the French sentence?",
        "answer": "\"All doll jade ideas sleep irately\""
    },
    {
        "question": "Why does the author mention that the translations are done by humans?",
        "answer": "The author mentions that the translations are done by humans to emphasize that they should be acceptable, even if they are not the same."
    },
    {
        "question": "What does the BLEU metric look at to evaluate translations?",
        "answer": "The BLEU metric looks at unigrams and bigrams, individual words and pairs of words, to evaluate translations."
    },
    {
        "question": "How many words from the test sentence appear in any of the outputs or the gold standard references by the human translators?",
        "answer": "Three words from the test sentence appear in the outputs or references: 'colorless', 'ideas', and 'sleep'."
    },
    {
        "question": "What is the unigram precision in the test sentence from the machine translation output?",
        "answer": "4 out of 5"
    },
    {
        "question": "How many bigrams does the phrase 'Colorless green ideas sleep furiously' map onto?",
        "answer": "Four bigrams"
    },
    {
        "question": "What is the BLEU score calculated as in the given text?",
        "answer": "The BLEU score is calculated by multiplying together all the ngram scores and taking the n-th root of the result."
    },
    {
        "question": "What does the BLEU score indicate about the machine translation systems?",
        "answer": "The BLEU score indicates the quality or ranking of the machine translation systems, with a higher score indicating better performance."
    },
    {
        "question": "Why is machine translation difficult according to the text?",
        "answer": "Machine translation is difficult due to various reasons such as different writing systems in languages and differences in word segmentation or morphology."
    },
    {
        "question": "Why is it important to have a parallel corpus when translating from English to French?",
        "answer": "It is important to have a parallel corpus when translating from English to French because you need to have an English corpus and its corresponding French translations for each sentence."
    },
    {
        "question": "What sources provide a huge parallel corpus of United Nations legal texts for multiple languages?",
        "answer": "The United Nations provides a huge parallel corpus of legal texts for languages such as English, French, Chinese, Hindi, and Arabic."
    },
    {
        "question": "What additional tools are needed besides dictionaries for translating between different languages?",
        "answer": "Other tools like segmenters and parses are needed."
    },
    {
        "question": "What are some approaches mentioned for machine translation in the text?",
        "answer": "The text mentions simple word for word mapping, rules for messing about the order, and transfer models."
    },
    {
        "question": "What is one approach mentioned for translation that involves a semantic representation of the input sentence?",
        "answer": "An interlingual approach where the semantic representation of the input sentence is mapped onto the semantic representation in the target language."
    },
    {
        "question": "What are the two main factors to consider when determining if a machine translation system is good?",
        "answer": "The two main factors to consider are accuracy and fluency."
    },
    {
        "question": "What is one method mentioned in the text for accurate translations?",
        "answer": "IBM BLEU"
    },
    {
        "question": "How can the overlap between translation output and human translations be used to evaluate machine translation systems?",
        "answer": "The overlap between translation output and human translations can be counted for unigrams and bigrams, and then used to calculate a score that correlates well with human evaluations of machine translation systems."
    },
    {
        "question": "According to the text, where can one find more details about the algorithms mentioned?",
        "answer": "In the Jurafsky and Martin textbook"
    },
    {
        "question": "Who is the professor mentioned in the text?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "What communities can AI experts join for networking and sharing knowledge and resources?",
        "answer": "AI experts can join 'social media' communities for data mining text analytics professionals."
    },
    {
        "question": "What are some examples of social media platforms mentioned in the text?",
        "answer": "Facebook, LinkedIn, Quora"
    },
    {
        "question": "What are some examples of communities mentioned in the text that were set up to share knowledge, ask questions, and find resources?",
        "answer": "KDnuggets, Kaggle, WEKA, ICAME, ACL, SemEval, and the EI-JRC communities"
    },
    {
        "question": "What is the new name of the company previously known as Facebook?",
        "answer": "Meta, Meta Platforms, doing business as Meta"
    },
    {
        "question": "What is the parent organization of Facebook, Instagram, and WhatsApp?",
        "answer": "Meta"
    },
    {
        "question": "What is the warning given about joining the Facebook Research group?",
        "answer": "The warning is that Facebook users may join the group without actually knowing what they are talking about."
    },
    {
        "question": "What opportunities does Facebook provide for individuals with research proposals?",
        "answer": "Facebook invites people to submit proposals and may provide funding to implement them."
    },
    {
        "question": "What is LinkedIn and how is it similar to Facebook?",
        "answer": "LinkedIn is a networking website for professional contacts, similar to Facebook in that users can post ideas, news, or whatever."
    },
    {
        "question": "How many contacts has the speaker accumulated in the AI and natural language processing field?",
        "answer": "6,000 contacts"
    },
    {
        "question": "What are some of the features mentioned on the author's LinkedIn page?",
        "answer": "The author mentions features such as network, jobs, messaging, notifications, work, and learning on their LinkedIn page."
    },
    {
        "question": "What is Quora and how does it function?",
        "answer": "Quora is a question and answer website where users can post questions and receive answers from other people. It is a platform to gain and share knowledge, ask questions, and connect with individuals who provide unique insights and quality answers."
    },
    {
        "question": "What platform does the speaker suggest using to find answers to technical AI or NLP questions?",
        "answer": "Quora"
    },
    {
        "question": "What is the name of one of the longest established websites in the area of AI and data mining mentioned in the text?",
        "answer": "KDnuggets"
    },
    {
        "question": "What is knowledge discovery about?",
        "answer": "Knowledge discovery is about extracting useful knowledge from data."
    },
    {
        "question": "What can you find on the KDnuggets website?",
        "answer": "News, tutorials, reviews, job adverts, learning posts, and information on how to get certified as a data scientist."
    },
    {
        "question": "What is the text summarizing?",
        "answer": "The main findings in AI research papers from the past year."
    },
    {
        "question": "Why did companies like Google start contributing big data sets for machine learning competitions?",
        "answer": "Companies like Google started contributing big data sets for machine learning competitions because they realized that lots of enthusiasts would try to tackle their tasks and they might actually get some useful software and results out of this."
    },
    {
        "question": "What resources can you access by using Kaggle, which is now owned by Google?",
        "answer": "Kaggle offers a no-setup customizable, Jupyter Notebooks environment, access to free GPUs, and a huge repository of community published data and code."
    },
    {
        "question": "What is archived after the competitions are over?",
        "answer": "The data and discussions are archived, including how the participants developed their projects."
    },
    {
        "question": "Where can you find top competitions, data sets, code, and discussions according to the text?",
        "answer": "Kaggle"
    },
    {
        "question": "What are some examples of data sets mentioned in the text?",
        "answer": "Some examples of data sets mentioned in the text are heart disease mortality data set, sports data sets, health data sets, software trending data, caffeine content of drinks data set, and travel data."
    },
    {
        "question": "What suggestion is given for finding a project idea?",
        "answer": "One suggestion is to go to Kaggle and find a dataset related to what you're doing, or find an interesting dataset on Kaggle and adapt it to your specific interests."
    },
    {
        "question": "What is the name of the software mentioned in the text that stands for Waikato Environment for Knowledge Analysis?",
        "answer": "WEKA"
    },
    {
        "question": "Where was WEKA originally set up and for what purpose?",
        "answer": "WEKA was originally set up at University of Waikato for their machine learning research group."
    },
    {
        "question": "What sets the Waikato website apart from other software tools' websites?",
        "answer": "The Waikato website is much more open and community-based because it is run by enthusiasts only and not by a company trying to sell products."
    },
    {
        "question": "What types of resources are available for users of the software mentioned in the text?",
        "answer": "Documentation, frequently asked questions, books, papers, research projects, and the developers themselves"
    },
    {
        "question": "What is the International Computer Archive of Modern English (ICAME) specifically for?",
        "answer": "ICAME is specifically for corpus linguistics researchers on English, and in particular for English language teaching, teaching English as a second language or a foreign language."
    },
    {
        "question": "What are some examples of English text corpora mentioned in the text?",
        "answer": "The Brown Corpus, the LOB Corpus, The International Corpus of English"
    },
    {
        "question": "What is the purpose of the ICAME conference mentioned in the text?",
        "answer": "The purpose of the ICAME conference is to compare a million words of British English against a million words of American English."
    },
    {
        "question": "What is one of the very useful things that has been available for quite a long time according to the text?",
        "answer": "The CORPORA mailing list"
    },
    {
        "question": "How can you subscribe to the mailing list mentioned in the text?",
        "answer": "By sending an email to CORPORArequest@uib.no with the line 'subscribe'"
    },
    {
        "question": "Which companies present their research at ACL journals and conferences?",
        "answer": "Google, Microsoft, and other industry research labs"
    },
    {
        "question": "What are some examples of special interest groups mentioned in the text?",
        "answer": "Some examples of special interest groups mentioned in the text are SIGWAC (The Special Interest Group on Web as Corpus) and SIGSEM."
    },
    {
        "question": "Where is the Special Interest Group in Semantic Analysis based?",
        "answer": "America"
    },
    {
        "question": "What type of research papers might you find in computational linguistics and natural language processing journals?",
        "answer": "Research papers on projects in text analytics"
    },
    {
        "question": "What are some examples of the different types of journals mentioned in the text?",
        "answer": "Some examples of the different types of journals mentioned in the text are specialised journals on information retrieval, journals on linguistics, and journals on machine learning."
    },
    {
        "question": "What is the name of the conference mentioned in the text and where is it taking place?",
        "answer": "The conference mentioned is the 13th Conference on Language Resources and Evaluation (LREC) and it is happening in Marseilles in June 2022."
    },
    {
        "question": "What is one requirement for attending a workshop or conference mentioned in the text?",
        "answer": "Submitting a research paper"
    },
    {
        "question": "What is the ACL anthology mentioned in the text?",
        "answer": "The ACL anthology is a collection of papers published in conferences and journals related to computational linguistics."
    },
    {
        "question": "What had to be done before PDFs existed in order to convert old journals into PDF format?",
        "answer": "The paper versions of the old journals had to be scanned and converted into PDF."
    },
    {
        "question": "What are some examples of special interest groups mentioned in the text?",
        "answer": "Some examples of special interest groups mentioned in the text are ICAME, Web As Corpus, and Semantic Analysis."
    },
    {
        "question": "What are some of the activities mentioned in the text related to the workshop proceedings?",
        "answer": "Some of the activities mentioned in the text related to the workshop proceedings include writing papers on semantic research and participating in shared tasks, which are competitions involving data sets and specific tasks."
    },
    {
        "question": "What is one of the requirements for entering the competition mentioned in the text?",
        "answer": "Writing an academic paper, 4 to 8 pages long, describing methods, experiments, and results."
    },
    {
        "question": "What criteria do the competition organisers use to select the papers to present at the competition?",
        "answer": "The competition organisers select the best written papers with novel algorithms or methods, not necessarily the best results."
    },
    {
        "question": "What suggestion is given for finding a research topic related to SemEval?",
        "answer": "The suggestion is to look at the past SemEval competitions, choose an interesting task, and modify it to make it more relevant to your company, job, or interests."
    },
    {
        "question": "What can you find by looking at the task and the proceedings of the workshop described in the text?",
        "answer": "You can find a range of papers describing different approaches and their scores."
    },
    {
        "question": "Who has donated quite a few of the SemEval big data sets?",
        "answer": "Google or other companies"
    },
    {
        "question": "What are some examples of social tasks mentioned in the text?",
        "answer": "Some examples of social tasks mentioned in the text are Patronizing and Condescending Language Detection, Multimedia Automatic Misogyny Identification, and Intended Sarcasm Detection in English and Arabic."
    },
    {
        "question": "What type of data sets may be harder to collect compared to English data sets?",
        "answer": "Arabic data sets or other languages"
    },
    {
        "question": "What was task six in the summer of 2019 according to the text?",
        "answer": "Detecting offensive language in the data set"
    },
    {
        "question": "What information is provided in the data set mentioned in the text?",
        "answer": "The data set includes information on whether each tweet or social media post is marked as offensive or not offensive, and if offensive, whether it is a generic slur or targeted at something specific, and whether it is generally nasty or aimed at a particular target."
    },
    {
        "question": "What is ICAME and how did it come about?",
        "answer": "ICAME was a group of professors of core English corpus linguistics who set up a website and gradually grew as a specialized group."
    },
    {
        "question": "What is one way mentioned in the text to foster research and set up communities?",
        "answer": "Having a group around a piece of software like WEKA, forming subgroups within a large group of computational linguists, or funding groups of researchers by the European Union."
    },
    {
        "question": "What is the main purpose of the EU-JRC funding projects mentioned in the text?",
        "answer": "The main purpose of the EU-JRC funding projects is to foster EU collaboration, rather than to conduct research."
    },
    {
        "question": "What does the European Union want to do with research according to the text?",
        "answer": "The European Union wants not just to foster research but actually make money out of it."
    },
    {
        "question": "What is the author suggesting the reader should do if they are interested in becoming an expert in artificial intelligence?",
        "answer": "The author suggests that the reader should join social media communities for data mining and text analytics for professionals."
    },
    {
        "question": "Why would someone go to the mentioned source for knowledge and resources?",
        "answer": "To get ideas for an applied research project proposal and to find other people's work that is related to their proposal."
    },
    {
        "question": "What are some examples of social media platforms and communities mentioned in the text?",
        "answer": "Facebook, LinkedIn, Quora, KDnuggets, Kaggle, and WEKA"
    },
    {
        "question": "What is the professional organization for text analytics professionals and academics mentioned in the text?",
        "answer": "ACL (Association for Computational Linguistics)"
    },
    {
        "question": "What is one suggestion given in the text for students to access resources and knowledge for their coursework and projects?",
        "answer": "Join some online communities"
    },
    {
        "question": "What social media platform does the speaker mention they no longer use?",
        "answer": "Facebook"
    },
    {
        "question": "Who is the lecturer for the module on data mining and text analytics?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "Who is the lecturer introducing the students to in the text?",
        "answer": "Eric Atwell"
    },
    {
        "question": "What is Eric Atwell's area of research?",
        "answer": "Corpus linguistics and artificial intelligence for language, religious text analytics, Arabic and Islamic corpus linguistics, chat bots, and AI for university education."
    },
    {
        "question": "How many units are there in the overall module?",
        "answer": "There are six units in the overall module."
    },
    {
        "question": "What will be covered in unit three of the course?",
        "answer": "Unit three will cover the meanings of words and text, focusing on comparing texts based on how they mean."
    },
    {
        "question": "What are some examples of text analytics discussed in the text?",
        "answer": "The examples of text analytics discussed in the text are text analytics for machine translation and for information extraction."
    },
    {
        "question": "What is a current research interest at Leeds University related to chat bot and text analytics for University education?",
        "answer": "Chat bot and text analytics for University education"
    },
    {
        "question": "Who are the authors of the textbook 'Speech and Language Processing'?",
        "answer": "Dan Jurafsky and James Martin"
    },
    {
        "question": "What edition is currently available for purchase online?",
        "answer": "The second edition"
    },
    {
        "question": "What textbook on data mining is mentioned in the text?",
        "answer": "Data Mining: Practical machine learning tools and techniques by Ian Witten and his colleagues at Waikato University"
    },
    {
        "question": "What will the course cover in relation to machine learning algorithms?",
        "answer": "The course will cover how to use the algorithms to practical effect, but will not cover the details of the algorithms available."
    },
    {
        "question": "Where are research papers from conferences generally published and publicized?",
        "answer": "Research papers from conferences are generally published, publicized at a conference, and then appear in the conference proceedings."
    },
    {
        "question": "What are the two tests that students will have to take in this course?",
        "answer": "The two tests are an online test covering units one and two, and a test two covering units one to six."
    },
    {
        "question": "What is the main requirement at the end of the module mentioned in the text?",
        "answer": "At the end of the module, students have to write up and submit a report of a maximum of seven pages long."
    },
    {
        "question": "What is one suggestion given to help with writing a research proposal?",
        "answer": "To look at the Engineering and Physical Sciences Research Council guidance on how to write research project proposals."
    },
    {
        "question": "What is one requirement for the research proposal mentioned in the text?",
        "answer": "To write six pages of English text"
    },
    {
        "question": "What are some of the components required by EPSRC in addition to the hypothesis, objectives, and work program?",
        "answer": "EPSRC also requires you to have some other parts."
    },
    {
        "question": "What are some questions that the speaker suggests considering about the work being discussed?",
        "answer": "The speaker suggests considering what other work is similar, how the work contributes to knowledge, what is novel about it, and why it is important."
    },
    {
        "question": "What type of diagram should be included in the work plan for an AI project?",
        "answer": "A Gantt chart"
    },
    {
        "question": "What should be included in the research project according to the text?",
        "answer": "At least two data mining or text analytics methods, techniques, or resources introduced in the module."
    },
    {
        "question": "What is the main focus of the machine learning course mentioned in the text?",
        "answer": "The main focus is on learning the algorithms for doing various different sorts of machine learning."
    },
    {
        "question": "What is the focus of data mining in relation to machine learning?",
        "answer": "Data mining focuses on using machine learning as a tool to tackle practical problems."
    },
    {
        "question": "What is data wrangling in the context of data mining?",
        "answer": "Data wrangling involves taking data from various different sources and getting it into the right format so that machine learning can work on it."
    },
    {
        "question": "According to the text, what do data analysts typically spend the majority of their time doing?",
        "answer": "Data analysts typically spend the majority of their time in the process of data wrangling."
    },
    {
        "question": "What is a big part of data mining according to the text?",
        "answer": "Getting hold of data, getting it into the right format, and understanding what is supposed to be."
    },
    {
        "question": "According to the text, what is one of the phases in CRISP-DM that involves machine learning?",
        "answer": "modeling"
    },
    {
        "question": "What are some steps involved in the process of machine learning according to the text?",
        "answer": "Some steps involved in the process of machine learning include understanding the problem, collecting data, figuring out the meaning of the data, checking for data quality issues, and identifying any obvious patterns without using machine learning."
    },
    {
        "question": "What are the typical steps involved in data mining, as mentioned in the text?",
        "answer": "The typical steps involved in data mining include preparing the data, extracting necessary records or fields, cleaning the data, and running it through machine learning tools."
    },
    {
        "question": "What is one of the steps mentioned in using a tool kit like Weka for machine learning?",
        "answer": "Clicking a button for SVN or clicking another button for perceptron"
    },
    {
        "question": "What is text analytics?",
        "answer": "Text analytics is data mining applied to text."
    },
    {
        "question": "What term is often used in industry to refer to the process of analyzing text?",
        "answer": "Text analytics"
    },
    {
        "question": "What is the difference between corpus linguistics and computational linguistics?",
        "answer": "Corpus linguistics deals with data sets of text, while computational linguistics deals with computation."
    },
    {
        "question": "What is the term preferred by the industry for using computational linguistics or NLP as a toolkit for tackling practical problems?",
        "answer": "Text analytics"
    },
    {
        "question": "What does the text suggest about the importance of text data collection, understanding annotation, and wrangling in text analytics compared to theoretical fields of natural language processing?",
        "answer": "The text suggests that these tasks are much more important in text analytics than they are in the theoretical fields of natural language processing."
    },
    {
        "question": "What is one key challenge in applying machine learning algorithms to text data?",
        "answer": "Converting text into vectors of numbers"
    },
    {
        "question": "What is the key thing in text analytics according to the text?",
        "answer": "The key thing in text analytics is having a way of converting the words and sentences into numbers, or vectors of numbers."
    },
    {
        "question": "Who was commissioned by the British Council to write the text?",
        "answer": "Eric Atwell"
    },
    {
        "question": "What did the British Council decide to create a book about?",
        "answer": "computational linguistics"
    },
    {
        "question": "What is the focus of the text regarding language machines?",
        "answer": "The text explores the technological, social, and educational implications of language machines in the future."
    },
    {
        "question": "Why does the speaker mention that a 20-year-old book in IT is still quite relevant?",
        "answer": "The concepts in the book are still applicable and relevant, especially the introductory part which hasn't changed much."
    },
    {
        "question": "Where is the linguistics department located at Leeds University?",
        "answer": "The linguistics department is part of the School of Languages, Cultures, and Societies at Leeds University."
    },
    {
        "question": "What is lexicography and what does it study?",
        "answer": "Lexicography is a study of words or vocabulary items in a dictionary. It studies the way words are written, spoken, their meaning, grammatical function, syntax, and morphemes."
    },
    {
        "question": "What is the difference between semantics, pragmatics, and discourse modeling in the study of language?",
        "answer": "Semantics focuses on the meanings of words and sentences, pragmatics analyzes language in practical use considering the context, and discourse modeling deals with phenomena that span over multiple utterances in a discourse or dialogue."
    },
    {
        "question": "What are academics interested in when it comes to the language machine or text analytics?",
        "answer": "Academics are interested in computer models of language."
    },
    {
        "question": "What are some language resources mentioned in the text that are useful for machine learning and communication between people and computers?",
        "answer": "Some language resources mentioned in the text that are useful for machine learning and communication between people and computers are a corpus, text data set, and a dictionary."
    },
    {
        "question": "How can machine translation assist communication according to the text?",
        "answer": "Machine translation can assist communication by eliminating the need to learn a new language, as mentioned in the example of teaching in China."
    },
    {
        "question": "Why is text analytics considered expensive for the UK and British companies?",
        "answer": "Text analytics is considered expensive for the UK and British companies because big IT companies like Google, Apple, Amazon, Microsoft, IBM, and Facebook have huge resources and big research labs."
    },
    {
        "question": "What type of tools does the individual researcher hope to develop compared to Google research labs?",
        "answer": "The individual researcher hopes to develop very specialist niche tools, like of the Quran, compared to Google research labs which can develop very general text analytics tools."
    },
    {
        "question": "Why do people often struggle to articulate what they want from text analytics?",
        "answer": "People often struggle to articulate what they want from text analytics because they don't know what text analytics can do."
    },
    {
        "question": "Why is it not appropriate to use simple English for tasks like inputting data into a spreadsheet or understanding the data in a spreadsheet?",
        "answer": "It doesn't make sense to use simple English for these tasks because they involve numbers and tables, which cannot be converted into English sentences straightforwardly."
    },
    {
        "question": "What are some challenges mentioned in the text when it comes to switching from using keyboards to using English language to interact with computers?",
        "answer": "Some challenges mentioned include the decreased usefulness of touch screens when talking to a computer, the need for users to receive training and time to learn new interactive methods, and the adjustment from typing essays to dictating them."
    },
    {
        "question": "What is the text analytics research initiative mentioned in the text?",
        "answer": "UK and European Union research initiatives"
    },
    {
        "question": "What is one reason why some of the research projects mentioned in the book may be considered dated?",
        "answer": "The text mentions that the book is 20 years old, so some of the research projects are a bit dated."
    },
    {
        "question": "Why has the UK government decided that it's not worth funding research into speech interfaces anymore?",
        "answer": "The UK government believes that they cannot compete with companies like Amazon Echo and Google who have already developed advanced speech interfaces."
    },
    {
        "question": "Who is involved in the EduBot project at Leeds?",
        "answer": "The speaker is involved in the EduBot project at Leeds with three other universities and two companies over two years."
    },
    {
        "question": "What did BT decide to do in the 1990s in terms of predicting the future of technology?",
        "answer": "BT decided to pay people to try to predict what was coming in the future in terms of technology."
    },
    {
        "question": "What was predicted to be possible by 2005 in terms of interaction with machines?",
        "answer": "Full voice interaction with machines."
    },
    {
        "question": "What technology did not really take off as expected by 2007?",
        "answer": "Domestic robots"
    },
    {
        "question": "Why did robots not take off for domestic duties according to the text?",
        "answer": "Robots did not take off for domestic duties because it is very complicated to clean a house or cook a meal, tasks that were thought to be easy for robots."
    },
    {
        "question": "According to the text, what did the author believe would not be done by robots by 2012?",
        "answer": "Hospital cleaners and hospital nurses"
    },
    {
        "question": "What did they think would be useful for input output by 2025?",
        "answer": "Thought recognition"
    },
    {
        "question": "According to the text, what is one potential way human brain intelligence could be enhanced in the future?",
        "answer": "By linking it to AI"
    },
    {
        "question": "What do soldiers in Bosnia wear on their chests according to the text?",
        "answer": "Soldiers in Bosnia wear a small computer on their chests."
    },
    {
        "question": "What was AltaVista and which company owned it at the time?",
        "answer": "AltaVista was a big web search company owned by the computer giant Digital."
    },
    {
        "question": "What did AltaVista launch on the internet?",
        "answer": "AltaVista launched a free machine translation service on the internet."
    },
    {
        "question": "What is ALF and what service does it provide for Lufthansa?",
        "answer": "ALF is a friendly flight information service for Lufthansa that can hold conversations with callers at 300 airports."
    },
    {
        "question": "Why are you not supposed to listen to email messages or dictate replies while driving?",
        "answer": "Because it can distract you from driving and is not very safe to do."
    },
    {
        "question": "What can you find on the webpage mentioned in the text?",
        "answer": "Some happy songs you can listen to and an overview of the data mining and text analytics module and the assessments that are coming."
    },
    {
        "question": "What type of research project proposal are the students being asked to come up with?",
        "answer": "A data mining and text analytics research project proposal"
    },
    {
        "question": "What does the speaker hope the students will feel comfortable with after reading the book and assessment specification?",
        "answer": "The speaker hopes the students will feel comfortable with what they actually have to do to make sure they all come up with really good research proposals."
    },
    {
        "question": "Who is the author of the textbook on speech and language processing mentioned in the text?",
        "answer": "Dan Jurafsky and James Martin"
    },
    {
        "question": "What is the first issue in text mining as opposed to data mining?",
        "answer": "The first issue in text mining as opposed to data mining is to do with the format of the data."
    },
    {
        "question": "What is the difference between the data format typically used in machine learning and the format of text data?",
        "answer": "The data format typically used in machine learning is a sequence of numbers separated by commas in CSV format, while text data is a sequence of characters made up of words, sentences, and paragraphs."
    },
    {
        "question": "What is the purpose of using regular expressions in processing text?",
        "answer": "Regular expressions are used to specify patterns of text strings, recognize sequences of characters, and recognize similar sequences with minor variations."
    },
    {
        "question": "What does a disjunction mean in the context of capturing woodchuck in its various forms?",
        "answer": "A disjunction means 'or' in the context of capturing woodchuck in its various forms."
    },
    {
        "question": "What are some programming languages and operating systems where regular expressions are available?",
        "answer": "Python, Java, Linux, Unix, OS X"
    },
    {
        "question": "What does the carat or up arrow symbol mean in the context of the text?",
        "answer": "The carat or up arrow symbol means 'not'."
    },
    {
        "question": "What does a little 's' represent in the context of the text?",
        "answer": "Any letter or any character other than 'S'"
    },
    {
        "question": "What is another term for a disjunction or 'or' in programming?",
        "answer": "Pipe letter or character"
    },
    {
        "question": "What does the question mark symbol in regular expressions indicate?",
        "answer": "The question mark symbol in regular expressions indicates an optional previous character."
    },
    {
        "question": "What does the plus symbol mean in the context of the text?",
        "answer": "The plus symbol means one or more of the previous characters."
    },
    {
        "question": "Who are the operators star and plus named after?",
        "answer": "Stephen Kleene"
    },
    {
        "question": "What does the dot operator represent in regular expressions?",
        "answer": "The dot operator in regular expressions represents matching any one character."
    },
    {
        "question": "What does the symbol 'carat' followed by 'A to Z' and 'a to z' mean in the given text?",
        "answer": "The symbol 'carat' followed by 'A to Z' and 'a to z' means the start of a string that is not a capital or lowercase letter."
    },
    {
        "question": "What does a backslash in front of a dot signify in the text?",
        "answer": "A backslash in front of a dot signifies that the dot should be used as a full stop, rather than as a pattern character."
    },
    {
        "question": "What does 'dot dollar' mean in the context of string processing?",
        "answer": "The 'dot dollar' notation refers to the last letter of a string."
    },
    {
        "question": "What is the issue with using the character pattern 't-h-e' to match the word 'the'?",
        "answer": "It will miss out 'the' if it's at the start of a sentence."
    },
    {
        "question": "What specific pattern is being described in the text?",
        "answer": "The pattern described is a letter of the alphabet followed by lowercase t or uppercase T, followed by h-e, followed by not any other letter of the alphabet."
    },
    {
        "question": "Why is theology ruled out in the given context?",
        "answer": "The word 'theology' is ruled out because there is a letter after 'the', which does not match the pattern being discussed."
    },
    {
        "question": "What are false positives and why do we need to look out for them?",
        "answer": "False positives are instances where a search term is incorrectly matched with something that is not the intended target. We need to look out for them because they can lead to inaccurate results."
    },
    {
        "question": "What is an example of a false negative in the text?",
        "answer": "Capital T-h-e is considered a false negative because it is ruled out even though it is actually wanted."
    },
    {
        "question": "What are false positives and false negatives in the context of classification tasks?",
        "answer": "False positives and false negatives are two types of error that occur in classification tasks in data mining and machine learning."
    },
    {
        "question": "What does it mean to increase accuracy or precision according to the text?",
        "answer": "To increase accuracy or precision means to minimize false positives and ensure that anything predicted as being the actual result is indeed the actual result."
    },
    {
        "question": "What is one reason why regular expressions are important in text analytics?",
        "answer": "Regular expressions can be quite good at doing some sort of text processing."
    },
    {
        "question": "What are some ways regular expressions can be used in conjunction with machine learning?",
        "answer": "Regular expressions can be used for pre-processing the data, capturing features for classifiers, and capturing generalizations."
    },
    {
        "question": "What is another term for replacing a regular expression with something else in Python and Unix commands?",
        "answer": "Substitutions"
    },
    {
        "question": "What is an example of a regular expression mentioned in the text?",
        "answer": "c-o-l-o-u-r"
    },
    {
        "question": "What does the regular expression open square brackets 0 to 9 close brackets plus represent?",
        "answer": "Any sequence of matching patterns where the pattern is a digit."
    },
    {
        "question": "What is the process described for converting the pattern in brackets to open angle brackets followed by the pattern and closed angle brackets?",
        "answer": "The process involves replacing the pattern in brackets, which is any sequence of 0 to 9 digits, with open angle bracket, the pattern, and closed angle bracket."
    },
    {
        "question": "What does the pattern 'the, dot, star, er' mean in the text?",
        "answer": "The pattern 'the, dot, star, er' means to match anything followed by 'er'."
    },
    {
        "question": "What are look-ahead assertions and how are they described in the text?",
        "answer": "Look-ahead assertions are complicated patterns described in the text as not very commonly used."
    },
    {
        "question": "What type of pattern is described in the text for matching any sequence of characters other than the word 'volcano'?",
        "answer": "A pattern that matches any single word apart from 'volcano'"
    },
    {
        "question": "What is the name of the famous chat bot mentioned in the text?",
        "answer": "ELIZA"
    },
    {
        "question": "According to the text, what is a potential use of chat bots in the field of mental health?",
        "answer": "Talking to people about their mental health problems"
    },
    {
        "question": "What is a common pattern in the interaction between a psychotherapist and a patient, as described in the text?",
        "answer": "The patient might say 'I need x' and the psychotherapist responds by asking 'What would it mean to you if you got x?'"
    },
    {
        "question": "How can you encourage someone to talk more about a statement they made?",
        "answer": "By repeating it back in a more convoluted way or asking for specific examples."
    },
    {
        "question": "What does the person suggest you can do to show empathy when someone says they are depressed?",
        "answer": "The person suggests that you can repeat back 'depressed' but say, 'I'm sorry to hear you are depressed.'"
    },
    {
        "question": "What is an example of a simple pattern mentioned in the text?",
        "answer": "An example of a simple pattern mentioned in the text is repeating back 'I'm sorry to hear you are depressed or sad' when someone says they are feeling that way."
    },
    {
        "question": "What are the simple patterns that ELIZA has, as mentioned in the text?",
        "answer": "If they say anything followed by always, followed by anything, ELIZA responds with 'can you think of a specific example?'"
    },
    {
        "question": "What is a corpus and how is it typically processed?",
        "answer": "A corpus is a text data set that is typically split up into words. One common task is counting up how many words there are in the corpus."
    },
    {
        "question": "What types of elements, such as 'ooh,' 'ah,' pauses, interruptions, and repeated words, are mentioned as potential challenges in transcriptions of speech?",
        "answer": "Elements like 'ooh,' 'ah,' pauses, interruptions, and repeated words are mentioned as potential challenges in transcriptions of speech."
    },
    {
        "question": "What is the difference between word type and word tokens?",
        "answer": "Word type refers to the different forms a word can take, while word tokens refer to the individual occurrences of a word in a text."
    },
    {
        "question": "How many words are there in the example provided?",
        "answer": "There are two words in the example provided: 'the' and 'the'."
    },
    {
        "question": "How many tokens are there if 'San Francisco' is considered as one word?",
        "answer": "14 tokens"
    },
    {
        "question": "How many types are mentioned in the text?",
        "answer": "12 types"
    },
    {
        "question": "According to the text, why might someone say 'they' and 'there' are basically the same word?",
        "answer": "Someone might say 'they' and 'there' are basically the same word because they are both the same pronoun but in different forms."
    },
    {
        "question": "What is the relationship between the size of the vocabulary and the number of tokens according to Heaps Law?",
        "answer": "The size of the vocabulary is different from the number of tokens."
    },
    {
        "question": "According to the text, how does the vocabulary size relate to the number of word tokens?",
        "answer": "The text suggests that the vocabulary size grows as the number of word tokens grows, and it is usually greater than the square root of the number of tokens."
    },
    {
        "question": "What is one way of measuring the size of a corpus mentioned in the text?",
        "answer": "One way of measuring the size of a corpus is by counting how many words there are."
    },
    {
        "question": "How many word tokens are there in the complete works of Shakespeare according to the text?",
        "answer": "Less than a million word tokens"
    },
    {
        "question": "What was one of the author's first jobs a long time ago?",
        "answer": "One of the author's first jobs a long time ago was to work on the LOB Corpus."
    },
    {
        "question": "How many words of text were collected for the project described in the text?",
        "answer": "About a million words of text were collected for the project."
    },
    {
        "question": "What is one reason why British English and American English in the 1960's were more varied than the works of Shakespeare?",
        "answer": "The British English and American English in the 1960's were more varied because they included newspapers written by lots of different people, novels, books of various sorts, and government reports, while Shakespeare's works were just his own."
    },
    {
        "question": "Why does the text suggest that it's a good idea to work for Google research labs if you want to do research on text?",
        "answer": "Because Google has access to the entire world wide web and a huge amount of text data."
    },
    {
        "question": "What is an Ngram?",
        "answer": "An Ngram is a sequence of N words, where a unigram is one word, a bigram is two words, a trigram is three words, and so on up to five grams."
    },
    {
        "question": "What is the corpus in the text based on?",
        "answer": "The corpus in the text is based on a trillion different Ngrams that appeared on the web."
    },
    {
        "question": "What information should be recorded as metadata when collecting a corpus?",
        "answer": "The language, the variety, and the purpose of the text."
    },
    {
        "question": "Why might English words be present in a Spanish corpus or in conversations among Arabic PhD students?",
        "answer": "English words might be present due to code switching, especially when technical computing terms are involved."
    },
    {
        "question": "What factors should be taken into account when considering the vocabulary likely to be used in a piece of writing?",
        "answer": "The demographics of the author such as age, gender, ethnicity, as well as the type of writing (news, fiction, scientific articles, Wikipedia) should be considered."
    },
    {
        "question": "What information is important to have along with the data when collecting a corpus?",
        "answer": "Metadata about what's in the data"
    },
    {
        "question": "What does the speaker want the listener to do for their exercise this week?",
        "answer": "The speaker wants the listener to collect the corpus and note down metadata about important features of the corpus."
    },
    {
        "question": "What is the first step mentioned in segmenting the text according to the text provided?",
        "answer": "Segment the text into words"
    },
    {
        "question": "Who wrote the paper called 'UNIX for Poets'?",
        "answer": "Ken Church"
    },
    {
        "question": "What can you do with a single line of Unix command when given a text file?",
        "answer": "You can tokenise the text file, extract the words from it, and generate a word frequency list."
    },
    {
        "question": "What does the Unix command 'tr -sc A-Z a-z' do?",
        "answer": "It changes all the non-alpha characters to new lines and puts each word on a new line."
    },
    {
        "question": "What does the 'uniq minus c' command do in the text?",
        "answer": "The 'uniq minus c' command counts up repeats of the same word and merges each repeated version of the same word while counting how many times it appears."
    },
    {
        "question": "What is the current limitation of the sorting and merging process described in the text?",
        "answer": "The current limitation is that it does not merge strings with different capitalization, such as 'AARON' and 'Aaron'."
    },
    {
        "question": "What method is described for breaking up the sonnets by William Shakespeare into words?",
        "answer": "By replacing every sequence of alphabetic characters with a new line"
    },
    {
        "question": "What is the suggested action to take with the Shakespeare text in the given example?",
        "answer": "Translate all the uppercase into lowercase and then sort the counts."
    },
    {
        "question": "What is the purpose of sorting the word counts into frequency order?",
        "answer": "The purpose is to have the most frequent word first, followed by the second most frequent word, and so on."
    },
    {
        "question": "What is the most frequent word mentioned in the text?",
        "answer": "the"
    },
    {
        "question": "Why is the word 'I'd' counted as two words?",
        "answer": "The word 'I'd' is counted as two words because it contains an enclitic, which separates the word 'I' from the word 'D'."
    },
    {
        "question": "What is an example of a word that should be kept together as one word?",
        "answer": "Clitic is an example of a word that should be kept together as one word."
    },
    {
        "question": "What does 'j'ai' mean in French and how is it actually written?",
        "answer": "'J'ai' means 'I have' in French, and it is actually written as 'je ai'."
    },
    {
        "question": "What is the natural language toolkit mentioned in the text?",
        "answer": "NLTK, which stands for the natural language toolkit, is a collection of Python code for natural language processing."
    },
    {
        "question": "What does the example in the text allow you to do with the patterns?",
        "answer": "The example in the text allows you to use the sort of patterns and includes them in the Python code."
    },
    {
        "question": "What makes regular expressions very powerful according to the text?",
        "answer": "Regular expressions are very powerful, particularly if you can combine them in a more complicated way."
    },
    {
        "question": "In Chinese, how are words composed?",
        "answer": "Words in Chinese are composed of characters."
    },
    {
        "question": "Why is deciding what counts as a word quite complicated?",
        "answer": "Deciding what counts as a word is quite complicated because even for linguists, there is no agreement on it."
    },
    {
        "question": "How many words does the character sequence 'Yao Ming reaches the finals' consist of, according to the text?",
        "answer": "5 words"
    },
    {
        "question": "What does the author suggest about the translation of words between English and Chinese?",
        "answer": "The author suggests that one cannot assume the number of words in a Chinese translation based solely on the number of words in the English translation, as Chinese may think differently conceptually than English."
    },
    {
        "question": "How many characters are mentioned in the text and how are they treated in Chinese?",
        "answer": "7 characters are mentioned in the text and in Chinese, each character is commonly treated as a token."
    },
    {
        "question": "What method can be used to tokenise words or meaning units in languages like Thai and Japanese?",
        "answer": "If you have a training data set with segmented Japanese text, you can use a deep-learning neural sequence model to learn segmentation."
    },
    {
        "question": "What is mentioned in the text as a method of text tokenisation in English?",
        "answer": "coding using single character segmentation of English"
    },
    {
        "question": "What is an example of word normalization mentioned in the text?",
        "answer": "The text mentions that 'U-dot-S-dot-A' and 'U-S-A' should both be included as 'USA' for word normalization."
    },
    {
        "question": "Why might you want to reduce all letters to lowercase when performing information retrieval?",
        "answer": "To find matches for the word regardless if it's in lowercase or uppercase."
    },
    {
        "question": "Why is the case of the letters important in text analytics?",
        "answer": "The case of the letters is important in text analytics because different words can have different meanings based on whether they are in upper case or lower case."
    },
    {
        "question": "What is the purpose of representing words by their lemma in a dictionary?",
        "answer": "The purpose is to convert different forms of the same word into a single representation."
    },
    {
        "question": "What is the aim of normalizing the sentence 'So he is reading detective stories' into 'he, be, read, detective, story'?",
        "answer": "The aim is to extract the individual morphemes or bits from the sentence."
    },
    {
        "question": "What is the core meaning-bearing unit in language?",
        "answer": "The stem"
    },
    {
        "question": "What is the purpose of a morphological parser when analyzing words like 'cats' or 'amaren'?",
        "answer": "The purpose of a morphological parser is to parse or analyze words into their constituent parts, such as breaking 'cats' into 'cat' and 's' or 'amaren' into 'amar' and 'n'."
    },
    {
        "question": "What does the author suggest about the words 'cat' and 'cats' in the given text?",
        "answer": "The author suggests that 'cat' and 'cats' are both underlyingly 'cat', and you just need to throw away the suffix without needing to know it's plural."
    },
    {
        "question": "What is the author's opinion on stemming?",
        "answer": "The author believes that stemming is too simplistic."
    },
    {
        "question": "What is the purpose of the stemmer available in NLTK and Weka mentioned in the text?",
        "answer": "The purpose of the stemmer is to apply rules for patterns which match, such as changing 'ational' to 'ate' and removing 'ing' at the end of a word."
    },
    {
        "question": "According to the text, what should you do if you come across the letters S-S-E-S?",
        "answer": "Throw away the E-S"
    },
    {
        "question": "What does it mean when a language is described as agglutinating?",
        "answer": "When a language is described as agglutinating, it means that you can link together lots of words into one word."
    },
    {
        "question": "How does the text discuss the problem of dividing up the text into sentences?",
        "answer": "The text mentions that one way to divide up the text into sentences is by looking for punctuation marks such as full stops, question marks, or exclamation marks."
    },
    {
        "question": "What are some of the different roles that a full stop (period) can play in a sentence?",
        "answer": "A full stop can mark a sentence boundary, an abbreviation, or be a part of a number."
    },
    {
        "question": "What can be used to learn a way of predicting sentence boundaries or part of the word in a text?",
        "answer": "Machine learning, deep learning, or whatever"
    },
    {
        "question": "What can help you identify numbers or percentages in a text?",
        "answer": "A dictionary of abbreviations and rules"
    },
    {
        "question": "What is the topic of Professor Eric Atwell's lecture?",
        "answer": "Word embeddings or meanings and vector semantics"
    },
    {
        "question": "Why does the speaker recommend reading the chapter for more detail?",
        "answer": "The speaker recommends reading the chapter for more detail because they are not going to cover everything in the chapter."
    },
    {
        "question": "What is a word embedding in natural language processing used for?",
        "answer": "A word embedding is used to represent the word meaning for text analysis by encoding the meaning of a word in a real valued vector."
    },
    {
        "question": "What are some ways to make word vectors more realistic and better?",
        "answer": "Using TFIDF (term frequency divided by document frequency) or pointwise mutual information as alternatives to raw frequency."
    },
    {
        "question": "What is the first issue discussed in the text?",
        "answer": "The first issue discussed in the text is 'what do we mean by word meaning?'"
    },
    {
        "question": "Why is using a word as a string of characters or an index to a vocabulary list not very satisfactory in n-gram models or text classification models?",
        "answer": "Using a word as a string of characters or an index to a vocabulary list is not very satisfactory because different strings, such as 'class' and 'classes', or 'wedding' and 'marriage', mean different things and have nothing in common in terms of meaning."
    },
    {
        "question": "What are some examples of applications of AI mentioned in the text?",
        "answer": "Some examples of applications of AI mentioned in the text are in the non-representation module, machine learning, deep learning, computer vision applications, and philosophical logic."
    },
    {
        "question": "Who asked the question 'what is the meaning of life?' in the text?",
        "answer": "Barbara Partee"
    },
    {
        "question": "What is typically used as the label for an image in computer vision classification?",
        "answer": "A character string such as 'dog', 'cat', or the name of a person"
    },
    {
        "question": "What does the Latin word 'desiderate' mean?",
        "answer": "Desirable things"
    },
    {
        "question": "What is the study of lexical semantics?",
        "answer": "The study of lexical semantics is the study of words and their meanings."
    },
    {
        "question": "What does the term 'lemma' mean in the context of a dictionary entry?",
        "answer": "Lemma means a dictionary entry."
    },
    {
        "question": "What is the meaning component of a word called?",
        "answer": "A sense or a concept"
    },
    {
        "question": "What is the definition of synonyms according to the text?",
        "answer": "Synonyms have the same meaning in some or all contexts."
    },
    {
        "question": "Give an example from the text that illustrates how words in the dictionary as synonyms may have different meanings in different contexts.",
        "answer": "The example given in the text is 'mouse,' which means something different in a computer text compared to a biology text."
    },
    {
        "question": "What is the difference between the meanings of 'big sister' and 'large sister' in the text?",
        "answer": "In the text, 'big sister' refers to the older sister, while 'large sister' refers to the sister who wears large size clothing."
    },
    {
        "question": "According to the text, what is the expectation regarding words with different forms?",
        "answer": "Difference in form leads to a difference in meaning."
    },
    {
        "question": "What is the difference between synonyms and words that have similarity in meaning?",
        "answer": "Synonyms are exactly the same, while words with similarity in meaning share some element of meaning but are not exactly the same."
    },
    {
        "question": "How do psychologists typically measure the similarity between words in experiments?",
        "answer": "Psychologists typically give pairs of words to humans and ask them to rate how similar the words are on a scale of 1 to 10."
    },
    {
        "question": "According to the text, how are the words 'vanish' and 'disappear' rated in terms of similarity?",
        "answer": "Nearly a 10"
    },
    {
        "question": "What is the difference between similarity through relatedness and similarity through synonyms?",
        "answer": "Similarity through relatedness refers to words that are related in some way but not necessarily synonyms, while similarity through synonyms refers to words that have the same or similar meanings."
    },
    {
        "question": "What is the difference between the meaning of 'cup' and 'coffee' in the context of the text?",
        "answer": "The text states that 'cup' and 'coffee' are not the same in meaning, but rather in terms of being near synonyms."
    },
    {
        "question": "What is one type of relation mentioned in the text apart from synonyms?",
        "answer": "Antonyms"
    },
    {
        "question": "What did the speaker add to the Arabic word net?",
        "answer": "The speaker added antonym relations to the Arabic word net."
    },
    {
        "question": "How is meaning compared to features in the text?",
        "answer": "Meaning could be broken down into features, similar to how in machine learning an instance is characterized by a number of features."
    },
    {
        "question": "What are some examples of antonyms mentioned in the text?",
        "answer": "Some examples of antonyms mentioned in the text are long and short, fast and slow."
    },
    {
        "question": "What is the difference between the words 'copy' and 'fake' according to the text?",
        "answer": "Copy is considered positive while fake is considered negative."
    },
    {
        "question": "What are some examples of positive sentiment terms and negative sentiment terms mentioned in the text?",
        "answer": "Positive sentiment terms include 'great' and 'love', while negative sentiment terms include 'terrible' and 'toxic'."
    },
    {
        "question": "What are some of the dimensions used to measure sentiment in lexicons?",
        "answer": "Intensity of emotion and degree of control"
    },
    {
        "question": "What is the difference between word senses and word meanings according to the text?",
        "answer": "Word senses refer to the different interpretations or definitions of a word, while word meanings are more straightforward and singular."
    },
    {
        "question": "According to the text, how can word senses be related to each other?",
        "answer": "Word senses can be related to each other in terms of synonyms, antonyms, similarity, frequency of occurrence together, or common connotations."
    },
    {
        "question": "According to Jurafsky and Martin, what do modern NLP algorithms use as a representation of word meanings?",
        "answer": "embeddings"
    },
    {
        "question": "According to the text, what is the standard model in language processing for representing word meanings?",
        "answer": "Vector semantics"
    },
    {
        "question": "According to John Firth, how is a word characterized?",
        "answer": "A word is characterized by the company it keeps."
    },
    {
        "question": "According to the text, how can the meaning of a word be represented?",
        "answer": "The meaning of a word can be represented by looking at the collocations, the words immediately before and after, and by examining the hundred words that appear before and after the word."
    },
    {
        "question": "According to Zelig Harris, how does he define synonyms based on their environments?",
        "answer": "Zelig Harris defines synonyms as words that have almost identical environments."
    },
    {
        "question": "What is the example given in the text to illustrate the challenge of computational understanding of unknown words?",
        "answer": "The example given is the computer not being able to understand that 'little' and 'small' are similar because it doesn't know their meanings."
    },
    {
        "question": "What can you find by searching for 'ong choy' in the British National Corpus, American National Corpus, or N 10 10 in sketch engine?",
        "answer": "Sentences like 'ong choy is delicious sauteed with garlic', 'ong choy is superb over rice', or 'ong choy leaves with salty sauces'"
    },
    {
        "question": "What are some examples of leafy greens mentioned in the text?",
        "answer": "Spinach, chard, and collard greens"
    },
    {
        "question": "How can you find a picture of ong choy according to the text?",
        "answer": "You can Google it and find it ong choy."
    },
    {
        "question": "What does the meaning of a word in language use refer to?",
        "answer": "The meaning of a word in language use refers to its distribution, including its neighboring words or its grammatical environment."
    },
    {
        "question": "What are the three dimensions mentioned when simplifying the concept of word similarity in the text?",
        "answer": "The three dimensions mentioned are pleasantness or valence, arousal or intensity, and dominance or control."
    },
    {
        "question": "What are the three dimensions used to describe the connotation of a word?",
        "answer": "Pleasantness, arousal, and dominance"
    },
    {
        "question": "According to the text, how is meaning defined in vector semantics?",
        "answer": "Meaning is defined by linguistic distribution and using linguistic distribution to populate the vector, giving numbers in the vector."
    },
    {
        "question": "How can we build the semantic space for words automatically?",
        "answer": "By counting up the words which are nearby in a large sample of text."
    },
    {
        "question": "According to the text, where are words like 'good' and 'fantastic' located in terms of sentiment?",
        "answer": "Words like 'good' and 'fantastic' are located in a different part of space in terms of sentiment."
    },
    {
        "question": "What is the author assuming in the text regarding the dimensional space?",
        "answer": "The author is assuming that the reader has 100 dimensional space, which can be squashed down or mapped onto two dimensional space for display."
    },
    {
        "question": "What does embedding mean in the context of mathematics?",
        "answer": "Embedding means taking a large dimensional space and mapping it out into a small dimensional space."
    },
    {
        "question": "How many word types were contained in the LOB corpus?",
        "answer": "About 50,000 word types"
    },
    {
        "question": "Why did the author have to deal with a much smaller vector of the 100 most frequent words only?",
        "answer": "The author had to deal with a much smaller vector of the 100 most frequent words only because representing the meaning of a word as a vector of 50,000 possible context words was too big for processing on computers in 1986."
    },
    {
        "question": "What university mainframe did the author use for running the program mentioned in the text?",
        "answer": "Lancaster University"
    },
    {
        "question": "What is the standard way to represent meanings in natural language processing?",
        "answer": "Using embeddings to represent word meanings"
    },
    {
        "question": "Why should the vectors for 'wedding' and 'marriage' be close in the meaning space?",
        "answer": "Assuming that 'wedding' and 'marriage' appear in the same sorts of contexts, the vectors for these words should be close in the meaning space."
    },
    {
        "question": "What is one challenge mentioned when doing sentiment analysis on just words?",
        "answer": "Having to have a whole lot of features which represent bad words and another whole lot of features representing good words."
    },
    {
        "question": "Why does the author consider the sentence 'this is probably a bad sentence' to be problematic?",
        "answer": "The author considers the sentence 'this is probably a bad sentence' to be problematic because it contains a word that isn't the same as 'terrible', but has a very similar vector, which allows for generalization to similar but unseen words."
    },
    {
        "question": "What was the issue with having a word in the test set that wasn't in the training set before?",
        "answer": "It was considered out of vocabulary and required techniques like add-one smoothing or assigning an unknown value."
    },
    {
        "question": "What is TFIDF and how is it commonly used?",
        "answer": "TFIDF stands for term frequency inverse document frequency. It is commonly used in information retrieval, such as in Google search or library search."
    },
    {
        "question": "What is the purpose of word2vec in the context of building vectors?",
        "answer": "The purpose of word2vec is to map sparse vectors onto smaller but denser vectors by training a classifier to predict whether a word is likely to appear nearby."
    },
    {
        "question": "What are the two different examples mentioned in the text?",
        "answer": "The two different examples mentioned in the text are vector representations of meaning."
    },
    {
        "question": "According to the text, what can happen once you understand the meaning of words?",
        "answer": "You can forget the words."
    },
    {
        "question": "What is a common sort of model in information retrieval?",
        "answer": "Finding the right document or finding the right web page which matches your keywords"
    },
    {
        "question": "What types of documents are mentioned in the text as examples?",
        "answer": "A web page, a verse from the Quran, and plays by William Shakespeare"
    },
    {
        "question": "How many plays by William Shakespeare are mentioned in the text?",
        "answer": "Four plays are mentioned: As you Like It, Twelfth Night, Julius Caesar, and Henry V."
    },
    {
        "question": "How many times does the word 'battle' occur in the play 'Henry V'?",
        "answer": "13 times"
    },
    {
        "question": "What is the meaning of the document vector space in the context of the text?",
        "answer": "The meaning of the document vector space is the visualization of spaces representing different words in a document, such as 'battle' and 'fool', in a two-dimensional space."
    },
    {
        "question": "How many battles does Julius Caesar have in comparison to As You Like It and Twelfth Night?",
        "answer": "Julius Caesar has 7 battles, As You Like It has 1 battle, and Twelfth Night has 0 battles."
    },
    {
        "question": "What distinguishes comedies from the other two types of stories mentioned in the text?",
        "answer": "Comedies have more falls and wit and fewer battles."
    },
    {
        "question": "According to the text, which word is associated with historical characters like Julius Caesar and Henry V?",
        "answer": "battle"
    },
    {
        "question": "What is one way to determine if two words have similar meanings according to the text?",
        "answer": "If their context vectors are similar."
    },
    {
        "question": "According to the text, which word is very common next to 'cherry'?",
        "answer": "pie"
    },
    {
        "question": "According to the text, which term is more frequent: digital or information?",
        "answer": "Information is more frequent than digital."
    },
    {
        "question": "How can similarity be measured in the vector space according to the text?",
        "answer": "Similarity can be measured by the closeness of vectors to each other in the vector space."
    },
    {
        "question": "How is word similarity computed using dot product and cosine similarity?",
        "answer": "Word similarity is computed using dot product and cosine similarity by taking the dot product between two vectors."
    },
    {
        "question": "What is the dot product of two vectors and how is it calculated?",
        "answer": "The dot product of two vectors is calculated by summing together the products of the corresponding values in each vector."
    },
    {
        "question": "What does a dot product provide in terms of similarity match-up between vectors?",
        "answer": "A dot product provides a useful similarity match-up between vectors."
    },
    {
        "question": "What method is suggested for normalizing the vector in the given text?",
        "answer": "Taking into account all the values, squaring them all, adding them together, and then taking the square root"
    },
    {
        "question": "Why is it important to normalize or divide by the frequency when dealing with frequent words like 'of' and 'the'?",
        "answer": "It is important to normalize or divide by the frequency when dealing with frequent words like 'of' and 'the' because these words co-occur with lots of other words, leading to a very high dot product value."
    },
    {
        "question": "What operation is performed with the vectors in the given text?",
        "answer": "The dot product is performed with the vectors."
    },
    {
        "question": "What is the relationship between normalizing the dot product for vector length and the cosine of the angle between two vectors?",
        "answer": "Normalizing the dot product for vector length is the same as calculating the cosine of the angle between the two vectors."
    },
    {
        "question": "What does the cosine value of 1 indicate in the context of the dot product of vectors?",
        "answer": "The cosine value of 1 indicates that the words are very similar."
    },
    {
        "question": "What is a key characteristic of the raw frequencies mentioned in the text?",
        "answer": "The raw frequencies are non-negative, meaning they do not have any negative values."
    },
    {
        "question": "What word occurs very frequently with pie in the example?",
        "answer": "Cherry"
    },
    {
        "question": "What mathematical operations are being performed to calculate the cosine metric between 'cherry' and 'information'?",
        "answer": "The mathematical operations being performed are multiplication, addition, and division."
    },
    {
        "question": "What is the normalised value overall after calculating the normalising value described in the text?",
        "answer": "0.017"
    },
    {
        "question": "What is the point of using the cosine metric in the context described in the text?",
        "answer": "The point of using the cosine metric is to measure the distance between 'cherry' and 'information', which is quite small and close to zero."
    },
    {
        "question": "What is the significance of the angle between digital information in the two-dimensional diagram?",
        "answer": "The significance of the angle between digital information in the two-dimensional diagram is that it is very small, which results in the cosine being nearly one."
    },
    {
        "question": "According to the text, what does an angle of 90 degrees represent in terms of cosine values?",
        "answer": "An angle of 90 degrees represents a cosine value of 0."
    },
    {
        "question": "What are some problems associated with raw frequencies when computing word similarity?",
        "answer": "Raw frequencies tend to over empower very frequent words and do not deal well with lower frequency words or comparing low frequency with high frequency words."
    },
    {
        "question": "What is the author's main point about the relationship between the words 'sugar' and 'apricot'?",
        "answer": "The author suggests that 'sugar' is closer in meaning to 'apricot' compared to words like 'the'."
    },
    {
        "question": "What type of words could the University mainframe computer cope with when running for a month?",
        "answer": "The computer could only cope with the most frequent words, such as 'the', 'and', and 'of'."
    },
    {
        "question": "What is TFIDF and how does it help in balancing the importance of words in a document?",
        "answer": "TFIDF stands for term frequency multiplied by inverse document frequency. It helps in balancing the importance of words by giving less weight to common words like 'the' and 'it' which have a low inverse document frequency."
    },
    {
        "question": "What is one way to assign higher weight to words like sugar in a text?",
        "answer": "One way to assign higher weight to words like sugar in a text is by using inverse document frequency."
    },
    {
        "question": "What is the concept discussed in the text that downgrades the effect of a common word like 'the'?",
        "answer": "Dividing by the frequency of the individual word"
    },
    {
        "question": "Why is it important to add one or do something to normalize when counting the frequency of a word in a corpus?",
        "answer": "To deal with 0 counts and ensure that the count is not zero."
    },
    {
        "question": "Why is it better to take logarithms when dealing with probabilities?",
        "answer": "It is better to take logarithms when dealing with probabilities because probabilities are very small numbers in the range of 0 to 1 or fractions, and when multiplying several fractions together, the result is vanishingly small fractions."
    },
    {
        "question": "What is the term frequency formula mentioned in the text?",
        "answer": "The term frequency is essentially the logarithm of the count plus one."
    },
    {
        "question": "What is the inverse document frequency of the word 'Romeo' in the corpus of Shakespeare?",
        "answer": "1.57"
    },
    {
        "question": "What is the inverse document frequency of the word 'salad' in the given text?",
        "answer": "1.27"
    },
    {
        "question": "What is considered a unit in terms of documentation in the context of Wikipedia and Quran AI research?",
        "answer": "Each Wikipedia article counts as a document in the context of Wikipedia, while each verse could be considered a document in Quran AI research."
    },
    {
        "question": "What does the TFIDF weight value for a word represent?",
        "answer": "The TFIDF weight value for a word represents the term frequency (how many times the term appears in the overall corpus) multiplied by the inverse document frequency (or divided by the document frequency)."
    },
    {
        "question": "Why does the word 'good' receive a zero score in TFIDF calculations?",
        "answer": "The word 'good' receives a zero score in TFIDF calculations because it appears in all of the plays and adds no specific information to any particular document."
    },
    {
        "question": "What does PMI stand for in the context of the text?",
        "answer": "Pointwise Mutual Information"
    },
    {
        "question": "What is the pointwise mutual information between two words?",
        "answer": "The pointwise mutual information between two words is how many times the words occur together more often than if they were independent."
    },
    {
        "question": "Why can the pointwise mutual information be negative?",
        "answer": "The pointwise mutual information can be negative because the logarithm can be negative."
    },
    {
        "question": "What is the probability of words W1 and W2 occurring together, given that the probability of each word is one in a million?",
        "answer": "The probability of words W1 and W2 occurring together is one in a billion, or 10 to the power of -12."
    },
    {
        "question": "What is the calculation for positive PMI (PPMI) between two words?",
        "answer": "The calculation for positive PMI (PPMI) between two words is the probability that they occur together divided by the probability of the first word times the probability of the second word, with the result being the logarithm of that value. If the logarithm is positive, then that value is used. If the logarithm is negative, then the value is taken as 0."
    },
    {
        "question": "What does PPMI represent in the text?",
        "answer": "PPMI represents a number between 0 and very large, which is calculated based on the probability of words appearing together divided by their independent probabilities."
    },
    {
        "question": "Why do we take the probability as zero if the logarithm turns out to be negative?",
        "answer": "Because the probability is very small."
    },
    {
        "question": "What is the probability of the word 'information' occurring out of all words mentioned in the text?",
        "answer": "0.3399"
    },
    {
        "question": "What is the probability of the word 'data' occurring in the text?",
        "answer": "0.4842"
    },
    {
        "question": "What is the Pointwise Mutual Information (PPMI) score for cherry pie in the given text?",
        "answer": "4.38"
    },
    {
        "question": "What types of pairs receive non-zero scores in the text?",
        "answer": "Pairs that should actually come together and are meaningful in some sense, such as cherry-pie, cherry-sugar, strawberry-pie, strawberry-sugar, digital-computer, and digital-data."
    },
    {
        "question": "What is the problem with very rare words having very high PMI values?",
        "answer": "The problem is that very rare words can have very high PMI values, which is biased towards infrequent events."
    },
    {
        "question": "What is one way of dealing with probabilities mentioned in the text?",
        "answer": "PMI is one way of dealing with probabilities."
    },
    {
        "question": "What are TFIDF and PMI used for in the context of creating vectors?",
        "answer": "TFIDF and PMI are used to give meaning vectors which are long and have values for every word in the vocabulary."
    },
    {
        "question": "How many different words are there in the LOB corpus?",
        "answer": "50,000"
    },
    {
        "question": "What is the meaning of a word represented as in the text?",
        "answer": "The meaning of any word is represented as a vector of values, with the meaning of any word in Shakespeare being a vector of 20,000 values and the meaning of any word in the LOB corpus being a vector of 50,000 values."
    },
    {
        "question": "What is the author's suggestion for making calculations easier when comparing two words?",
        "answer": "The author suggests having vectors with shorter lengths, such as 1,000 or even down to 50 values, where all values are non-zero and within the range of 0 to 1."
    },
    {
        "question": "Why would you prefer to compare to 1,000 value vectors rather than to 50,000 value vectors?",
        "answer": "Because short, dense vectors are preferred over long sparse vectors for faster computation."
    },
    {
        "question": "What is embedding in the context of machine learning?",
        "answer": "Embedding is a way of mapping down or squashing down very large vectors into much smaller vectors, which can be easier to use as features in machine learning."
    },
    {
        "question": "According to the text, why are dense vectors better at capturing synonyms?",
        "answer": "Dense vectors can be much better at capturing synonyms because if you can map the feature vectors for similar words onto the same set of features, then it may be that the vectors are very similar."
    },
    {
        "question": "Why did word2vec become popular according to the text?",
        "answer": "Word2vec became popular because in lots of competitions or practical applications, it works better due to short, dense vectors."
    },
    {
        "question": "What are some methods for obtaining short, dense vectors according to the text?",
        "answer": "The main method that's actually used is neural or deep learning language models like word2vec or GloVe."
    },
    {
        "question": "What are some methods mentioned in the text that are not as widely used as word2vec or GloVe?",
        "answer": "Singular value decomposition, latent semantic analysis, static embeddings, and contextual embeddings like BERT"
    },
    {
        "question": "Who developed Word2vec?",
        "answer": "Google research labs"
    },
    {
        "question": "What can you download from the web pages mentioned in the text?",
        "answer": "You can download the code and the embeddings, which are basically a dictionary."
    },
    {
        "question": "What is a popular embedding method mentioned in the text?",
        "answer": "word2vec"
    },
    {
        "question": "What is the purpose of word2vec according to the text?",
        "answer": "The purpose of word2vec is to predict the best vector for a particular word by analyzing how good it is at predicting whether or not a word will appear."
    },
    {
        "question": "What is the specific parameter example mentioned in the text?",
        "answer": "skip gram with negative sample or SGNS"
    },
    {
        "question": "What does the author warn about when it comes to developing more embedding examples?",
        "answer": "The author warns that developing more embedding examples is the sort of work that might require a project and cannot be done in one unit on the course."
    },
    {
        "question": "What was revolutionary about the approach mentioned in the text?",
        "answer": "The approach was revolutionary because it combined ideas to create a computationally tractable deep learning neural network model and shifted from counting word occurrences to training a classifier for binary prediction."
    },
    {
        "question": "What is the purpose of the classifier mentioned in the text?",
        "answer": "The purpose of the classifier is to determine if a word is likely to appear next to 'apricot' or not likely to appear next to 'apricot'."
    },
    {
        "question": "What is one way to represent the meaning of a word in machine learning, as mentioned in the text?",
        "answer": "By using the weights in the classifier"
    },
    {
        "question": "What type of learning is described in the text?",
        "answer": "Self-supervised learning"
    },
    {
        "question": "What is the concept discussed by Bengio, Colobert, and other researchers, where the words in the text are considered as labels for the target word?",
        "answer": "The concept discussed is that the words in the text can be considered as labels for the target word."
    },
    {
        "question": "What is considered a positive example in the context of the text?",
        "answer": "The target word, t, and a neighboring context word, c, are considered a positive example."
    },
    {
        "question": "What method can be used to train a classifier to distinguish between positive and negative examples in the text?",
        "answer": "Logistic regression"
    },
    {
        "question": "What are the weights in the learning model according to the text?",
        "answer": "The weights in the learning model are the embeddings."
    },
    {
        "question": "What is the goal of training a classifier in the given context?",
        "answer": "The goal is to train a classifier to assign a probability to a candidate word and context pair."
    },
    {
        "question": "What is the relationship between the words 'aardvark' and 'jam' in the given text?",
        "answer": "Aardvark is used as a negative example, while jam is used as a positive example in the text."
    },
    {
        "question": "What is the process described for improving the embeddings of words in the text?",
        "answer": "The process involves starting off with random numbers as the embeddings and gradually improving them so they are more likely to predict correctly and less likely to predict wrong."
    },
    {
        "question": "What is the purpose of having a set of embeddings for all the target words and all the vocabulary words?",
        "answer": "The purpose is to have embeddings for both the target words and the vocabulary words to represent their meanings and relationships in the context."
    },
    {
        "question": "What is the model for word2vec based on?",
        "answer": "The model for word2vec is based on the relationship between words in a context, such as 'apricot' and 'aardvark'."
    },
    {
        "question": "What are some examples of positive instances of the word 'apricot' mentioned in the text?",
        "answer": "Some examples of positive instances of the word 'apricot' mentioned in the text are: apricot tablespoon, apricot jam, and apricot a."
    },
    {
        "question": "Why does the author mention 'stop words' in the text?",
        "answer": "The author mentions 'stop words' because they are words that can be removed from the vocabulary as they are not essential for the context."
    },
    {
        "question": "According to the text, why are more frequent words more likely to appear in negative examples?",
        "answer": "More frequent words are weighted higher so that they are more likely to appear in negative examples."
    },
    {
        "question": "What is the goal of learning in the context described in the text?",
        "answer": "The goal of learning is to adjust the word vectors to maximize the similarity of target word-content word pairs and minimize the similarity of negative pairs."
    },
    {
        "question": "What is mentioned as a starting point for the process described in the text?",
        "answer": "embeddings which are random numbers"
    },
    {
        "question": "What algorithm is used to gradually change values to maximize positive values and downgrade minimal ones?",
        "answer": "Stochastic gradient descent"
    },
    {
        "question": "What approach is taken to adjust word weights in the text analytics process described in the text?",
        "answer": "The approach taken is to make positive pairs more likely and negative pairs less likely over the entire training set by adjusting the word weights."
    },
    {
        "question": "What should be done if apricot and matrix are not going to occur together?",
        "answer": "Decrease the apricot matrix value."
    },
    {
        "question": "What is the common approach to handling the two matrices, target matrix embeddings and context matrix embeddings, in order to represent the meaning of a word?",
        "answer": "It is common to add the target embeddings and context embeddings together to form a single matrix."
    },
    {
        "question": "How are positive examples generated for training the classifier in neural networks?",
        "answer": "Positive examples are generated by taking pairs of words that co-occur in the corpus."
    },
    {
        "question": "How is the process of learning the embeddings described in the text?",
        "answer": "By adding a very small value to positive examples and subtracting a very small value to negative examples repeatedly until the neural network correctly predicts both positive and negative examples."
    },
    {
        "question": "What is the benefit of using word2vec for text analytics purposes?",
        "answer": "The benefit of using word2vec for text analytics purposes is that it provides dense vectors with only 1,000 numbers to represent meanings, as opposed to 50,000 numbers."
    },
    {
        "question": "What are the characteristics of the vectors produced by TFIDF or pointwise mutual information?",
        "answer": "The vectors produced by TFIDF or pointwise mutual information are very large and very sparse."
    },
    {
        "question": "What are some advantages of using embeddings for computation?",
        "answer": "They are much more computable, easier, more efficient, and provide better results."
    },
    {
        "question": "What information could the computers in 1986 provide about words like 'of', 'and', 'in', and 'to'?",
        "answer": "The computers in 1986 could provide information that these words are similar in meaning because they are all prepositions."
    },
    {
        "question": "What is the common characteristic shared by Hogwarts, Sunnydale, and Blandings according to the text?",
        "answer": "They are all names of schools."
    },
    {
        "question": "What three terms are mentioned as being related to Hogwarts in the text?",
        "answer": "Dumbledore, half-blood, and Malfoy"
    },
    {
        "question": "In the text, what activity can you do close to Leeds related to Harry Potter?",
        "answer": "Go to the Harry Potter experience, see how the Harry Potter films were made, go to Hogwarts, and go to Diagon Alley"
    },
    {
        "question": "In the context of the text, what analogy is being made between apple and tree, and grape and vine?",
        "answer": "The analogy being made is that apple is to tree as grape is to vine."
    },
    {
        "question": "What mathematical operation can be performed on word vectors to find a word that is similar to 'queen'?",
        "answer": "subtract the vector for man, add on the vector for woman"
    },
    {
        "question": "According to the text, what vector operation can be used to find a vector similar to the vector of Rome?",
        "answer": "subtract the vector for France, add on the vector for Italy"
    },
    {
        "question": "According to the text, what similarity is observed in the vector space between various pairs of words?",
        "answer": "The distance between brother and sister is similar to the distance between uncle and aunt, man and woman, and Sir and Madam."
    },
    {
        "question": "What is the open area of research mentioned in the text?",
        "answer": "Understanding the limitations of using the method for countries and capitals only."
    },
    {
        "question": "What were some of the words associated with 'gay' in the 1900s and the 1950s according to the text?",
        "answer": "In the 1900s, 'gay' was associated with daunting, flaunting, and daft. In the 1950s, it was associated with bright, witty, and frolicsome."
    },
    {
        "question": "How has the meaning of the word 'gay' changed over time?",
        "answer": "The meaning of the word 'gay' has changed over time and appears to be related to bisexual, homosexual, or lesbian at the end of a century."
    },
    {
        "question": "In the given text, what analogy is used to explain the relationship between 'father' and 'doctor'?",
        "answer": "father is to doctor as mother is to nurse"
    },
    {
        "question": "What is the equivalent for a woman when comparing 'man is to computer programmer as woman is to x'?",
        "answer": "homemaker"
    },
    {
        "question": "Why can algorithms that use embeddings often be gender biased?",
        "answer": "Algorithms that use embeddings can often be gender biased because the training corpus tends to be gender biased in its source."
    },
    {
        "question": "What caution should be taken regarding the meanings in embeddings?",
        "answer": "One should be careful as the meanings in embeddings reflect the biases present in the training data."
    },
    {
        "question": "What is one reason given in the text for using the Google Books corpus?",
        "answer": "For training purposes, you need thousands of millions of words of text, and very large corpora like Google Books or the world wide web over time provide this."
    },
    {
        "question": "What issue do many corpora used for linguistic analysis include?",
        "answer": "Many corpora used for linguistic analysis include ethical bias issues."
    },
    {
        "question": "Why is it not helpful to say the meaning of a word is spelled out letter by letter?",
        "answer": "It doesn't really help for the meanings of words. We want to have vectors of word meanings."
    },
    {
        "question": "What are two different ways mentioned in the text for normalizing frequencies?",
        "answer": "TFIDF and pointwise mutual information"
    },
    {
        "question": "Why is it ideal to have a smaller vector of only 1,000 points instead of 50,000 points?",
        "answer": "Having a smaller vector of only 1,000 points makes the computation more straightforward."
    },
    {
        "question": "What should you watch out for when using vectors for word or document similarity?",
        "answer": "You have to watch out that any biases in the training data will be reflected in the embeddings."
    },
    {
        "question": "What is mentioned about the gender bias in historical accounts?",
        "answer": "Even fairly recent past tends to have gender bias in them."
    },
    {
        "question": "Who is the professor mentioned in the text?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "What is part of speech tagging and named entity recognition in the context of the text?",
        "answer": "Part of speech tagging involves adding to every word in a text its part of speech, while named entity recognition involves labeling entities or names or things or people or places in a text."
    },
    {
        "question": "Who is likely to be more familiar with the concept mentioned in the text?",
        "answer": "Someone who has learned English as a second or foreign language."
    },
    {
        "question": "Who is credited with describing parts of speech around the time of Christ?",
        "answer": "Dionysus Thrax of Alexandria"
    },
    {
        "question": "What are some of the categories into which words were categorized by the scholar?",
        "answer": "Nouns, verbs, pronouns, prepositions, adverbs, conjunctions, participles, and articles"
    },
    {
        "question": "What are the two broad classes of words mentioned in the text?",
        "answer": "The two broad classes of words mentioned in the text are closed class and open class."
    },
    {
        "question": "What are some examples of words that are considered 'determined' in the English language according to the text?",
        "answer": "Pronouns like she, he, and I, and prepositions like on, under, over, and near"
    },
    {
        "question": "What advice was given to the speaker regarding gender differentiation in research proposals?",
        "answer": "The speaker was advised to avoid gender stereotypes by using the pronoun 'they' instead of 'she' or 'he' when describing or referring to a person."
    },
    {
        "question": "What types of words are considered as content words in the text?",
        "answer": "Nouns, verbs, adjectives, adverbs, and possibly interjections like oh, ouch, uh-huh."
    },
    {
        "question": "What is an example of a word being used as a verb in the text?",
        "answer": "Google"
    },
    {
        "question": "What are the two main categories of nouns mentioned in the text?",
        "answer": "The two main categories of nouns mentioned in the text are proper nouns and common nouns."
    },
    {
        "question": "What are some examples of open class words mentioned in the text?",
        "answer": "Adjectives, adverbs, and interjections"
    },
    {
        "question": "What are some examples of closed class words mentioned in the text?",
        "answer": "Determiners, conjunctions, pronouns, prepositions, and particles"
    },
    {
        "question": "What is the author trying to convey about numbers in the English language?",
        "answer": "The author is explaining that numbers in English are made up of a set number of digits and can be combined in various ways to form different numbers."
    },
    {
        "question": "What is part of speech tagging and what does it involve?",
        "answer": "Part of speech tagging involves assigning a speech or tag to each word in the text, and it is made more difficult because you cannot simply look up each word in a dictionary."
    },
    {
        "question": "What is an example of a word that can have more than one part of speech?",
        "answer": "An example of a word that can have more than one part of speech is 'book'."
    },
    {
        "question": "Why is it more difficult to look up words in a dictionary when trying to determine their part of speech?",
        "answer": "It is more difficult because a word may have two or more parts of speech listed in the dictionary, and the context of the sentence needs to be considered to determine the correct part of speech."
    },
    {
        "question": "What is the purpose of developing a universal dependencies tag set?",
        "answer": "To apply to all languages"
    },
    {
        "question": "What are the two main types of classes mentioned in the text?",
        "answer": "The two main types of classes mentioned in the text are open classes and closed classes."
    },
    {
        "question": "Why is punctuation important in text according to the text?",
        "answer": "Punctuation is important in text because it gets its own symbol or tag."
    },
    {
        "question": "What is one way of writing tags for each word in a text, as mentioned in the passage?",
        "answer": "One way of writing tags for each word is to write the word, then a slash, and then the tag."
    },
    {
        "question": "What can you do in Sketch Engine with a word and a tag?",
        "answer": "You can do tagging in Sketch Engine with a word and a tag."
    },
    {
        "question": "What are some differences mentioned in the text regarding tag sets for Arabic and English?",
        "answer": "The text mentions that the tag set for Arabic is different from the tag set for English, and there are also many different tag sets for English."
    },
    {
        "question": "Who developed the International Corpus of English and what did he think of the Brown and LOB tag sets?",
        "answer": "Professor Greenbaum at London University developed the International Corpus of English and he decided that the Brown and LOB tag sets weren't quite right."
    },
    {
        "question": "Why would you want to do part-of-speech tagging?",
        "answer": "To merge different data sets that were tagged in different ways and to work out a way of mapping between different tag sets."
    },
    {
        "question": "Why is the topic discussed in the text interesting for linguists and language teachers?",
        "answer": "The topic is interesting for linguists and language teachers because it involves understanding how nouns and verbs can be used in different ways."
    },
    {
        "question": "Why is it important to identify nouns and verbs when working out the grammatical structure of a sentence?",
        "answer": "It is important to identify nouns and verbs when working out the grammatical structure of a sentence because it helps in understanding the relationships between different parts of the sentence and is crucial for tasks like language learning, syntactic parsing, and machine translation."
    },
    {
        "question": "What is one extra thing you have to do when translating from Spanish to English?",
        "answer": "Swap the adjective and noun around"
    },
    {
        "question": "Why are adjectives particularly important in sentiment analysis?",
        "answer": "Adjectives are important in sentiment analysis because they typically convey emotions or opinions, such as good, happy, nice, bad, or rubbish."
    },
    {
        "question": "What is the homograph in the text that is spelled 'lead' but has different meanings?",
        "answer": "The homograph in the text is 'lead', which can mean a metal, a verb meaning to guide, or a noun referring to a leash for a dog."
    },
    {
        "question": "Why was it difficult to analyze text in the 1980s?",
        "answer": "It was difficult to analyze text in the 1980s because the computers of that time couldn't deal with a million words of text."
    },
    {
        "question": "What percentage of words are considered ambiguous according to the text?",
        "answer": "15%"
    },
    {
        "question": "What is an example of a word that is commonly ambiguous in a corpus of word tokens?",
        "answer": "Back"
    },
    {
        "question": "What part of speech is 'back' in the sentence 'Buy back - back there is a participle adding to the meaning of the word buy'?",
        "answer": "In the sentence, 'back' is a participle."
    },
    {
        "question": "What percentage accuracy do fairly sophisticated models achieve in tagging effort words in a piece of text?",
        "answer": "Fairly sophisticated models achieve around 97% accuracy in tagging effort words in a piece of text."
    },
    {
        "question": "What was the accuracy percentage achieved on the LOB corpus tag by the Lancaster-Oslo/Bergen Corpus tagger in the 1980s?",
        "answer": "95%, 97%"
    },
    {
        "question": "What percentage of words do two different people have problems with when tagging a piece of text?",
        "answer": "3%"
    },
    {
        "question": "What is the baseline model in the text described as?",
        "answer": "The baseline model is described as a stupid model that looks up a word in the dictionary and assigns a tag based on the most frequent tag if the word has more than one tag."
    },
    {
        "question": "What strategy does the text suggest for tagging unknown words?",
        "answer": "The text suggests that for unknown words, if they're not in a dictionary, they should be tagged as nouns, as most new words are made up of nouns."
    },
    {
        "question": "What does a part-of-speech tagger do?",
        "answer": "A part-of-speech tagger looks up each word in a dictionary."
    },
    {
        "question": "How can you determine the part of speech of an ambiguous word?",
        "answer": "By listing the possible parts of speech and working out from the context which one it is."
    },
    {
        "question": "According to the text, what can help determine whether a word is a noun or a verb?",
        "answer": "The presence of the word 'the' before it"
    },
    {
        "question": "What suggestion is given for identifying the part of speech of a word that is not in the dictionary?",
        "answer": "It is suggested to look at the suffixes or prefixes of the word to determine its part of speech."
    },
    {
        "question": "What does the text mention about the use of capital letters in Arabic and Chinese languages?",
        "answer": "The text mentions that in Arabic there are no capital letters, and in Chinese there are also no capital letters."
    },
    {
        "question": "What is the basis of the constraint grammar mentioned in the text?",
        "answer": "The basis of the constraint grammar is ruling out certain possibilities based on the appearance of certain words."
    },
    {
        "question": "Where did the PhD students develop constraint grammars for various languages?",
        "answer": "Helsinki University"
    },
    {
        "question": "What are some common approaches in AI for language processing mentioned in the text?",
        "answer": "Some common approaches in AI for language processing mentioned in the text are supervised machine-learning algorithms, Hidden Markov models, Engram models, and maximum entropy Markov models."
    },
    {
        "question": "What type of training set do neural network models like transformers and BERT need?",
        "answer": "hand-labelled training set"
    },
    {
        "question": "Why is there still a need to develop a hand-labelled training set despite the existence of a tagger?",
        "answer": "The tagger is not perfect and using it alone would not provide perfect training data."
    },
    {
        "question": "What approach did the researchers take to tag the LOB corpus?",
        "answer": "The researchers developed a rule-based system to tag part of the corpus, applied a Hidden Markov model on top of that, and then used it to tag the whole corpus."
    },
    {
        "question": "What is the reason mentioned in the text for not finding it worthwhile to have a 100% perfect training set anymore?",
        "answer": "There is no longer a demand for a large, hand-labelled training set as they make use of other information for models like Hidden Markov models."
    },
    {
        "question": "What is one current research area mentioned in the text related to deep learning and neural network models?",
        "answer": "Part-of-speech tagging"
    },
    {
        "question": "What types of tags are considered named entities?",
        "answer": "Person, location, organisation, geopolitical entity, place"
    },
    {
        "question": "What are the two steps involved in tagging a proper name entity?",
        "answer": "The two steps involved are identifying which words are the entity and determining the type of the entity."
    },
    {
        "question": "By how much did United Airlines increase fares per roundtrip?",
        "answer": "$6"
    },
    {
        "question": "What is the difference between United Airlines and Tim Wagner mentioned in the text?",
        "answer": "United Airlines is an organization, while Tim Wagner is a person."
    },
    {
        "question": "What is the difference between New York and York Street according to the text?",
        "answer": "The text explains that New York is not an entity, but rather York Street, which goes from Leeds to York, is the entity. Later on, a new street was built next to York Street and named the new York Street."
    },
    {
        "question": "What is a common application of identifying entities in text?",
        "answer": "A common application is in sentiment analysis, question answering, and information extraction."
    },
    {
        "question": "What is one of the goals when performing information extraction?",
        "answer": "To find the entities in a text and the relationships between those entities."
    },
    {
        "question": "What is the author discussing in the text regarding the word 'Washington'?",
        "answer": "The author is discussing the ambiguity of the word 'Washington' as it could refer to a person, an organization, a location, a geopolitical entity, or a state."
    },
    {
        "question": "What is one way to handle the challenge of using algorithms designed for part-of-speech tagging on multiple words together?",
        "answer": "One way to handle this challenge is by using BIO tagging, which involves having tags like person, organisation, location, and so on."
    },
    {
        "question": "According to the text, what type of person is Jane and what type of person is Villanova?",
        "answer": "Jane is a begin person and Villanova is an inside person."
    },
    {
        "question": "What is the relationship between the tags 'begin person' and 'inside person' in the given text?",
        "answer": "The tags 'begin person' and 'inside person' are ambiguous for the word 'Jane', but 'begin person' typically occurs after another tag and never after an 'inside person' tag."
    },
    {
        "question": "How many different types of tags are mentioned in the text?",
        "answer": "Four tags are mentioned: Person, organisation, location, and an extra tag."
    },
    {
        "question": "How many tags are there in total when there are n begin tanks and n other tags?",
        "answer": "2n plus 1"
    },
    {
        "question": "What are some variants of the BIO tagging mentioned in the text?",
        "answer": "Some variants mentioned are including extra tags for begin, inside, and other; having only inside and other tags (I tags and O tags); or having begin, inside, and end tags as well."
    },
    {
        "question": "What are some examples of standard algorithms mentioned in the text for information extraction?",
        "answer": "Hidden Markov models for supervised machine learning"
    },
    {
        "question": "What are the two different sorts of tagging mentioned in the text?",
        "answer": "Rule-based systems for named entity extraction and other sorts of tagging"
    },
    {
        "question": "What is one suggested step to take before performing information extraction or machine translation on a text?",
        "answer": "Label each of the words with its part of speech and label each of the entities with its entity category."
    },
    {
        "question": "What should the reader do after reading the text?",
        "answer": "Enjoy the rest of their day"
    },
    {
        "question": "Who is the author of the Speech and Language Processing textbook mentioned in the text?",
        "answer": "Dan Jurafsky and James Martin"
    },
    {
        "question": "What type of classifier is the Naive Bayes classifier mentioned in the text?",
        "answer": "Naive Bayes classifier"
    },
    {
        "question": "What is one example of how the underlying model is applied in learning according to the text?",
        "answer": "Sentiment analysis is a good example."
    },
    {
        "question": "What are some measures mentioned in the text besides accuracy for sentiment classification evaluation?",
        "answer": "Precision, recall, and F measure"
    },
    {
        "question": "What is one ethical issue that can arise when using classification algorithms like Naive Bayes?",
        "answer": "One ethical issue that can arise is the potential for the algorithm to reproduce bias present in the training data."
    },
    {
        "question": "What is one example of text classification mentioned in the text?",
        "answer": "Spam identification"
    },
    {
        "question": "What was a big issue in American history before the existence of Twitter or the internet?",
        "answer": "The issue of independent states like New York or Florida deciding whether or not to join up to become the United States."
    },
    {
        "question": "How did J. Madison and Hamilton try to persuade the people to join the United States?",
        "answer": "They wrote letters or papers."
    },
    {
        "question": "Who were the mathematicians that used Bayesian mathematics to analyze the texts in the 1960s?",
        "answer": "Mosteller and Wallace"
    },
    {
        "question": "What is the purpose of the medical subject hierarchy mentioned in the text?",
        "answer": "The purpose of the medical subject hierarchy is to work out the classes and subclasses in medicine for medical research."
    },
    {
        "question": "What is the article discussing according to the MeSH Subject Category Hierarchy?",
        "answer": "The article is discussing blood supply and drug therapy."
    },
    {
        "question": "What was the tone of the movie review mentioned in the text?",
        "answer": "Positive"
    },
    {
        "question": "What method does the author suggest for conducting a simple sentiment analysis in reviews?",
        "answer": "The author suggests counting up the positive words and counting up the negative words, then determining the overall category based on which count is higher."
    },
    {
        "question": "What does Amazon invite customers to do when they buy something?",
        "answer": "Amazon invites customers to give a review in plain English and a star rating."
    },
    {
        "question": "What is one way to predict whether stocks or shares are going to go up or down?",
        "answer": "One way to predict whether stocks or shares are going to go up or down is to analyze newspaper articles about that stock and see if they predict positive or negative movement."
    },
    {
        "question": "What are the different types of affective states in the Scherer typology mentioned in the text?",
        "answer": "The different types of affective states in the Scherer typology mentioned in the text are emotion, mood, interpersonal stance, attitude, and personality traits."
    },
    {
        "question": "What is sentiment analysis in computational terms primarily focused on?",
        "answer": "Sentiment analysis in computational terms is primarily focused on capturing the attitudes of people when they write things, such as whether they like, love, or hate something."
    },
    {
        "question": "What is sentiment analysis focused on detecting?",
        "answer": "Attitudes"
    },
    {
        "question": "What are some examples of applications of text classification mentioned in the text?",
        "answer": "Some examples of applications of text classification mentioned in the text are sentiment analysis (positive or negative) and spam analysis (spam or not spam)."
    },
    {
        "question": "What are some examples of situations where you might need to identify multiple classes or categories?",
        "answer": "Some examples include having several possible offers, identifying the language of a text, determining the topic of a medical document, or categorizing a research paper."
    },
    {
        "question": "What is the input for the classifier mentioned in the text?",
        "answer": "The input for the classifier is a document and a fixed set of classes."
    },
    {
        "question": "What is one simple solution mentioned in the text to deal with spam emails?",
        "answer": "Having rules such as blacklisting email addresses, detecting the presence of dollars, or identifying phrases like 'you have been selected.'"
    },
    {
        "question": "What is one way to address the issue of building and maintaining expensive rules for spam detection?",
        "answer": "One way to address the issue is to do supervised machine learning, where the input is a document, a set of classes, and a training set of hand-labeled documents."
    },
    {
        "question": "What is one of the challenges for supervised machine learning in text analytics?",
        "answer": "One of the challenges is that experts have to label each document in the training set."
    },
    {
        "question": "What is the importance of the learning algorithm used in the text?",
        "answer": "The importance of the learning algorithm used in the text is not that significant, as various algorithms like Naive Bayes, neural networks, and logistic regression can work equally well."
    },
    {
        "question": "What is the Naive Bayes classifier known for?",
        "answer": "The Naive Bayes classifier is known for making simplifying assumptions that seem to work reasonably well."
    },
    {
        "question": "What is the 'bag of words' assumption mentioned in the text?",
        "answer": "The 'bag of words' assumption is when you consider a document as just a set of words without taking into account the order of the words."
    },
    {
        "question": "What is the bag of words representation?",
        "answer": "The bag of words representation is essentially a list of words that are created in a document and, possibly, how many times they occur."
    },
    {
        "question": "What is the purpose of using a Naive Bayes classifier?",
        "answer": "The purpose of using a Naive Bayes classifier is to predict the class based on a particular combination of words, such as sentiment (yes or no) or good or bad."
    },
    {
        "question": "What is Bayes' rule and how is it used in a Naive Bayes classifier?",
        "answer": "Bayes' rule is a way of determining the probability of a class given a document. In a Naive Bayes classifier, it is used to maximize the probability of a class given a document by calculating the probability of a document given the class, the probability of the class, and the probability of the document."
    },
    {
        "question": "What is the goal when finding the most likely class for a particular document?",
        "answer": "The goal is to maximize the probability of a document given a class times the probability of a class."
    },
    {
        "question": "What is the goal when trying to maximize the probability of a document given a class?",
        "answer": "The goal is to find, for all the possible classes, the probability of a document given the class times the probability of the class, and then determine which one has the highest score."
    },
    {
        "question": "What is the task described in the text related to finding the maximized probability of?",
        "answer": "The task described in the text is related to finding the maximized probability of the set of words in the document given the class."
    },
    {
        "question": "How can we determine the probability of the class according to the text?",
        "answer": "By counting the relative frequencies in a corpus using the labeled training set, which indicates whether each document (e.g. tweet) is positive or negative."
    },
    {
        "question": "What is one challenge in estimating the likelihood of words for a particular class?",
        "answer": "Having a very large number of training examples is required."
    },
    {
        "question": "Why is it important to have a reasonably large corpus when working out the probability of a word given a class?",
        "answer": "It is important to have a reasonably large corpus because we want to work out the probability of each word given a class, which requires lots of examples of each word for each of the classes."
    },
    {
        "question": "According to the text, how is the word counted?",
        "answer": "The word is counted up regardless of where it occurs."
    },
    {
        "question": "According to the text, what assumption is made about the probability of each word given a class?",
        "answer": "The assumption is that the probability of each word given a class is independent of the conditions and independent of other words."
    },
    {
        "question": "According to the text, what is the probability of a particular word sequence, like 'the cat sat on the mat,' given a particular class?",
        "answer": "The probability of a particular word sequence, like 'the cat sat on the mat,' given a particular class, does not depend on the order of the words."
    },
    {
        "question": "How does the Naive Bayes classifier calculate the probability of a document?",
        "answer": "By multiplying together the probabilities of each word given a particular class."
    },
    {
        "question": "How is the Naive Bayes classifier applied to text classification according to the text?",
        "answer": "By multiplying all the word positions in a test document together and determining which one is the highest."
    },
    {
        "question": "Why does multiplying lots of probabilities together in N-gram modeling result in a practical problem?",
        "answer": "Multiplying lots of probabilities together results in floating point underflow because each probability is in the range 0 to 1, leading to very small fractions being multiplied."
    },
    {
        "question": "Why do we use logarithms of numbers and add them together instead of multiplying the numbers directly?",
        "answer": "Using logarithms helps avoid underflow and is more efficient in terms of computation."
    },
    {
        "question": "What method is suggested as a faster alternative to lots of multiplications in the text?",
        "answer": "Sum of the logarithms"
    },
    {
        "question": "Why is Naive Bayes called a linear classifier?",
        "answer": "Naive Bayes is called a linear classifier because it maximizes the sum of weights, making it a linear function of the inputs."
    },
    {
        "question": "What is the Naive Bayes classifier?",
        "answer": "The Naive Bayes classifier is a sort of important thing for mathematicians, used in data mining and learning."
    },
    {
        "question": "What method is suggested for the first attempt to estimate probabilities in the given text?",
        "answer": "Using the frequencies of words in the data as an estimate of probabilities"
    },
    {
        "question": "How do you calculate the fraction of times a word appears in documents of a specific topic or sentiment?",
        "answer": "By creating a mega document for that topic or sentiment, which is formed by concatenating all documents in that category, and then calculating the frequency of the word in that mega document."
    },
    {
        "question": "What is the potential problem mentioned in the text regarding counting the frequency of a word in documents?",
        "answer": "The potential problem is when there are no training documents with a specific word, such as 'fantastic', classified as positive."
    },
    {
        "question": "What is the issue with calculating the probability of the word 'fantastic' being positive if it never occurs in the positive documents?",
        "answer": "The issue is that the probability will be 0 divided by some number, resulting in a probability of 0."
    },
    {
        "question": "Why is a 0 probability a problem in the context of the text?",
        "answer": "A 0 probability is a problem because when multiplying together all the probabilities, a 0 probability will make the overall probability 0."
    },
    {
        "question": "Why is it important to do smoothing when dealing with probabilities of words in a text?",
        "answer": "It is important to do smoothing because if any word has a probability of 0, then the overall probability of the whole text becomes 0."
    },
    {
        "question": "What is the purpose of adding 1 or Laplace smoothing for N-gram modeling and probability modeling in Naive Bayes?",
        "answer": "The purpose is to avoid zero probabilities and handle unseen words by adding 1 to the count."
    },
    {
        "question": "What is the formula for calculating the probability of a particular word, like 'fantastic', in the given text?",
        "answer": "The formula is (count of 'fantastic' in all documents + 1) / (count of all words in the corpus + size of the vocabulary)"
    },
    {
        "question": "How can you calculate the probability of classes in a document corpus?",
        "answer": "By counting up how many documents have a particular class and dividing it by the total number of documents in the corpus."
    },
    {
        "question": "How can the probability of positive and negative be calculated based on the given information?",
        "answer": "The probability of positive is 50 divided by 100, or a half. The probability of negative is also 50 divided by 100, or a half."
    },
    {
        "question": "What is the process described for calculating the probability of a word in a set of documents?",
        "answer": "For each word in the vocabulary, count up how many times it occurs in the set of documents. Then, calculate the probability by dividing the count plus some constant by the total number of words plus some constant times the vocabulary size."
    },
    {
        "question": "How is the probability of an unknown word calculated in the given text?",
        "answer": "The probability of an unknown word is calculated by taking the count of the word 'fabulous' plus 1, and dividing it by the total vocabulary size plus 1."
    },
    {
        "question": "Why was encountering an out-of-vocabulary word in the test data a problem in N-gram modeling?",
        "answer": "Encountering an out-of-vocabulary word in the test data was a problem in N-gram modeling because it would result in a 0 probability, which is undesirable."
    },
    {
        "question": "What suggestion is given for dealing with tweets that contain unknown words?",
        "answer": "The suggestion is to ignore the unknown words and pretend they're not there, rather than trying to calculate any probability."
    },
    {
        "question": "What is one difference between classifiers and N-gram modeling mentioned in the text?",
        "answer": "One difference mentioned is that classifiers focus on known words of positive or negative sentiment, while N-gram modeling focuses on the number of unknown words in different classes."
    },
    {
        "question": "Why does the text suggest that it is not necessary to remove stop words in Naive Bayes algorithms?",
        "answer": "Because words like 'the' and 'uh' will appear in all of the classes in Naive Bayes algorithms."
    },
    {
        "question": "Why does the author suggest that it's not worth the effort to remove 'and' and 'uh' from the tweets?",
        "answer": "The author suggests that it's not worth the effort to remove 'and' and 'uh' from the tweets because Naive Bayes algorithms basically use all of the words and removing them is time consuming."
    },
    {
        "question": "What is the approach to applying Naive Bayes described in the text?",
        "answer": "The approach described in the text is to apply the Naive Bayes algorithm very naively."
    },
    {
        "question": "How many negative examples are mentioned in the text?",
        "answer": "Three negative examples are mentioned in the text."
    },
    {
        "question": "What methodology is mentioned in the text and what is the first step according to it?",
        "answer": "The CRISP-DM methodology is mentioned in the text and the first step according to it is to check for any obvious results."
    },
    {
        "question": "How many negative examples are there in the data set?",
        "answer": "Three"
    },
    {
        "question": "What is the approach taken when encountering the word 'with' in the test instance predictor?",
        "answer": "The approach taken is to forget about the word 'with' and drop it, as it is out of vocabulary."
    },
    {
        "question": "What is the model interested in doing for the words 'predictable', 'no', and 'fun' in the test set?",
        "answer": "The model is interested in adding 1 for each of these words and then dividing by the count of all words in all of the data."
    },
    {
        "question": "How many words are there altogether in the negative set?",
        "answer": "2"
    },
    {
        "question": "How many words are in the entire vocabulary according to the text?",
        "answer": "20"
    },
    {
        "question": "What is the probability of the word 'predictable' appearing in negative tweets?",
        "answer": "2 divided by 34"
    },
    {
        "question": "What is the probability of a word being predictable given that it's positive, according to the text?",
        "answer": "1 divided by 29"
    },
    {
        "question": "What is the probability of a sentence being negative, given that the probability of a negative class is 3 out of 5?",
        "answer": "The probability of a sentence being negative, given that the probability of a negative class is 3 out of 5, is 3 out of 5."
    },
    {
        "question": "What is the probability of a sentence being negative, predictable, and no fun according to the text?",
        "answer": "6.1 times 10 to the power of minus 5, or 6.1"
    },
    {
        "question": "What is the probability of a sentence being 'fun' given that the class is positive?",
        "answer": "2 out of 29"
    },
    {
        "question": "Why do the speakers predict that the score must be negative?",
        "answer": "The speakers predict that the score must be negative because the score for negative is twice, more or less, the score for positive."
    },
    {
        "question": "What does the text suggest is more important than word frequency for tasks like sentiment analysis?",
        "answer": "The occurrence of the word"
    },
    {
        "question": "What is the difference between binary multinominal Naive Bayes and Bernoulli Naive Bayes?",
        "answer": "Binary multinominal Naive Bayes involves counting if a word is present or not, while Bernoulli Naive Bayes is another mathematical model that is not discussed in the text."
    },
    {
        "question": "What is the main concept discussed in the text?",
        "answer": "The main concept discussed in the text is removing duplicates of words from a document to retain only a single instance of each word."
    },
    {
        "question": "How many positive examples are mentioned in the text?",
        "answer": "Two positive examples are mentioned in the text."
    },
    {
        "question": "How many times does the word 'great' appear in the original count?",
        "answer": "Three times"
    },
    {
        "question": "What is the purpose of getting rid of duplicates in the text?",
        "answer": "The purpose is to cut out all the function words like 'was' and 'the' in order to improve the text."
    },
    {
        "question": "What is the reason for the decrease in the count of the word 'and' in the text?",
        "answer": "The second occurrence of 'and' was thrown away during the simplification of the documents."
    },
    {
        "question": "What does the example with the word 'great' in the text demonstrate?",
        "answer": "The example demonstrates that the word 'great' is a good indicator of positive documents because it appears in both positive documents."
    },
    {
        "question": "What is one issue that the text mentions has not been dealt with in sentiment classification?",
        "answer": "Negation"
    },
    {
        "question": "How does negation change the meaning of words in the text?",
        "answer": "Negation changes the meaning of positive words to negative, such as changing 'like' to 'don't like' or 'dismiss' to 'don't dismiss'."
    },
    {
        "question": "What method has been used to negate every word between negation and the end of the clause or phrase?",
        "answer": "Adding a punctuation mark"
    },
    {
        "question": "What does the speaker emphasize as important in their statement about the movie?",
        "answer": "The speaker emphasizes that 'not like' is important in their statement about the movie."
    },
    {
        "question": "Why is it important to have lots of labeled training data according to the text?",
        "answer": "It is important to have lots of labeled training data because we need to have lots of examples of tweets where someone's read them and said they're positive or they're negative."
    },
    {
        "question": "What are prebuilt lexicons used for in the text?",
        "answer": "Prebuilt lexicons are used to identify particular words that are definitely positive."
    },
    {
        "question": "What kind of words can be found in the MPQA Subjectivity Cues Lexicon mentioned in the text?",
        "answer": "The MPQA Subjectivity Cues Lexicon contains lots of positive words like admirable, beautiful, confident, dazzling, and lots of negative words like awful, bad, bias, and so on."
    },
    {
        "question": "What is the purpose of the General Inquirer mentioned in the text?",
        "answer": "The purpose of the General Inquirer is to classify sentiment by counting positive and negative words found in the text."
    },
    {
        "question": "What are the limitations of using positive and negative lexicons for sentiment analysis?",
        "answer": "The limitations include being restricted to the words in the lexicon and not utilizing all words for analysis."
    },
    {
        "question": "Why are some words ignored by the sentiment analysis method mentioned in the text?",
        "answer": "Some words are ignored because they do not fall into the categories of positive or negative words like 'good' and 'bad'."
    },
    {
        "question": "What type of task is spam filtering considered to be?",
        "answer": "Spam filtering is considered to be a sort of classification task."
    },
    {
        "question": "What are some examples of features that an expert might consider as good indicators of spam?",
        "answer": "Examples of features that an expert might consider as good indicators of spam include mentions of millions of dollars, From address starting with a lot of numbers, and subject being all capitals."
    },
    {
        "question": "What is one application of Naive Bayes mentioned in the text?",
        "answer": "Language identification"
    },
    {
        "question": "Why is it important to have a good variety of each language when building a classifier?",
        "answer": "It is important to have a good variety of each language when building a classifier because including different variations of the language (e.g. American English, British English, African English, Indian English) helps improve the accuracy and effectiveness of the classifier."
    },
    {
        "question": "What are some advantages of Naive Bayes over deep learning and other machine learning methods?",
        "answer": "Naive Bayes is very fast, has low storage requirements, and works reasonably well with even small amounts of training data."
    },
    {
        "question": "According to the text, what should you try first before moving on to deep learning?",
        "answer": "Naive Bayes and other very simple methods"
    },
    {
        "question": "In what domain is it problematic to use decision trees like j48 when there are thousands of features?",
        "answer": "Sentiment analysis"
    },
    {
        "question": "What does Naive Bayes rely on in text classification?",
        "answer": "Naive Bayes relies on independence, assuming that each word is positive or negative regardless of the other words."
    },
    {
        "question": "Why is it important to start with a baseline model before trying more complicated models?",
        "answer": "It's important to start with a baseline model to see if other models make much improvement. If they don't, then there's no point in using a more complicated model."
    },
    {
        "question": "What modeling technique is mentioned as being similar to N-gram modeling in the text?",
        "answer": "N-gram modeling"
    },
    {
        "question": "What is the author suggesting to look at for text classification?",
        "answer": "The author suggests looking at the class of each word in the text."
    },
    {
        "question": "What does Naive Bayes look at when determining if a text is positive or negative?",
        "answer": "Naive Bayes looks at the words in the text and other features, such as the email address it came from."
    },
    {
        "question": "What is one important factor to consider for fake news detection, according to the text?",
        "answer": "The trustworthiness of the source"
    },
    {
        "question": "What concept in Islam is mentioned in the text?",
        "answer": "hadith"
    },
    {
        "question": "What is a hadith according to the text?",
        "answer": "A hadith is something that Muhammad, the original messenger, said or did, and this was written down and passed on through a chain of people until it was eventually recorded in written form, known as the isnad."
    },
    {
        "question": "What is the isnad and why is it important in understanding the text?",
        "answer": "The isnad is the chain of narrators who the text came from. It is important in understanding the text because it helps verify the authenticity and reliability of the information, similar to fake news detection."
    },
    {
        "question": "What does Naive Bayes classification start to look very similar to when considering all the words in the text?",
        "answer": "N-gram language modeling"
    },
    {
        "question": "What is the concept of a unigram language model as described in the text?",
        "answer": "The concept of a unigram language model is assigning a probability to each word given a class and then multiplying together the probabilities of each word given the class."
    },
    {
        "question": "What is the method mentioned in the text for calculating the probability of a whole sentence?",
        "answer": "Multiplying the probabilities of individual words together"
    },
    {
        "question": "What method is used to determine if a sentence is positive or negative in the text?",
        "answer": "N-gram modeling for positive sentences and N-gram modeling for negative sentences"
    },
    {
        "question": "What are some other metrics mentioned in the text for evaluating classifiers?",
        "answer": "Precision, recall, and F measure"
    },
    {
        "question": "What is the main focus of the Delicious Pie tweet detector mentioned in the text?",
        "answer": "The main focus is to find tweets about Delicious Pie Company as the positive class, while considering all other tweets as the negative class."
    },
    {
        "question": "What is typical in text analytics when it comes to binary classification systems?",
        "answer": "In text analytics, it is typical to have a binary classification system where one class is very small or skewed, and there's a lot of the other class."
    },
    {
        "question": "What are the gold positives in the context of finding Delicious Pie tweets?",
        "answer": "The gold positives are the tweets which are about the pie company."
    },
    {
        "question": "According to the text, which type of labels will dominate the total counts?",
        "answer": "Gold negatives will dominate the total counts."
    },
    {
        "question": "What is accuracy in the context of the text?",
        "answer": "Accuracy is essentially all the correct predictions, the true positives plus the true negatives, divided by all of the labels."
    },
    {
        "question": "What is the factor that dominates the accuracy mentioned in the text?",
        "answer": "True negatives"
    },
    {
        "question": "What is precision in the context of the text?",
        "answer": "Precision is the true positives, the ones you've correctly predicted as being positive, divided by the true positives plus the false positives."
    },
    {
        "question": "What is recall in the context of the text?",
        "answer": "Recall is the true positives divided by the true positives plus the false negatives."
    },
    {
        "question": "What type of classifier is being described in the text?",
        "answer": "0R classifier"
    },
    {
        "question": "Why is the accuracy of 99.99% considered to be a very useless classifier in this case?",
        "answer": "The accuracy of 99.99% is considered to be a very useless classifier in this case because none of the predicted items are about pie, which is the main focus."
    },
    {
        "question": "What is the significance of true positives in the context of precision?",
        "answer": "True positives are the instances that are correctly predicted by the system, and having a high number of true positives leads to a high precision score."
    },
    {
        "question": "What is the recall and precision for a dumb pie classifier that doesn't get any true positives?",
        "answer": "The recall and precision for a dumb pie classifier that doesn't get any true positives are both 0."
    },
    {
        "question": "What is the difference between precision and recall?",
        "answer": "Precision emphasizes the true positives, while recall counts how many times you got the ones you're not interested in correct as well as the ones you are interested in."
    },
    {
        "question": "What measure is used in competitions like SemEval to combine precision and recall?",
        "answer": "F measure"
    },
    {
        "question": "What is the formula for calculating the F1 score?",
        "answer": "2 * precision * recall / (precision + recall)"
    },
    {
        "question": "What is the potential issue that may arise if a test set is not very similar to the training set?",
        "answer": "You may end up with a high score on the training set and a much lower score on the test set."
    },
    {
        "question": "What is the purpose of creating a development test set in machine learning?",
        "answer": "The purpose of creating a development test set is to fine-tune the model separately from the training set and avoid overfitting."
    },
    {
        "question": "What method is mentioned in the text for using as much data as possible for training and development?",
        "answer": "Cross validation method"
    },
    {
        "question": "What is cross validation and how can it be used to create a model for testing?",
        "answer": "Cross validation is a way of combining a data set for training and testing. It can be used to create a development set and a training set, from which the best model can be derived and tested on a separate test set."
    },
    {
        "question": "Why is accuracy not a very safe score for skewed data sets?",
        "answer": "Accuracy is not a very safe score for skewed data sets because most of the data is one class, and only a small amount is the other class. This can be particularly bad if you're trying to find the small class in the data."
    },
    {
        "question": "What are some evaluation metrics mentioned in the text for finding offensive tweets, spam emails, or tweets about pies?",
        "answer": "Precision, recall, and F measure"
    },
    {
        "question": "What is the challenge in terms of evaluation when there are more than two classes in a model?",
        "answer": "The challenge is that you can have precision and recall for each of the classes separately."
    },
    {
        "question": "What are the different categories mentioned for separating emails in the text?",
        "answer": "The different categories mentioned for separating emails are urgent, normal, and spam."
    },
    {
        "question": "What are the two different ways of getting an overall score mentioned in the text?",
        "answer": "The two different ways of getting an overall score mentioned in the text are macro averaging and micro averaging."
    },
    {
        "question": "What is the average precision value calculated for the classes urgent, normal, and spam?",
        "answer": "0.73"
    },
    {
        "question": "What is the macro average precision when precision is calculated for the entire data set?",
        "answer": "0.6"
    },
    {
        "question": "What is the F measure and how is it related to precision and recall?",
        "answer": "The F measure is the combination of precision and recall."
    },
    {
        "question": "What is a potential problem with Naive Bayes classifiers mentioned in the text?",
        "answer": "Naive Bayes classifiers will reproduce whatever is in the training set, including any implicit bias present."
    },
    {
        "question": "Why do sentiment classifiers assign lower sentiment and more negative emotion to sentences with African-American names?",
        "answer": "Sentiment classifiers assign lower sentiment and more negative emotion to sentences with African-American names because in the training set, it was found that sentences with African-American names tended to be associated with negative emotions, leading to this bias being reproduced in the test set."
    },
    {
        "question": "What is a common application of classifiers mentioned in the text?",
        "answer": "Detecting hate speech or abuse or harassment"
    },
    {
        "question": "What is the potential consequence of hate speech directed at gay people, according to the text?",
        "answer": "The potential consequence is censorship of discussion about gay people because nasty things are said about them."
    },
    {
        "question": "Why are sentences about gay people that are not actually toxic being flagged for censorship?",
        "answer": "Because they contain gay words, leading to potential censorship of discussion about these groups."
    },
    {
        "question": "Why was the protagonist unable to find information about sex education on the web?",
        "answer": "The information about sex education was filtered out by her school's filter because it was about sex."
    },
    {
        "question": "What is the idea of model cards mentioned in the text?",
        "answer": "The idea of model cards is to note down all the information about the training data when releasing an algorithm in a data set."
    },
    {
        "question": "What are some key aspects that should be specified in a document about training data according to the text?",
        "answer": "The training data sources, how it was developed, the motivation, the preprocessing steps, the evaluation process, intended use and users, and whether it is targeted towards a specific demographic or environmental group."
    },
    {
        "question": "What topics were discussed in the text?",
        "answer": "The text discussed documentating data sets and algorithms, text classification, Naive Bayes classifier, machine learning, and sentiment analysis."
    },
    {
        "question": "What is a common challenge when evaluating Naive Bayes classifier or text classifiers?",
        "answer": "Skewed data problems"
    },
    {
        "question": "What are better metrics than accuracy for certain purposes?",
        "answer": "Precision and recall"
    },
    {
        "question": "Why is it important to document presumptions made in defining classes and labeling data sets?",
        "answer": "It is important to document presumptions to make it clear that there are inherent biases in the data."
    },
    {
        "question": "Who is the professor mentioned in the text?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "What approach does the text mention in relation to a text analytic challenge called the Morpho Challenge?",
        "answer": "The text mentions the CHEAT approach."
    },
    {
        "question": "Who wrote a paper on the NLTK, Natural Language Toolkit?",
        "answer": "Steven Bird"
    },
    {
        "question": "What can you do with Sketch Engine without any programming?",
        "answer": "You can do text analytics experiments without any programming."
    },
    {
        "question": "What are some features of Weka toolkit mentioned in the text?",
        "answer": "Some features of Weka toolkit mentioned in the text include loading data, running filters, choosing from a wide range of classifiers and algorithms, visualization tools, and data analysis tools."
    },
    {
        "question": "What is the reason for needing to write code when developing and testing new machine learning algorithms?",
        "answer": "For developing and testing new machine learning algorithms, writing code is necessary because the algorithms are new and not available in existing libraries like Weka."
    },
    {
        "question": "What existing tool is mentioned in the text for text analysis?",
        "answer": "NLTK"
    },
    {
        "question": "According to the text, what are some examples of toolkits that contain existing bits of code that can be reused?",
        "answer": "Google Code Archive, NLTK, and many other toolkits"
    },
    {
        "question": "What is the advice given regarding coding in the text?",
        "answer": "The advice given is to not code unless you really have to."
    },
    {
        "question": "What is one example of a tool that can be found by googling 'text analytics Python' or 'computational linguistics Python tools'?",
        "answer": "Natural Language Toolkit"
    },
    {
        "question": "What is NLTK known for in the field of Python programming?",
        "answer": "NLTK is known as the leading platform for building Python programs to work with human language data."
    },
    {
        "question": "What is WordNet and how are words grouped together in it?",
        "answer": "WordNet is a resource where words are grouped together into synsets, which are groups of words with similar meanings."
    },
    {
        "question": "What are some of the text processing libraries mentioned in the text?",
        "answer": "Some of the text processing libraries mentioned in the text are for text classification, text tokenization, stemming, tagging, parsing, and semantic reasoning."
    },
    {
        "question": "What resources are available for users of NLTK outside of the NLTK library itself?",
        "answer": "There are wrappers for industry strength NLP libraries and an active discussion forum."
    },
    {
        "question": "Who are the authors of the textbook on introduction to natural language processing using Python?",
        "answer": "Steven Bird and others"
    },
    {
        "question": "Why did the speaker decide not to use the Python programming textbook for the course?",
        "answer": "The speaker decided not to use the Python programming textbook for the course because it is a bit specialized and they wanted to focus more on theory and underlying methods in the module."
    },
    {
        "question": "What can the NLTK do besides text analysis?",
        "answer": "The NLTK has tools for testing classifiers, displaying results, and working out the grammatical structure of a sentence."
    },
    {
        "question": "What module contains parsed corpora with parse trees attached to each sentence in NLTK?",
        "answer": "Treebank"
    },
    {
        "question": "What is the Wall Street Journal known for in the field of computational linguistics?",
        "answer": "The Wall Street Journal is known for being one of the very first large scale corpus of American English."
    },
    {
        "question": "Who will join the board as a non-executive director on November 29th?",
        "answer": "Peter Vinkin"
    },
    {
        "question": "Who is being mentioned as the new non-executive director in the text?",
        "answer": "Peter Vinkin"
    },
    {
        "question": "What is another toolkit besides NLTK that is widely recommended for use within Python?",
        "answer": "SpaCy"
    },
    {
        "question": "What is one way in which NLTK differs from other tools in terms of resources and purpose?",
        "answer": "NLTK is more designed for educational purposes and has less direct connection to industrial interfaces."
    },
    {
        "question": "What are some examples of specific NLP toolkits mentioned in the text?",
        "answer": "Gensim is mentioned as a toolkit with techniques for topic modelling and modelling semantics with sentences and words as vectors."
    },
    {
        "question": "What is Gensim specifically good for according to the text?",
        "answer": "Gensim is specifically good for topic modelling."
    },
    {
        "question": "What are some examples of toolkits mentioned in the text that are useful for developing new variants of algorithms or trying different algorithms for new tasks?",
        "answer": "Weka and Sketch Engine"
    },
    {
        "question": "What is one recommendation given for those interested in novel AI research?",
        "answer": "Reading conference proceedings papers from the latest AI and computational linguistics conferences."
    },
    {
        "question": "Who conducted a novel piece of research on scaling to very large corpora?",
        "answer": "Microsoft Research Labs"
    },
    {
        "question": "What is the purpose of shared tasks in conferences related to AI and machine learning?",
        "answer": "The purpose of shared tasks is for organizers to provide annotated data sets, evaluation and training data sets, and a specific task for participants to work on."
    },
    {
        "question": "What is an example of a shared task workshop in text analytics mentioned in the text?",
        "answer": "SemEval"
    },
    {
        "question": "What was the focus of the Morpho Challenge mentioned in the text?",
        "answer": "The focus of the Morpho Challenge was unsupervised machine learning segmentation of words into morphemes."
    },
    {
        "question": "How can the word 'unsupervised' be segmented according to the text?",
        "answer": "The word 'unsupervised' can be segmented into 'un,' 'supervise,' and 'duh' or 'un,' 'super,' 'vise,' and 'duh.'"
    },
    {
        "question": "What are some challenges when developing an algorithm for processing text in languages like Finnish and Turkish?",
        "answer": "Some challenges include the long words with multiple morphemes found in Finnish and Turkish, compared to English where most words have just one or two morphemes."
    },
    {
        "question": "What is one way an algorithm can determine morphemes in a language?",
        "answer": "By analyzing repeated patterns in words and identifying common prefixes or suffixes."
    },
    {
        "question": "What does the text suggest about the word 'unsupervised'?",
        "answer": "The text suggests that 'unsupervised' consists of the morphemes 'un', 'supervise', and 'ed'."
    },
    {
        "question": "What programming language did the author use to code the machine learning algorithms for segmentation of words into morphemes?",
        "answer": "Python"
    },
    {
        "question": "Who allowed Andy to work on developing a more sophisticated version of the algorithm?",
        "answer": "Pearson"
    },
    {
        "question": "What was the task given to the people in the text?",
        "answer": "The task was to come up with an unsupervised machine learning algorithm that can segment words into their morphemes."
    },
    {
        "question": "What was the arrangement for producing results on the test set and sending them off to Helsinki University?",
        "answer": "Students were given a training set and a separate test set, and had to produce results on the test set and send them off to Helsinki University."
    },
    {
        "question": "What is CHEAT in the context of the text?",
        "answer": "CHEAT is an ensemble classifier which combines the results from several different Leeds student entries into a combined system."
    },
    {
        "question": "According to the text, what did Banko and Brill find about using an ensemble of classifiers?",
        "answer": "Banko and Brill found that an ensemble of several classifiers is generally better than one individual classifier."
    },
    {
        "question": "What is the suggestion made in the text regarding recording lectures and talks?",
        "answer": "The suggestion is to video record the lecture and talk, and then put the video recording on the internet for people to see."
    },
    {
        "question": "What company started in 2005 and invited people to upload their own short videos?",
        "answer": "YouTube"
    },
    {
        "question": "What is the maximum duration for a video on TikTok according to the text?",
        "answer": "Two minutes"
    },
    {
        "question": "What website did a group of academics set up to address the issue of lectures being longer than 10 minutes?",
        "answer": "videolectures.net"
    },
    {
        "question": "How was the presentation in the text delivered?",
        "answer": "The presentation was delivered using a PowerPoint slide with pre-recorded voice on each slide, played one by one during the lecture."
    },
    {
        "question": "What challenges may viewers face when trying to watch the video mentioned in the text?",
        "answer": "Viewers may have difficulty watching the video on some current browsers due to the technology used at the time."
    },
    {
        "question": "Why are the backgrounds for the slides different from the backgrounds on other slides?",
        "answer": "The backgrounds for the slides are different because they follow the standard University of Leeds PowerPoint format from 2005."
    },
    {
        "question": "Who is going to give a talk on the CHEAT system?",
        "answer": "The speaker is going to give a talk on the CHEAT system."
    },
    {
        "question": "Who are the individuals mentioned as contributors to the CHEAT Approach to Morpho Challenge 2005?",
        "answer": "Eric Atwell, Andrew Roberts, Karim Ahmad, Adolfo Allendes Osorio, Louis Bonner, Sa Chaudhuri, Min Dang, David Howard, Simon Hughes, Iftikhar Hussain, Li Ki Ching, Nicholas Molaison, Edward Manley, Kalid Rehman, Ross Williamson, and Hong Tung Xiao"
    },
    {
        "question": "What guiding principle is mentioned in the text and who is it inspired by?",
        "answer": "The guiding principle mentioned is to get others to do the work, inspired by Homer Simpson."
    },
    {
        "question": "What type of learning is the Morpho Challenge program considered to be?",
        "answer": "Unsupervised learning"
    },
    {
        "question": "What is the concept of 'triple layer super-sized unsupervised learning' mentioned in the text?",
        "answer": "The concept involves unsupervised learning by the students, then unsupervised learning by the student programs, and finally unsupervised learning by the cheat.py program."
    },
    {
        "question": "What type of learning were the students engaged in when they were not given example answers?",
        "answer": "Unsupervised learning"
    },
    {
        "question": "What was the purpose of the Python program cheat.py mentioned in the text?",
        "answer": "The purpose of the Python program cheat.py was to perform unsupervised learning by reading outputs of other systems line by line and selecting them through majority vote analysis."
    },
    {
        "question": "What criteria does the program use to select the result if there is a tie?",
        "answer": "The program selects the result from the best system, the one with the highest F measure."
    },
    {
        "question": "Who did the author manage to get to develop a more robust cheat2.py program?",
        "answer": "Andrew Roberts"
    },
    {
        "question": "According to the text, how does the performance of the top five systems compare to the existing systems?",
        "answer": "The top five systems perform better than any of the existing systems."
    },
    {
        "question": "What is the CHEAT program mentioned in the text?",
        "answer": "The CHEAT program is actually a committee of unsupervised learners."
    },
    {
        "question": "What is the novel idea in student learning mentioned in the text?",
        "answer": "The novel idea in student learning mentioned in the text is CHEAT, which stands for students implementing learning programs."
    },
    {
        "question": "What is the last word in the text?",
        "answer": "PLAYBACK"
    }
]