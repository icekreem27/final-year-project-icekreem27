[
    {
        "question": "Who are the research fellows mentioned in the text?",
        "answer": "Noorhan Abbas and Tom Pickard"
    },
    {
        "question": "What is one specific example mentioned in the text where chatbots are being used?",
        "answer": "Course evaluations at Leeds University"
    },
    {
        "question": "What are some of the problems mentioned with using tick box questions to get feedback from students?",
        "answer": "The problems mentioned are that you don't get much useful information from tick box responses and some students may get fed up with having to answer them, leading to poor-quality responses."
    },
    {
        "question": "What is the name of the demonstrator developed by Amazon Web Services that allows users to ask questions about their timetable, grades, and lectures?",
        "answer": "Ask LU"
    },
    {
        "question": "What is the purpose of the Coding Tutor system mentioned in the text?",
        "answer": "The Coding Tutor system helps students in introductory programming courses by analyzing the software and providing English language feedback on errors."
    },
    {
        "question": "What was the main use case for the project mentioned in the text?",
        "answer": "Getting feedback from students about the course."
    },
    {
        "question": "What type of courses are most of the courses in the project focused on?",
        "answer": "Most of the courses in the project are computing-oriented or business-oriented."
    },
    {
        "question": "What is the main difference between the traditional method of asking questions and the approach described in the text?",
        "answer": "The main difference is that the approach described in the text involves engaging in a conversation with the student to tease out more responses, rather than just presenting questions for the student to type into."
    },
    {
        "question": "What were the typical scores given by students when asked to rate the course overall on a scale of 1 to 10?",
        "answer": "Most of the students gave scores around 9 or 10."
    },
    {
        "question": "Why does Hubert try to find something other than just accepting that the student is very happy?",
        "answer": "Hubert tries to find something because they heard that some students got harder tasks than others, and wants to ensure fairness in the tasks given."
    },
    {
        "question": "What did the user think of the evaluation method compared to a regular survey?",
        "answer": "The user found the evaluation method very interesting and unique, and preferred it over filling out a form."
    },
    {
        "question": "What unexpected behavior did some computing students exhibit when asked to fill in a survey questionnaire form?",
        "answer": "Some computing students started trying to break the chatbot by asking difficult questions to test its intelligence."
    },
    {
        "question": "What does Hubert say when the user asks about coronavirus?",
        "answer": "Hubert says, 'I'll try to answer that. May be totally wrong. Give me a sec. SARS-CoV-2 virus.'"
    },
    {
        "question": "What feature of the chatbot allowed students to change their answers?",
        "answer": "Students could change their minds and modify their answers."
    },
    {
        "question": "What are some examples of rule-based chatbots mentioned in the text?",
        "answer": "ELIZA, PARRY, AIML, ALICE, Hubert"
    },
    {
        "question": "What are some examples of personal assistants mentioned in the text?",
        "answer": "SIRI for Apple, Alexa from Amazon, Cortana from Microsoft, Google Assistant"
    },
    {
        "question": "What are some of the functions that Alexa can perform, as mentioned in the text?",
        "answer": "Alexa can play music, set timers, chat for fun, answer general knowledge questions, help with booking travel reservations and shopping on Amazon, and assist with mental health issues by allowing conversations with clinicians or chatbots."
    },
    {
        "question": "What are the two kinds of chatbots mentioned in the text?",
        "answer": "The two kinds of chatbots mentioned in the text are chatting conversational agents and targeted task-based dialogue agents."
    },
    {
        "question": "What are the two types of chatbot architectures mentioned in the text?",
        "answer": "The two types of chatbot architectures mentioned are rule-based systems and machine learning from a corpus."
    },
    {
        "question": "What is the Turing test and how did PARRY's responses relate to it?",
        "answer": "The Turing test is a test where human judges decide if they are conversing with an AI agent or a real person. PARRY's responses were so convincing that psychologists examining transcripts couldn't tell if PARRY was a human or a chatbot, leading to the suggestion that PARRY passed the Turing test."
    },
    {
        "question": "According to the text, why do companies like SIRI, Cortana, and Alexa rely on the fact that people aren't that inquisitive about how the underlying algorithm works?",
        "answer": "These companies rely on the fact that people aren't that inquisitive about how the underlying algorithm works because they just want it to apparently work."
    },
    {
        "question": "What is the main focus of task-based dialogue agents mentioned in the text?",
        "answer": "The main focus of task-based dialogue agents is on completing a specific task, such as setting a timer or making a travel reservation."
    },
    {
        "question": "What are some typical slots that a travel reservation system may need to fill?",
        "answer": "Some typical slots that a travel reservation system may need to fill are origin, destination, departure time, departure date, and airline."
    },
    {
        "question": "According to the text, what is the difference between conversing with a graphical user interface and conversing with a chatbot?",
        "answer": "The difference is that when conversing with a chatbot, people start making assumptions that it will talk in a natural human way, unlike with a graphical user interface where the designer can pretty much decide what to put on it."
    },
    {
        "question": "What are some examples of cues that help in turn-taking during a conversation?",
        "answer": "Some examples of cues that help in turn-taking during a conversation include pauses, intonation, interruptions, and knowing when to yield the floor."
    },
    {
        "question": "What does the agent mean by 'two non-stops' in the text?",
        "answer": "The agent is referring to two flights that are happening simultaneously without any stops in between."
    },
    {
        "question": "What are some examples of speech acts mentioned in the text?",
        "answer": "Some examples of speech acts mentioned in the text are directives, commissives, acknowledgments, and grounding principles."
    },
    {
        "question": "Why do agents performing an action require evidence that they have succeeded in performing it?",
        "answer": "Agents require evidence to ensure that they and others involved understand and acknowledge the successful completion of the action."
    },
    {
        "question": "What is the structure of conversations according to the text?",
        "answer": "Conversations have a structure of turn taking or adjacency pairs, where one utterance is followed by a related response. These pairs can include question and answer, proposal and acceptance/rejection, or compliment and downplay."
    },
    {
        "question": "What type of sub-dialogue is described when the system asks for clarification because it didn't understand where the user was going?",
        "answer": "Clarification sub-dialogue"
    },
    {
        "question": "What are the two types of conversation initiatives mentioned in the text?",
        "answer": "The two types of conversation initiatives mentioned are user initiative system and system issue."
    },
    {
        "question": "What is an example of a famous chatbot mentioned in the text?",
        "answer": "ELIZA"
    },
    {
        "question": "What is the purpose of the Rogerian approach in therapy?",
        "answer": "The purpose of the Rogerian approach in therapy is to engage the client in conversation and help them understand their own problems by talking about their problems themselves."
    },
    {
        "question": "What is ELIZA based on and how does it work?",
        "answer": "ELIZA is based on a large set of rules devised by Weizenbaum, where user input is matched against patterns until a match is found. Once a pattern is matched, a transformation rule generates an output."
    },
    {
        "question": "What is the general structure for each keyword in the ELIZA platform?",
        "answer": "The general structure for each keyword involves a pattern followed by several transforms, with keywords ranked from a very specific one to a very general one."
    },
    {
        "question": "What is the function of the memory list in the context of the text?",
        "answer": "The memory list is used to store what the user had input earlier so that it can be referred back to at a later point in the conversation."
    },
    {
        "question": "What ethical implications arose from the implementation of the AI system ELIZA?",
        "answer": "Some people became emotionally involved with the program, leading to concerns about privacy and the storage of conversations without permission."
    },
    {
        "question": "What are some of the global variables in the PARRY model used to study schizophrenia?",
        "answer": "The global variables in the PARRY model used to study schizophrenia are anger, fear, and mistrust."
    },
    {
        "question": "How does PARRY's responses depend on the mental state of the user?",
        "answer": "PARRY's responses depend on the value of the variable, fear or anger. If fear is high, it responds with actions involving running away, while if anger is high, the responses are more hostile."
    },
    {
        "question": "What is the name of the artificial intelligence markup language mentioned in the text?",
        "answer": "AIML (artificial intelligence markup language)"
    },
    {
        "question": "What is the Loebner prize competition and why did ALICE win it multiple times?",
        "answer": "The Loebner prize competition is where chatbots and humans interact to determine which ones are human and which ones are not. ALICE won the prize multiple times because it had more human interaction built into it, making it more plausible."
    },
    {
        "question": "What is the purpose of the FAQchat chatbot mentioned in the text?",
        "answer": "The purpose of the FAQchat chatbot is to find the most similar question in the FAQ and provide the answer to the user."
    },
    {
        "question": "What type of chatbots are particularly good for general conversations?",
        "answer": "Corpus-based chatbots are particularly good for general conversations."
    },
    {
        "question": "What are some examples of corpora mentioned in the text?",
        "answer": "Some examples of corpora mentioned in the text are the British National Corpus, the International Corpus of English (ICE), and the Korpus Gesproke Afrikaans."
    },
    {
        "question": "What are some examples of data sets mentioned in the text that can be used to train a chatbot?",
        "answer": "Transcripts of telephone conversations, frequently asked questions, IT help desk data sets, and movie subtitles"
    },
    {
        "question": "What is one ethical issue mentioned in the text regarding hiring people to have conversations for a specific topic?",
        "answer": "One ethical issue mentioned is the need to remove personally identifiable information to protect individuals' privacy."
    },
    {
        "question": "What method is used to find the turn in the corpus that is most similar to a user question in the neural method described in the text?",
        "answer": "BERT is used to find the turn in the corpus that is most similar to a user question in the neural method."
    },
    {
        "question": "How does the neural network mentioned in the text generate responses?",
        "answer": "The neural network generates responses by conditioning the encoding of the query and the response so far."
    },
    {
        "question": "What is one example given in the text to illustrate the concept of being 'stuck'?",
        "answer": "The example given is when someone says 'See you later' in response to 'See you later'."
    },
    {
        "question": "What are the potential issues with rule-based chatbots and information retrieval-based chatbots mentioned in the text?",
        "answer": "Rule-based chatbots may give the appearance of understanding but can still give silly answers unexpectedly. Information retrieval-based chatbots are as good as their training data, so bad training data can result in bad responses."
    },
    {
        "question": "What is the purpose of the dialogue state tracker mentioned in the text?",
        "answer": "The purpose of the dialogue state tracker is to maintain the current state of the dialogue."
    },
    {
        "question": "What does the system need to know when the user asks 'where is it?'",
        "answer": "The system needs to know that the user is asking for an address, in addition to wanting cheap Thai food and a location in the center of the city."
    },
    {
        "question": "How are chatbots typically evaluated according to the text?",
        "answer": "Chatbots are typically evaluated by humans through tasks like booking a ticket, conversational chat, human evaluation, observer evaluation, and the Turing test."
    },
    {
        "question": "What are some of the dimensions of quality that can be evaluated when a human chats with models according to the text?",
        "answer": "Some of the dimensions of quality that can be evaluated include avoiding repetition, being interesting, making sense, fluency, listening ability, inquisitiveness, human-likeness, and engagement."
    },
    {
        "question": "What is one way to evaluate chatbots according to the text?",
        "answer": "One way to evaluate chatbots is to have people have conversations with chatbots and then have annotators analyze the responses and decide which conversation is better."
    },
    {
        "question": "What is adversarial evaluation in the context of chatbot evaluation?",
        "answer": "Adversarial evaluation involves training a Turing-like classifier to distinguish between human responses and machine responses in order to evaluate the performance of a chatbot."
    },
    {
        "question": "What are some of the evaluation metrics mentioned in the text for assessing chatbots?",
        "answer": "Efficiency cost, quality cost, and user experience are some of the evaluation metrics mentioned in the text for assessing chatbots."
    },
    {
        "question": "What is a Wizard of Oz study in the context of building interfaces?",
        "answer": "A Wizard of Oz study involves having a real person pretend to be a chatbot and engage in conversations to gather insights on how the chatbot should interact with users."
    },
    {
        "question": "What is the name of the book written by Mary Shelley where a human is created out of bits of other humans?",
        "answer": "Frankenstein"
    },
    {
        "question": "Why is information leakage a problem with chatbots trained on a corpus?",
        "answer": "Information leakage is a problem because if the chatbot is trained on a corpus, it may know things about the people in the corpus, such as their Social Security number, which could then be leaked out to users."
    },
    {
        "question": "What happened to Microsoft's Twitter chatbot, Tay, after it was trained on Twitter data?",
        "answer": "Microsoft had to take Tay down after only 16 hours because it became offensive and abusive, giving out conspiracy theories and Nazi propaganda, reflecting racism and misogyny."
    },
    {
        "question": "What issue is pointed out regarding trained data sets for dialogue systems?",
        "answer": "The issue pointed out is that trained data sets are likely to have biases in them, including bias and hate speech."
    },
    {
        "question": "What are some of the ethical issues mentioned in the text regarding chatbots?",
        "answer": "Some of the ethical issues mentioned in the text regarding chatbots include privacy concerns in the corpus, intentional information leakage, and the importance of preserving privacy in training data."
    },
    {
        "question": "What are the two general sorts of architectures mentioned for chatbots in the text?",
        "answer": "The two general sorts of architectures mentioned for chatbots are rule-based systems and machine learning systems."
    },
    {
        "question": "What ethical issues should be considered when interacting with chatbots, according to the text?",
        "answer": "The ethical issues that should be considered when interacting with chatbots include ensuring they do not start spouting Nazi propaganda or giving out personal information that they shouldn't."
    },
    {
        "question": "What is the standard term for a numerical vector representation of word meaning, derived from a corpus?",
        "answer": "Word embedding"
    },
    {
        "question": "Who authored the groundbreaking paper on efficient estimation of word representations in vector space in 2013?",
        "answer": "Thomas McAuliffe and his group from Google Research"
    },
    {
        "question": "What is the difference between submitting a paper to a conference and submitting it to a journal?",
        "answer": "A conference paper may be four to eight pages long, while a journal paper contains longer papers with more details. Additionally, a journal paper is usually published separately from a conference."
    },
    {
        "question": "What is the origin of the word 'corpus' and what does it mean?",
        "answer": "The word 'corpus' is based on the Latin word 'corpus', which means a body."
    },
    {
        "question": "Why does the speaker mention reading AI research papers in the text?",
        "answer": "The speaker mentions reading AI research papers to learn about the current research, stay up to date with the latest research that has been peer-reviewed, and to get ideas for writing a research proposal."
    },
    {
        "question": "What are some key components of a typical research paper mentioned in the text?",
        "answer": "Some key components of a typical research paper mentioned in the text are the aim of the research, background information, methods, results, and possibly the title."
    },
    {
        "question": "What is the aim of the paper 'A Parsing Expert System which learns from Corpus Analysis'?",
        "answer": "The aim of the paper is to illustrate how difficult it is to do word embeddings."
    },
    {
        "question": "What was the aim of the paper presented at the ICAME conference in 1986?",
        "answer": "The aim of the paper was a discovery procedure for grammar, a way of automating a learning model of what words go together and why."
    },
    {
        "question": "What was the threshold set at for merging similar words in the corpus linguistics approach described in the text?",
        "answer": "0.8"
    },
    {
        "question": "What words were merged into a single word class based on the immediate lexical context in the text?",
        "answer": "The words 'will,' 'should,' 'could,' 'must,' 'may,' and 'might' were merged into a single word class."
    },
    {
        "question": "What was the particular task that Michelle Banko and Eric Brill chose to evaluate the performance of different learning methods?",
        "answer": "Confusion set disambiguation"
    },
    {
        "question": "What is the main conclusion drawn from evaluating a range of different classifiers on different data set sizes?",
        "answer": "The main conclusion is that the more data you have, the better the systems perform and the higher the accuracy of the classifiers."
    },
    {
        "question": "According to the text, what is the practical conclusion for text analytics researchers regarding the trade-off between algorithm development and corpus development?",
        "answer": "The practical conclusion is that researchers should reconsider the trade-off and focus more on spending time and money on corpus development by providing more training data, rather than constantly developing new machine learning algorithms."
    },
    {
        "question": "According to the text, what factor determines which classifier is the best?",
        "answer": "The amount of training data determines which classifier is the best."
    },
    {
        "question": "According to the text, what is the main factor that affects the accuracy of computational linguistics models?",
        "answer": "The main factor that affects the accuracy of computational linguistics models is the size of the training data set."
    },
    {
        "question": "What is one strategy mentioned in the text for determining the best classifier in machine learning?",
        "answer": "Using an ensemble of different algorithms and selecting the answer based on a majority vote."
    },
    {
        "question": "What is one of the strategies mentioned for dealing with cases where you don't have a large training set and know which classifier is best?",
        "answer": "One of the strategies mentioned is active learning."
    },
    {
        "question": "What does active learning involve in terms of choosing instances to label?",
        "answer": "Active learning involves choosing instances that the classifiers are not sure about for manual labeling."
    },
    {
        "question": "What is one of the conclusions mentioned in the text regarding researchers' efforts in natural language processing?",
        "answer": "One conclusion is that researchers should focus on increasing the size of annotated training collections and de-emphasise comparing different learning techniques trained on small training corpora."
    },
    {
        "question": "What are the two methods mentioned in the text for training neural network language models?",
        "answer": "Continuous Bag of Words (CBOW) and continuous SKIP-GRAM (or just SKIP-GRAM)"
    },
    {
        "question": "What are the two models mentioned in the text for predicting word meanings based on contexts?",
        "answer": "The two models mentioned are the CBOW model and the SKIP-GRAM model."
    },
    {
        "question": "What did Mikolov and his team aim to do with the word vector models they developed?",
        "answer": "Mikolov and his team aimed to define a comprehensive test set containing five types of semantic relations and nine types of syntactic relations to evaluate the quality of word vectors."
    },
    {
        "question": "What type of relationship is described between Athens and Greece in the text?",
        "answer": "Capital city relationship"
    },
    {
        "question": "What was the accuracy score achieved by Mikolov's recursive neural network system on semantics and syntactic relationships?",
        "answer": "8.6% accuracy on semantics and 36.5% accuracy on syntactic relationships"
    },
    {
        "question": "What method was used to determine the relationship between countries and their capitals in the text?",
        "answer": "The method used was to take the vector for the capital city, subtract the vector for the country, add on the vector for a different country, and then find the vector most similar to the result."
    },
    {
        "question": "According to the text, what is IBM to Linux as Microsoft is to?",
        "answer": "Windows"
    },
    {
        "question": "Why do the researchers conclude that SKIP-GRAM is better?",
        "answer": "The researchers conclude that SKIP-GRAM is better because it can cope with much larger data sets and provides a comprehensive test set with lots of examples."
    },
    {
        "question": "What can the resulting word vector file be used as in natural language processing and machine learning applications?",
        "answer": "Features"
    },
    {
        "question": "According to the text, what impact can be seen when moving from small data to large real world data sets?",
        "answer": "The impact on scaling can be observed, with more data leading to higher accuracy for all classifiers."
    },
    {
        "question": "What is the phrase that indicates the end of the text?",
        "answer": "END"
    },
    {
        "question": "Who is the researcher at the Natural Language Processing Group at University of Sheffield mentioned in the text?",
        "answer": "Diana Maynard"
    },
    {
        "question": "What is the difference between information retrieval and information extraction?",
        "answer": "Information retrieval involves pulling documents that match keywords from a text collection, while information extraction goes further by pulling facts and structured information from a document or set of documents."
    },
    {
        "question": "What is the purpose of information extraction as described in the text?",
        "answer": "The purpose of information extraction is to pull out named entities and relations between those named entities from a large text collection, making it easier to analyze the facts without having to read all the documents."
    },
    {
        "question": "What is the main difference between information retrieval and information extraction?",
        "answer": "Information extraction returns knowledge at a deeper level by pulling out entities and relations and structuring the information in a database, while information retrieval simply returns documents containing the information."
    },
    {
        "question": "What is the purpose of the Health and Safety Information Extraction System (HSE) developed at Sheffield?",
        "answer": "The purpose of the HSE is to look at company reports about health and safety."
    },
    {
        "question": "What does HSE do to improve health and safety in the workplace?",
        "answer": "HSE identifies sentences about health and safety issues or entities, extracts them, and populates a database with the entities and the relationships between them."
    },
    {
        "question": "What method was used to extract information about kibbutzes and related entities from newspaper stories?",
        "answer": "They extracted the information and put it into a database, which can be queried using SQL or a structured query language."
    },
    {
        "question": "What is the purpose of the query language mentioned in the text?",
        "answer": "The query language serves as the interface to the information extraction database."
    },
    {
        "question": "What nickname was given to Huda Salih Mahdi Ammash in the text?",
        "answer": "Mrs. Anthrax"
    },
    {
        "question": "Why is named entity recognition important as a foundation for building more complex information extraction?",
        "answer": "Named entity recognition is important as a foundation for building more complex information extraction because it allows for the identification of entities in text, which is necessary for finding relationships between them and providing tracking in the text sense, ontologies, and scenarios."
    },
    {
        "question": "What is one of the requirements for machine learning models, particularly deep learning models?",
        "answer": "Machine learning models, particularly deep learning models, require a lot of computing processor and memory."
    },
    {
        "question": "What approach was popular before powerful machine learning became feasible?",
        "answer": "The knowledge engineering approach"
    },
    {
        "question": "What is the main challenge in recognizing named entities according to the text?",
        "answer": "The main challenge in recognizing named entities is ambiguity, where a particular named entity can have different ways of referring to it and one name could refer to several different entities."
    },
    {
        "question": "What examples of ambiguity are mentioned in the text?",
        "answer": "The examples of ambiguity mentioned in the text include the name 'John Smith' which could refer to a person or a company, the name 'June' which could refer to a person or the month, the name 'Washington' which could refer to a person or a town, and the number '1945' which could refer to a year or a time."
    },
    {
        "question": "What are some factors that can provide additional information about words within a document, according to the text?",
        "answer": "The document structure, style, domain, genre, punctuation, spelling, spacing, and formatting can provide additional information about words within a document."
    },
    {
        "question": "What is the simplest baseline approach for named entity recognition mentioned in the text?",
        "answer": "The simplest baseline approach is to have lists of entities stored in gazetteers. The system recognizes entities by looking them up in the gazetteer lists."
    },
    {
        "question": "What are some strategies mentioned in the text for improving pharmaceutical named entity recognition?",
        "answer": "Some strategies mentioned in the text include keeping the list of pharmaceutical entities up-to-date, considering context clues such as specific words like City or Street, and looking for internal evidence within the unnamed entity."
    },
    {
        "question": "What is the example given to illustrate the concept of ambiguity in the text?",
        "answer": "The example given is about the difference between 'Quality Street' as a location and 'Quality Street' as a type of chocolate sweets."
    },
    {
        "question": "What is an example of structural ambiguity mentioned in the text?",
        "answer": "\"Cable and Wireless\" is quite a big company, whereas \"Microsoft and Dell\" is two companies, not one."
    },
    {
        "question": "What method is suggested for generating semantic patterns according to the text?",
        "answer": "Using a quick concordance to look for repeated contextual patterns."
    },
    {
        "question": "What is the purpose of semantic patterns in knowledge engineering or rule-based systems?",
        "answer": "The purpose of semantic patterns is to find entity types and relation types between entities."
    },
    {
        "question": "What approach does the MUSE system use for named entity recognition and code reference?",
        "answer": "The MUSE system uses a knowledge engineering approach with hand-crafted rules."
    },
    {
        "question": "What is the benefit of being able to adapt semantic grammars to other languages?",
        "answer": "The benefit is that you could adapt the grammars fairly straightforwardly to other languages, allowing for the development of the system for various languages."
    },
    {
        "question": "What are some of the challenges mentioned in developing resources for the Cebuano language?",
        "answer": "Some of the challenges mentioned include the lack of resources available for the language, as most resources are focused on Spanish and Portuguese, which are the official languages in South America."
    },
    {
        "question": "What are some of the requirements for adapting MUSE to other languages?",
        "answer": "Some of the requirements for adapting MUSE to other languages include support for other scripts and character sets, dictionaries for mapping, and a corpus for evaluation."
    },
    {
        "question": "What resources can be used to find names of people, places, and companies for text extraction?",
        "answer": "Phone books, Yellow Pages, and other resources on the web can be mined to find names of people, places, and companies for text extraction."
    },
    {
        "question": "What are some areas where information extraction is important according to the text?",
        "answer": "Some areas where information extraction is important according to the text are bioinformatics, medicine, finance industry, and research related to COVID."
    },
    {
        "question": "What is the difference between information extraction and information retrieval?",
        "answer": "Information extraction involves extracting named entities and relations from text, while information retrieval involves finding the documents."
    },
    {
        "question": "What is the name of the system developed at Sheffield University by the Natural Language Processing Research Group for named entity recognition?",
        "answer": "The MUSE system, which stands for MUlti-Source Entity Recognition System"
    },
    {
        "question": "What are the two different models Professor Eric Atwell mentions for matching keywords with documents in information retrieval?",
        "answer": "The two different models mentioned are a Boolean set theoretic model and a weighted vector model."
    },
    {
        "question": "What is one approach to searching for a particular keyword or string of characters in a database?",
        "answer": "One approach is to look for the string with wild cards before and after the keyword or characters."
    },
    {
        "question": "What information does the University of Leeds module catalogue contain?",
        "answer": "The University of Leeds module catalogue contains information about what modules are taught, what semester they're in, and other details."
    },
    {
        "question": "Why does the SQL query mentioned in the text not work?",
        "answer": "The SQL query doesn't work because it requires the T-value to be both 'database' and 'AI' simultaneously, which is not possible as a T-value can only have one value."
    },
    {
        "question": "What is the basic idea of an inverted file in information retrieval systems?",
        "answer": "The basic idea of an inverted file is to have a list of documents that each term contains, rather than having a database structure with records and fields."
    },
    {
        "question": "What is the purpose of pre-processing mentioned in the text?",
        "answer": "The purpose of pre-processing is to extract a dictionary of all the words that are in all the documents and determine which documents each word came from."
    },
    {
        "question": "How many times does the word 'and' appear in documents 1, 2, and 3 combined?",
        "answer": "The word 'and' appears three times in documents 1, 2, and 3 combined."
    },
    {
        "question": "What information may be contained in the inverted files mentioned in the text?",
        "answer": "The inverted files may contain a dictionary, a list of pointers into a data file identifying objects, positional information, and term frequency."
    },
    {
        "question": "What is the process described in the text for finding a match between a disjunctive normal form and a list of documents?",
        "answer": "The process involves converting the disjunctive normal form into its components, matching each component against the list of documents, and eliminating documents that do not match until a match is found."
    },
    {
        "question": "How many documents were found to have A or B and C in the given text?",
        "answer": "Four documents were found to have A or B and C."
    },
    {
        "question": "What is the importance of weighted weights in the context of issuing a query to Google?",
        "answer": "Weighted weights are important because they assign specific weights to words in a query, indicating the importance or relevance of each word in the search results."
    },
    {
        "question": "What is the purpose of assigning weights to words within documents in text analytics?",
        "answer": "The purpose of assigning weights to words within documents in text analytics is to indicate the importance of a word in a document relative to other words, with higher weights suggesting greater importance."
    },
    {
        "question": "What function is used to calculate the similarity between the query and documents in the given text?",
        "answer": "Cosine function"
    },
    {
        "question": "What information is stored in the inverted file mentioned in the text?",
        "answer": "For each word, the inverted file stores a list of document URLs and the weights of the words within each of those documents."
    },
    {
        "question": "What is one of the challenges Google faces in maintaining its index of web pages?",
        "answer": "One of the challenges Google faces is that it can be expensive to update the index if the information objects change, requiring the entire index to be recomputed."
    },
    {
        "question": "Why is an inverted file index considered more efficient than relational databases for information retrieval?",
        "answer": "An inverted file index is considered more efficient than relational databases for information retrieval because it can store frequencies or weights in the dictionary to allow ordering of results, unlike standard SQL which is not very good at expressing queries for finding records with multiple keywords."
    },
    {
        "question": "What is the difference between information retrieval and database management systems in terms of the expected output?",
        "answer": "For database management systems, you expect to issue an SQL query and get exactly what you asked for. In information retrieval, you type a query into Google and get a partial or best match, along with a rank ordering of matches."
    },
    {
        "question": "What is the difference between the predictability of queries in database management systems and information retrieval systems?",
        "answer": "In database management systems, issuing a query should predict exactly what you get back every time, while in information retrieval systems, the responses may not be the same due to probabilistic weights and random probabilities."
    },
    {
        "question": "What is the difference between a query in SQL and a query in information retrieval, according to the text?",
        "answer": "In SQL, the query specification has to be exact and complete, while in information retrieval, you can have a guess at some keywords and may find what you want even with an incomplete query."
    },
    {
        "question": "What is the difference between the error sensitivity of an SQL query and a query in information retrieval?",
        "answer": "An SQL query is sensitive to errors, meaning that if it's not quite right, the response will be completely wrong. On the other hand, in information retrieval, if a query is not quite right, you will still get more or less a match, making it relatively insensitive to errors."
    },
    {
        "question": "What are some of the interesting words mentioned in the text?",
        "answer": "Some of the interesting words mentioned in the text are 'pudding', 'jam', 'traffic', 'lane', and 'treacle'."
    },
    {
        "question": "What are the three documents described in the text and what do they contain?",
        "answer": "The three documents are: 1) a recipe for jam pudding, containing pudding and jam but not traffic, lane, and treacle; 2) a Department of Transport report on traffic lanes, containing traffic and lane but not the other words; 3) a radio item on a traffic jam in Pudding Lane, containing pudding, jam, traffic, and lane but not treacle."
    },
    {
        "question": "What does the simple representation mentioned in the text capture?",
        "answer": "The simple representation only captures unigrams and does not capture any bigrams."
    },
    {
        "question": "According to the text, what is the score given to a document if it matches one of the query vectors 11000, 10001, or 11001?",
        "answer": "The score given to a document is 1 if it matches one of the query vectors 11000, 10001, or 11001."
    },
    {
        "question": "What weights are assigned to the terms 'pudding', 'jam', 'traffic', 'lanes', and 'treacle pudding' in the query mentioned in the text?",
        "answer": "The weights assigned to the terms are: pudding - 1, jam - 0.6, traffic - 0, lanes - 0, treacle pudding - 0.8"
    },
    {
        "question": "What does the cosine coefficient measure in the context of data mining and text analytics?",
        "answer": "The cosine coefficient measures the similarity between a document and a query based on the weights of the words in both the document and the query."
    },
    {
        "question": "What ingredients are mentioned in the jam pudding recipe?",
        "answer": "Pudding, jam, and treacle"
    },
    {
        "question": "What is the score given to the jam pudding recipe based on the calculations mentioned in the text?",
        "answer": "0.89"
    },
    {
        "question": "What was the score calculated for the radio report on traffic jams in Pudding Lane?",
        "answer": "0.51"
    },
    {
        "question": "What are some drawbacks of the Boolean model mentioned in the text?",
        "answer": "Some drawbacks of the Boolean model mentioned in the text are that it does not rank the results, it only provides exact matches, it does not work well for very large web resources, and users find Boolean queries hard to formulate."
    },
    {
        "question": "Why is the vector model popular with search engines?",
        "answer": "The vector model is popular with search engines because it allows for partial matching, which gives scores to documents even if they are not perfect, allowing for ranking of search results."
    },
    {
        "question": "What is the difference between Google and Yahoo and Bing in terms of weighting formulae for words within documents?",
        "answer": "Google, Yahoo, and Bing have slightly different weighting formulae for words within documents, and they try them out on different populations to see which works best."
    },
    {
        "question": "What issue is discussed regarding search terms in the text?",
        "answer": "The issue discussed is that the model currently has separate index terms for 'football' and 'soccer', and does not allow for searching for one term and finding results for both."
    },
    {
        "question": "Why is it important to consider the context in which a word appears when searching for information?",
        "answer": "It is important to consider the context in which a word appears when searching for information because the same word can have different meanings depending on the context, and the context can provide valuable clues about the intended meaning of the word."
    },
    {
        "question": "What are some of the metrics mentioned in the text for measuring search engine results?",
        "answer": "Accuracy, recall, precision, and user satisfaction"
    },
    {
        "question": "What are the two general approaches mentioned for query broadening?",
        "answer": "The two general approaches mentioned for query broadening are asking the user for feedback and offering the user a thesaurus or a term bank to broaden the query themselves."
    },
    {
        "question": "How does adding a constant beta of the sum of all the good hits and relevant hits to the original query vector help in information retrieval?",
        "answer": "Adding a constant beta of the sum of all the good hits and relevant hits to the original query vector helps in moving the query vector closer to the centroid of the relevant, retrieved document vectors and further away from the centroid of non-relevant, retrieved documents. This creates a new query that includes more of the good hits and less of the bad hits, improving the relevance of the search results."
    },
    {
        "question": "What is the purpose of adjusting the weight of the term 'jelly' in the text analytics process described in the passage?",
        "answer": "The purpose of adjusting the weight of the term 'jelly' is to ensure that hits can use both 'jelly' and 'jam' interchangeably in the search process."
    },
    {
        "question": "What is the basic feedback mechanism described in the text?",
        "answer": "The basic feedback mechanism is where the user is invited to tick the ones they like, and then extra weights are added to those selected documents."
    },
    {
        "question": "What type of document is the fourth document mentioned in the text?",
        "answer": "The fourth document is a radio item on traffic jam in Pudding Lane."
    },
    {
        "question": "What adjustments were made to the query based on the relevance of the documents mentioned in the text?",
        "answer": "The adjustments made to the query included setting alpha to 0.5, beta to 0.5, and gamma to 0.2, reducing the weight of non-relevant documents, and adding the good document with relevant information."
    },
    {
        "question": "What are the new scores assigned to the documents after feeding the new query into the cosine coefficient?",
        "answer": "The new scores are 0.96 for one document, 0 for the traffic lane document, and 0.86 for the treacle pudding recipe document."
    },
    {
        "question": "What is one method mentioned in the text for query broadening?",
        "answer": "Using a thesaurus or ontology"
    },
    {
        "question": "How can replacing words with documents and query words with synonyms from a controlled language improve precision and recall?",
        "answer": "Replacing words with documents and query words with synonyms from a controlled language can improve precision and recall by standardizing terms, increasing the chances of matching index terms in the content, and providing narrower terms for better scores."
    },
    {
        "question": "According to the text, why is a traditional database unsuited to retrieval of unstructured information?",
        "answer": "A traditional database is unsuited to retrieval of unstructured information because SQL doesn't allow for Boolean queries like A or B or C, and one has to convert queries into disjunctive normal form."
    },
    {
        "question": "What is the main difference between the Boolean or set theoretical model and the weighted vector model in information retrieval?",
        "answer": "The main difference is that the weighted vector model allows you to rank order the results, while the Boolean or set theoretical model does not."
    },
    {
        "question": "How does the speaker end their speech?",
        "answer": "The speaker ends their speech by saying 'thank you very much for listening and goodbye.'"
    },
    {
        "question": "What is a Multi Word Expression (MWE) according to Tom Pickard?",
        "answer": "A Multi Word Expression (MWE) is a group of more than one word which functions linguistically as a single unit of meaning."
    },
    {
        "question": "According to the text, what percentage of entries in WordNet consist of more than one word?",
        "answer": "Up to about half of the entries in WordNet consist of more than one word."
    },
    {
        "question": "What is one property of Multi Word Expressions mentioned in the text?",
        "answer": "One property of Multi Word Expressions mentioned in the text is non-compositionality."
    },
    {
        "question": "Why do Multi Word Expressions cause problems for language learners and natural language processing?",
        "answer": "Multi Word Expressions cause problems because they cannot be understood by simply breaking up the text into individual words, as they have a meaning that goes beyond the sum of their parts."
    },
    {
        "question": "Why is handling Multi Word Expressions considered difficult?",
        "answer": "Handling Multi Word Expressions is considered difficult because there may not be an equivalent expression in the target language as there is in the source language, and new expressions are frequently coined, requiring ongoing identification and discovery."
    },
    {
        "question": "What is one method mentioned in the text for identifying collocations?",
        "answer": "Looking for words that occur together more often than expected by chance"
    },
    {
        "question": "What method is described in the text for analyzing the co-occurrence of words like 'fake' and 'news'?",
        "answer": "The text describes using the Pointwise Mutual Information measure to analyze how often pairs of words occur together compared to how often they occur individually."
    },
    {
        "question": "What approach did the author suggest for identifying Multi Word Expressions in the text?",
        "answer": "The author suggested using semantic embeddings or word vector embeddings, such as word2vec or GloVe, to capture the meaning of words and their relationships in a multi-dimensional space."
    },
    {
        "question": "What method does the author suggest using to determine if a phrase has a different meaning than its individual components?",
        "answer": "The author suggests using vector embeddings to compare the vectors representing the individual words with the vector representing the phrase as a whole."
    },
    {
        "question": "What methodology was the research based on?",
        "answer": "The research was based on a methodology by Will Roberts and Markus Egg."
    },
    {
        "question": "What was the main purpose of comparing word2vec and GloVe in the text?",
        "answer": "The main purpose was to see which approach was more useful or effective for the task."
    },
    {
        "question": "What kind of sample did the speaker take from English Wikipedia to work from?",
        "answer": "The speaker took a 10% sample of the sentences in English Wikipedia to work from."
    },
    {
        "question": "What are some differences in how words are structured in different languages mentioned in the text?",
        "answer": "Some languages like English separate words with spaces, while languages like German combine nouns into compound nouns. Additionally, languages like Chinese and Arabic have different word boundaries."
    },
    {
        "question": "What is the author particularly interested in studying regarding Multi Word Expressions?",
        "answer": "The author is particularly interested in studying the way in which people use Multi Word Expressions in particular domains, such as academia, data science modules, and medicine."
    },
    {
        "question": "What kind of neural network is BERT and how does it learn word embeddings?",
        "answer": "BERT is a different kind of neural network for learning word embeddings. It processes a pair of words as a single unit of meaning, which allows it to handle Multi Word Expressions to some extent."
    },
    {
        "question": "What is the name of the episode of Star Trek-- The Next Generation mentioned in the text?",
        "answer": "Darmok"
    },
    {
        "question": "What does the author mention putting in at the end of the text?",
        "answer": "A little plug for themselves"
    },
    {
        "question": "What is Sketch Engine used for?",
        "answer": "Sketch Engine is used for text analytics and corpus linguistics research."
    },
    {
        "question": "What are some of the features and tools available in the online tool mentioned in the text?",
        "answer": "Some of the features and tools available in the online tool include word sketch, collocations summary, concordance, distributional thesaurus, parallel corpus, and WebBootCaT for creating a specialized corpus from the web."
    },
    {
        "question": "What is the purpose of terminology extraction mentioned in the text?",
        "answer": "The purpose of terminology extraction is to extract words and multi-word terms that are characteristic of a corpus and are likely specialist terms."
    },
    {
        "question": "What is the purpose of a concordance in the context of building a dictionary?",
        "answer": "To gather lots of examples of a particular word or phrase from a corpus in order to figure out its meaning."
    },
    {
        "question": "What is the purpose of WebBootCaT mentioned in the text?",
        "answer": "WebBootCaT allows you to create a corpus from a web page."
    },
    {
        "question": "What is the purpose of comparing the words in your corpus against a standard English corpus?",
        "answer": "To find words that are more frequent in your corpus than in the standard corpus."
    },
    {
        "question": "What is one advantage of having a collected corpus over several years, as mentioned in the text?",
        "answer": "One advantage is being able to see trends or changes in usage over time."
    },
    {
        "question": "What does the speaker mention as a requirement for accessing some of the videos on the Sketch Engine website?",
        "answer": "The speaker mentions that you have to be logged in to YouTube in order to access some of the videos on the Sketch Engine website."
    },
    {
        "question": "What is the purpose of the 30-day free trial license mentioned in the text?",
        "answer": "The purpose of the 30-day free trial license is to allow users to try out Sketch Engine without having to pay for it."
    },
    {
        "question": "What type of corpus is being created in the text?",
        "answer": "A corpus on data mining"
    },
    {
        "question": "What search settings does the speaker mention when setting up the search?",
        "answer": "The speaker mentions the need to type at least three words or phrases, such as 'web search,' 'data mining,' and 'data and mining.'"
    },
    {
        "question": "What does the speaker mention about the settings and experimenting with them?",
        "answer": "The speaker mentions that there are other settings that can be experimented with, but they are not going to bother with those."
    },
    {
        "question": "How many tokens and words were collected in the corpus mentioned in the text?",
        "answer": "29,000 tokens and 24,000 words were collected in the corpus."
    },
    {
        "question": "What is the default reference corpus used for key words extraction in the text?",
        "answer": "The default reference corpus used for key words extraction is the English Web 2020, or enTenTen, which contains 10 billion words collected from the web."
    },
    {
        "question": "Why is it important to choose a large corpus as a reference corpus in data mining?",
        "answer": "Choosing a large corpus as a reference corpus is important in data mining because it provides statistically significant meanings and helps in identifying multi-word terms that are good indicators of a specialist domain."
    },
    {
        "question": "What is the real reason for looking at the web page mentioned in the text?",
        "answer": "The real reason for looking at this web page is it has some information on open source development of large corpora."
    },
    {
        "question": "What is the purpose of WebBootCaT in the text?",
        "answer": "WebBootCaT is used to collect a billion word corpus by sending query combinations to search engines, extracting text, and getting rid of unwanted content."
    },
    {
        "question": "What is the purpose of using the Association of Computational Linguistics Special Interest Group in finding information about a topic area?",
        "answer": "To find lots of conference proceedings in that topic area."
    },
    {
        "question": "What source does the text suggest for collecting a billion word corpus?",
        "answer": "Leeds internet corpora"
    },
    {
        "question": "What is BERT and what is its purpose according to the text?",
        "answer": "BERT stands for Bidirectional Encoder Representations from Transformers and it is a method and tool kit from Google Research Labs for understanding meaning relationships between sentences or measuring the similarity in meaning between two sentences."
    },
    {
        "question": "What is one of the clever things about BERT mentioned in the text?",
        "answer": "One of the clever things about BERT is that it does a lot of unsupervised learning of morphology, syntax, and semantics."
    },
    {
        "question": "What is the basic assumption that allows researchers to pluck pairs of sentences with related meanings?",
        "answer": "The basic assumption is that in running text, two sentences next to each other should be related in meaning."
    },
    {
        "question": "What is the main idea behind BERT according to the text?",
        "answer": "The main idea behind BERT is learning pre-trained representations from large amounts of unlabelled text, which can then be fine-tuned for specific tasks."
    },
    {
        "question": "Where do they get the huge corpus for pre-training?",
        "answer": "They use 800 million words from the Google Books corpus, which includes books scanned in from Oxford library, Oxford University library, and many other University libraries, as well as English Wikipedia."
    },
    {
        "question": "What is the focus of unsupervised machine learning discussed in the text?",
        "answer": "The focus is on morphology, syntax, grammar, and semantics."
    },
    {
        "question": "What is the Morpho Challenge and what was its purpose?",
        "answer": "The Morpho Challenge was unsupervised learning of morphological analysis, segmenting words into morphemes. It was developed for highly complex languages like Finnish or Turkish, where words could be quite long, consisting of several morphemes."
    },
    {
        "question": "What is the first stage in WordPiece, unsupervised machine learning of text segmentation?",
        "answer": "The first stage is to segment each of the sentences into bits, but not necessarily into words."
    },
    {
        "question": "What is the purpose of WordPiece in machine translation?",
        "answer": "The purpose of WordPiece in machine translation is to handle languages with limited resources by breaking down words into common pieces to improve analysis."
    },
    {
        "question": "What method is used to handle rare words in the text?",
        "answer": "The rare words are chopped into smaller parts and then various possible permutations and combinations are tried until the best entropy combination with reasonably low-frequency pieces is found."
    },
    {
        "question": "What is the default assumption about the number of WordPieces per sentence in the code mentioned in the text?",
        "answer": "The default assumption is about 25 WordPieces per sentence."
    },
    {
        "question": "What is the main task that BERT tries to accomplish?",
        "answer": "BERT tries to predict the missing word in a sentence by optimizing the vector representing the meaning."
    },
    {
        "question": "What is the bigram model for sentence meaning mentioned in the text?",
        "answer": "The bigram model for sentence meaning is the idea that each sentence is related in meaning to the next sentence."
    },
    {
        "question": "What is the relationship between sentences A and B in the training sentences mentioned in the text?",
        "answer": "50% of the time, B is the actual next sentence that follows from A, while the other 50% of the time, it is a random sentence from anywhere else in the corpus."
    },
    {
        "question": "What is the purpose of the GLUE competition mentioned in the text?",
        "answer": "The purpose of the GLUE competition is to put together existing datasets into a general language understanding evaluation set of tasks for researchers to evaluate their classifiers."
    },
    {
        "question": "What is the task in the QNLI dataset?",
        "answer": "Given a question and a sentence, does the sentence contain the correct answer?"
    },
    {
        "question": "What is the purpose of the MNLI and RTE test sets within GLUE?",
        "answer": "To determine if the first sentence entails, contradicts, or is neutral towards the second sentence."
    },
    {
        "question": "What did the researchers find when comparing BERT Base and BERT Large to existing systems on various tasks?",
        "answer": "Both BERT Base and BERT Large outperformed all of the existing systems on all of the tasks."
    },
    {
        "question": "What are the differences between the Base and Large neural networks mentioned in the text?",
        "answer": "The Base neural network has 12 layers, while the Large neural network has 24 layers."
    },
    {
        "question": "What is the purpose of studying people who've had accidents or brain damage in psychology?",
        "answer": "To gain insights on normal brain behavior by observing how individuals behave when parts of their brain do not work anymore."
    },
    {
        "question": "What is one suggestion the author gives for readers who want to learn more about the current state-of-the-art in natural language processing research?",
        "answer": "The author suggests that readers should read all the papers in the references to get an overview of the current state-of-the-art in natural language processing research."
    },
    {
        "question": "What aspect of the BERTology paper is not discussed in detail according to the text?",
        "answer": "Specific examples of sentences or pairs of sentences which work and pairs of sentences which don't work, along with explanations or analysis of why they didn't work."
    },
    {
        "question": "What is one limitation of BERT mentioned in the text?",
        "answer": "The text mentions that BERT is limited in that it is only about English and does not mention how it transfers to other languages."
    },
    {
        "question": "What tokenisation method does the text mention and how does it handle rare and out of vocabulary words?",
        "answer": "The text mentions the WordPiece tokenisation method, which captures the meanings of common words and breaks up rare and out of vocabulary words into pieces, hoping that the pieces are common."
    },
    {
        "question": "According to the text, what is the assumption about the relationship between sentences in the BERT model?",
        "answer": "The assumption is that there is an inherent, implicit label between each sentence, where sentence one is related to sentence two, sentence two is related to sentence three, and sentence one is not related to any other sentence at random."
    },
    {
        "question": "What is the main reason why everyone is using BERT according to the text?",
        "answer": "The results are quite astounding, and both BERT Base and BERT Large outperform all the other systems they tried on all of the tasks by a substantial margin."
    },
    {
        "question": "What is the author's advice regarding understanding the neural network model of BERT?",
        "answer": "The author advises that if you use BERT, you don't necessarily have to understand the neural network model unless you want to become a machine learning researcher developing variations of neural networks."
    },
    {
        "question": "What is the course code for the Data mining and text analytics course at the University of Leeds?",
        "answer": "OCOM5204M"
    },
    {
        "question": "What is the purpose of probabilistic language models according to the text?",
        "answer": "The purpose of probabilistic language models is to model a sequence of words or other things by estimating the probabilities of n-grams and evaluating the language model's perplexity against a corpus."
    },
    {
        "question": "What are some applications where assigning probabilities to sentences is useful?",
        "answer": "Some applications include machine translation and spelling correction."
    },
    {
        "question": "What is the importance of computing the probability of a sequence of words in speech recognition and other language processing tasks?",
        "answer": "Computing the probability of a sequence of words helps in determining the most likely word sequence, which is crucial for tasks like speech recognition, summarization, question answering, and understanding the overall meaning of a sentence."
    },
    {
        "question": "What is a language model in computational linguistics?",
        "answer": "A language model is a model that computes either the probability of a complete sequence of words or the probability of the next word given the words up to that point."
    },
    {
        "question": "According to the text, how is the probability of a word sequence calculated?",
        "answer": "The probability of a word sequence is calculated by multiplying the probabilities of each word in the sequence, given the previous words in the sequence."
    },
    {
        "question": "How can you compute the probability of the words in a sentence according to the text?",
        "answer": "You can compute the probability of the words in a sentence by multiplying the probabilities of each word given the preceding words in the sentence."
    },
    {
        "question": "Who came up with a simplifying assumption about probabilities over 100 years ago?",
        "answer": "Andrei Markov"
    },
    {
        "question": "According to the text, what does the term 'unigram' refer to?",
        "answer": "The term 'unigram' refers to looking at the individual word and considering the probabilities of each individual word."
    },
    {
        "question": "What is the purpose of looking at bigrams, trigrams, 4-grams, and 5-grams in the context of language modeling?",
        "answer": "The purpose is to analyze word sequences and improve the generation of more English-like sentences by considering the probability of word combinations and dependencies on previous words."
    },
    {
        "question": "What method is used to estimate the probabilities of n-grams in the text?",
        "answer": "Counting up the pairs of words and the individual words in a large corpus."
    },
    {
        "question": "How many bigrams involve the start of a sentence followed by the word 'I'?",
        "answer": "There are three bigrams involving the start of a sentence followed by 'I'."
    },
    {
        "question": "What type of corpus is being discussed in the text?",
        "answer": "The text is discussing a corpus of typical sentences or conversations related to booking places at restaurants."
    },
    {
        "question": "How many times does the word 'I' occur in the text?",
        "answer": "The word 'I' occurs 5 times in the text."
    },
    {
        "question": "What is the probability of the sentence 'I want English food' based on the information provided?",
        "answer": "0.000031"
    },
    {
        "question": "Why do we convert probabilities into logarithms in computation?",
        "answer": "To avoid underflow and make combining probabilities faster."
    },
    {
        "question": "What is the Google n-gram and how was it created?",
        "answer": "The Google n-gram is a collection of 1,000 million five-word sequences that occurred at least 40 times. It was created by counting and processing one million million words, resulting in 13 million unique words."
    },
    {
        "question": "How can you determine if a language model is good according to the text?",
        "answer": "You can determine if a language model is good by checking if it assigns higher probability to real sentences rather than ungrammatical or bad sentences."
    },
    {
        "question": "What is the main focus of the research group at Leeds mentioned in the text?",
        "answer": "The research group at Leeds focuses on research on the Quran and Hadith, which are the religious texts of Islam."
    },
    {
        "question": "What is the difference between intrinsic evaluation and extrinsic evaluation in the context of language models?",
        "answer": "Intrinsic evaluation focuses on evaluating the model itself, while extrinsic evaluation involves testing the model on something else, like a machine translation system."
    },
    {
        "question": "What is perplexity and how is it used to evaluate a language model?",
        "answer": "Perplexity is a measure of how good a language model is, without testing it on specific tasks like machine translation or spelling checking. It gives an idea of the model's quality, which can be useful in pilot experiments."
    },
    {
        "question": "According to the text, why are unigrams not very good for predicting in language models?",
        "answer": "Unigrams are not very good for predicting because they do not consider any context, such as the word before or two words before, which is necessary for making accurate predictions."
    },
    {
        "question": "What is the purpose of calculating perplexity in the context of data mining?",
        "answer": "The purpose of calculating perplexity in data mining is to compare two models and decide whether or not to train on a specific corpus for a particular problem."
    },
    {
        "question": "What does the author suggest is the relationship between minimizing perplexity and maximizing probability?",
        "answer": "The author suggests that minimizing perplexity is the same as maximizing probability."
    },
    {
        "question": "What method can be used to predict the probability of a sentence like 'I want to eat Chinese food' according to the text?",
        "answer": "Taking the bigram probabilities and multiplying them together"
    },
    {
        "question": "What is the main issue highlighted in the text regarding the occurrence of bigrams and quadrigrams in Shakespeare's corpus?",
        "answer": "The main issue highlighted is that most of the possible bigrams and quadrigrams never occur, with nearly all quadrigrams having a zero probability."
    },
    {
        "question": "What is the difference between using quadrigrams from Shakespeare and the Wall Street Journal corpus to generate random text?",
        "answer": "The quadrigrams generated from Shakespeare will contain things that occur only in Shakespeare and don't occur anywhere else, while the quadrigrams from the Wall Street Journal corpus will be more plausible due to the larger corpus size."
    },
    {
        "question": "What is an example of overfitting mentioned in the text?",
        "answer": "The example of overfitting mentioned in the text is that 3-gram models are very differentiated for Wall Street Journal compared to Shakespeare, which means they may not work well for another corpus."
    },
    {
        "question": "Why are zeros a problem in language models, according to the text?",
        "answer": "Zeros are a problem in language models because they represent things that never occur in the training set but may occur in the test set, leading to incorrect predictions and assigning zero probability to reasonable sentences that were not present in the training data."
    },
    {
        "question": "What is one method mentioned in the text to address the issue of sparse statistics in language modeling?",
        "answer": "One method mentioned is add-one smoothing or Laplace smoothing."
    },
    {
        "question": "What is the concept of add-one estimation in the context of smoothing?",
        "answer": "Add-one estimation, also known as Laplace smoothing, involves pretending that each word was seen one more time than it actually was in order to adjust the probabilities and generalize better."
    },
    {
        "question": "How does the add-one estimate method work in a language model?",
        "answer": "In the add-one estimate method, one is added to every frequency count of word pairs, and the probability of any word given the previous word is calculated as the count of that pair of words divided by the count of the individual word."
    },
    {
        "question": "What is the probability of the phrase 'I, I' occurring in the smoothed bigram corpus mentioned in the text?",
        "answer": "0.0015"
    },
    {
        "question": "What is the probability of the word 'want' occurring in the text based on the given information?",
        "answer": "The probability of the word 'want' occurring is 0.39."
    },
    {
        "question": "What is the advantage of redistributing the frequencies to the zeros in the bigram counts?",
        "answer": "The advantage is that there are no longer any zeros in the chain probabilities, which prevents the whole sentence from having a value of zero."
    },
    {
        "question": "Why is add-one estimation not used in state-of-the-art n-gram language models?",
        "answer": "Add-one estimation is not used in state-of-the-art n-gram language models because there are better, more complicated methods available."
    },
    {
        "question": "What are the different models mentioned for language modeling in the text?",
        "answer": "The different models mentioned are the trigram model, the bigram model, and the unigram model."
    },
    {
        "question": "What is one way to avoid overfitting when training a language model according to the text?",
        "answer": "One way to avoid overfitting is by using interpolation, which combines trigram, bigram, and unigram probabilities with different weights."
    },
    {
        "question": "What is the purpose of using the held-out corpus in language modeling?",
        "answer": "The purpose of using the held-out corpus is to optimize the lambda values in order to maximize the probability of the data."
    },
    {
        "question": "What is the purpose of creating an Unknown word Token (UNK) when training a model on a fixed lexicon?",
        "answer": "The purpose of creating an Unknown word Token (UNK) is to label any out-of-vocabulary word that is not in the fixed lexicon, allowing the model to still train on these words by treating them as if they were normal words."
    },
    {
        "question": "What is the main issue with labeling words as UNK in the training and test corpora?",
        "answer": "The main issue with labeling words as UNK is that it ignores everything apart from the 1,000 most common words or the n most common words, which means that the meanings of lots of words are not captured."
    },
    {
        "question": "What strategy does the text suggest for dealing with n-grams that occur less than 20 times?",
        "answer": "Remove all the ones, singletons or things that only occur once or things that only occur less than 20 times."
    },
    {
        "question": "What are some examples of techniques mentioned in the text for compressing data and numbers?",
        "answer": "Some examples of techniques mentioned in the text for compressing data and numbers include using clever Huffman coding to store words as bytes, using fewer bits for representing numbers, and using adaptive models for language modeling."
    },
    {
        "question": "What is one method mentioned in the text to address the problem of zeros in language modeling?",
        "answer": "Smoothing, such as add-one or Laplace smoothing, is mentioned as a method to address the problem of zeros in language modeling."
    },
    {
        "question": "What are some strategies mentioned for dealing with unseen trigrams in language models?",
        "answer": "Backoff, trying the bigram or unigram, and interpolation were mentioned as strategies for dealing with unseen trigrams in language models."
    },
    {
        "question": "Who is the professor mentioned in the text who has developed an Arabic computing research laboratory called the CAMeL lab?",
        "answer": "Professor Nizar Habash"
    },
    {
        "question": "What type of language models is Google interested in developing?",
        "answer": "Google is interested in developing language models which work in general with text of various alphabets, not just the Roman alphabet."
    },
    {
        "question": "What are some challenges for machine translation mentioned in the text?",
        "answer": "Some challenges for machine translation mentioned in the text include different character sets in different languages, ambiguous spellings, word boundaries in languages like Chinese, and the ambiguity of certain words like 'bank' in English."
    },
    {
        "question": "What is the distinction between the two senses of 'eat' in the German language?",
        "answer": "In German, 'essen' is used for human eating, while 'fressen' is used for animal eating."
    },
    {
        "question": "How does the tokenization process in Arabic differ from English and French?",
        "answer": "In Arabic, what might be an affix in English becomes an infix or even two bits of things. Sometimes what is a single word in one language becomes several words in another language."
    },
    {
        "question": "According to the text, what happens to the verb when translating a sentence into different languages?",
        "answer": "The verb may change form or position in the sentence when translated into different languages."
    },
    {
        "question": "What resources are necessary for training machine learning models for machine translation?",
        "answer": "A corpus, a dictionary or list of words in the language, and a parallel corpus with aligned source and target languages."
    },
    {
        "question": "What are some of the challenges for machine translation mentioned in the text?",
        "answer": "One of the challenges for machine translation mentioned in the text is the lack of webpages or parallel corpora for certain language pairs, such as English Bengali or Bengali Amharic."
    },
    {
        "question": "What does the author suggest is the best approach for translating text from one language to another?",
        "answer": "The author suggests that the best approach for translating text from one language to another is to take the grammatical structures of the source language and transfer them into the grammatical structures of the target language."
    },
    {
        "question": "What is the idea of a transfer lexicon mentioned in the text?",
        "answer": "The idea of a transfer lexicon is when a structured phrase in Spanish translates into a single word or verb in English."
    },
    {
        "question": "What is the purpose of having dictionaries, lexicons, and parallel corpora when translating words and phrases from one language to another?",
        "answer": "The purpose is to work out the equivalent words in the target language, understand the rules for translation, and have resources for word alignments and training in statistical machine translation."
    },
    {
        "question": "According to the text, what rule states that some words need to be converted into longer phrases in the translation process?",
        "answer": "The fertility rule"
    },
    {
        "question": "What is the purpose of phrase based statistical machine translation mentioned in the text?",
        "answer": "The purpose is to look for phrases of several words rather than just individual words in order to improve translation accuracy, especially for languages with very different grammar structures."
    },
    {
        "question": "According to the text, what is the importance of identifying whole phrases in machine translation systems?",
        "answer": "Identifying whole phrases in machine translation systems helps in building the system much better by allowing for the correct swapping of phrases and improving the accuracy of translations."
    },
    {
        "question": "What is one way of doing phrase-based machine translation as described in the text?",
        "answer": "Learning the whole phrase in English and translating it into a whole phrase in Spanish without swapping individual words."
    },
    {
        "question": "According to the text, how does Google Translate improve its translation capabilities over time?",
        "answer": "Google Translate improves its translation capabilities over time by inviting users to suggest better translations, learning from these suggestions to work out longer phrases, and eventually translating whole phrases and sentences without using word-for-word mapping."
    },
    {
        "question": "What is one of the key considerations when evaluating a machine translation system?",
        "answer": "One of the key considerations when evaluating a machine translation system is whether it is actually correctly translating the content."
    },
    {
        "question": "How do human translators typically evaluate the output of machine translation systems?",
        "answer": "Human translators rank the translations in terms of fidelity or accuracy on a scale from 1 to 5, where 5 means it's almost perfect, 1 means it completely misses the original meaning, and 2, 3, or 4 represent varying degrees of accuracy in conveying the meaning."
    },
    {
        "question": "What are the two measures used to evaluate machine translation according to the text?",
        "answer": "The two measures used to evaluate machine translation are fidelity (or accuracy) and fluency (or intelligibility)."
    },
    {
        "question": "What does the term 'BLEU' stand for and how does it help in the context of translation?",
        "answer": "The term 'BLEU' stands for 'bilingual evaluation understudy'. It helps in translation by calculating a score based on multiple translations provided by human translators, which correlates with human evaluation to determine the quality of machine translation systems."
    },
    {
        "question": "What is the purpose of comparing the machine translation against several human translations in the context of the text?",
        "answer": "To give a standardized score and determine which translation is better."
    },
    {
        "question": "What metric is used to evaluate machine translation by looking at unigrams and bigrams to see how well they are translated?",
        "answer": "BLEU metric"
    },
    {
        "question": "What is the unigram precision score for the test sentence from the machine translation output?",
        "answer": "4 out of 5"
    },
    {
        "question": "What is one reason mentioned in the text for why machine translation is difficult?",
        "answer": "Languages have different writing systems, words are divided up differently in morphology, and the ordering of words is different in different languages."
    },
    {
        "question": "What sources provide a huge parallel corpus for United Nations legal texts in multiple languages?",
        "answer": "Sources like the United Nations provide a huge parallel corpus for United Nations legal texts in multiple languages."
    },
    {
        "question": "What are the two main factors that need to be considered when evaluating a machine translation system?",
        "answer": "The two main factors are accuracy and fluency."
    },
    {
        "question": "What method was the first to come up with an automated or semi-automated method for evaluating machine translation systems?",
        "answer": "BLEU"
    },
    {
        "question": "What is the purpose of joining 'social media' communities for data mining and text analytics professionals?",
        "answer": "To network, meet others, share knowledge and resources, and get ideas for research projects."
    },
    {
        "question": "What are some examples of communities mentioned in the text that were set up by academics or practitioners in the AI and natural language processing fields?",
        "answer": "Some examples of communities mentioned in the text are KDnuggets, Kaggle, WEKA, ICAME, ACL, SemEval, and the EI-JRC communities."
    },
    {
        "question": "What is Facebook Research and what opportunities does it offer to individuals?",
        "answer": "Facebook Research is a huge research lab that offers opportunities for individuals to submit research proposals and potentially receive funding to implement them."
    },
    {
        "question": "What are some features of LinkedIn mentioned in the text?",
        "answer": "Some features of LinkedIn mentioned in the text include posting ideas, news, or whatever, following people and collecting followers, receiving notifications, job adverts, product adverts, networking, messaging, and learning opportunities."
    },
    {
        "question": "What is the purpose of Quora according to the text?",
        "answer": "Quora is a platform to gain and share knowledge, ask questions, connect with people who provide unique insights and quality answers, and get more specific and direct answers compared to a Google search."
    },
    {
        "question": "What is a 'knowledge nugget' as mentioned in the text?",
        "answer": "A knowledge nugget is a small item or piece of useful information that is often used in business trainers' videos."
    },
    {
        "question": "What is Kaggle and how has it evolved over time?",
        "answer": "Kaggle is a platform for machine learning competitions where companies like Google contribute big data sets and offer prizes. Originally started by enthusiasts, it has now been bought by Google and is increasingly sponsored by industry with prizes. The platform offers a no-setup customizable environment with access to free GPUs and a repository of community published data and code."
    },
    {
        "question": "What are some examples of data sets mentioned in the text?",
        "answer": "Indian coin denomination data, heart disease mortality data set, medical data sets, sports data sets, health data sets, software trending data, food data, caffeine content of drinks data set, travel data"
    },
    {
        "question": "What is the Waikato Environment for Knowledge Analysis (WEKA) known for?",
        "answer": "WEKA is known for being a hub for users to download software, access a textbook, tutorials, video tutorials, datasets, and other resources related to machine learning research."
    },
    {
        "question": "What is the International Computer Archive of Modern English (ICAME) specifically for?",
        "answer": "ICAME is specifically for corpus linguistics researchers on English, and in particular for English language teaching, teaching English as a second language or a foreign language."
    },
    {
        "question": "What is the CORPORA mailing list mentioned in the text used for?",
        "answer": "The CORPORA mailing list is used for subscribing to receive emails related to English corpus linguistics."
    },
    {
        "question": "What are some examples of industry research labs that present their research at ACL journals and conferences?",
        "answer": "Google, Microsoft, and other industry research labs present their research at ACL journals and conferences."
    },
    {
        "question": "What is the 13th Conference on Language Resources and Evaluation known as, and when is it happening?",
        "answer": "The 13th Conference on Language Resources and Evaluation is known as LREC, and it is happening in June 2022 in Marseilles."
    },
    {
        "question": "What is the ACL anthology mentioned in the text?",
        "answer": "The ACL anthology is a collection of research papers in the field of computational linguistics, including papers dating back to 1974."
    },
    {
        "question": "What is SemEval and what does it involve?",
        "answer": "SemEval stands for Semantic Evaluation. It involves papers in workshop proceedings, shared tasks which are essentially competitions with a data set and a particular task, and writing an academic paper describing methods, experiments, and results."
    },
    {
        "question": "What is the purpose of selecting the best papers in the competition mentioned in the text?",
        "answer": "The purpose is to select the best written papers with novel algorithms or methods, not necessarily the best results."
    },
    {
        "question": "What is one example of a task in the SemEval 2022 competition mentioned in the text?",
        "answer": "Comparing Dictionaries and Word Embeddings"
    },
    {
        "question": "What information can be found in the SemEval 2019 competition proceedings regarding the detection of offensive language?",
        "answer": "The SemEval 2019 competition proceedings contain papers describing how different contestants detected offensive language in the data set, including details on algorithms used and performance comparisons."
    },
    {
        "question": "What is the purpose of the European Union's Joint Research Center mentioned in the text?",
        "answer": "The purpose of the European Union's Joint Research Center is to fund groups of researchers to set up communities and foster EU collaboration, rather than solely for conducting research."
    },
    {
        "question": "What are some reasons mentioned in the text for joining social media communities related to data mining and text analytics?",
        "answer": "Some reasons mentioned in the text for joining social media communities related to data mining and text analytics include talking to each other, sharing knowledge, gathering knowledge, finding resources, getting ideas for research project proposals, and building on other people's work."
    },
    {
        "question": "What are some examples of organizations mentioned in the text that foster communities related to text analytics and AI?",
        "answer": "Some examples of organizations mentioned in the text that foster communities related to text analytics and AI are WEKA, ICAME, SemEval, ACL, European Union, and EPSRC."
    },
    {
        "question": "What is the last word in the text?",
        "answer": "END"
    },
    {
        "question": "Who is the lecturer for the data mining and text analytics module at the University of Leeds?",
        "answer": "Professor Eric Atwell"
    },
    {
        "question": "What are the topics covered in the first unit of the overall module mentioned in the text?",
        "answer": "The first unit covers an introduction to Corpus linguistics, text analytics, and sketch engine."
    },
    {
        "question": "What is the main focus of the current research interest at Leeds University mentioned in the text?",
        "answer": "The main focus of the current research interest at Leeds University is chat bot and text analytics for University education, such as teaching students."
    },
    {
        "question": "What is the purpose of using the Weka toolkit in the course mentioned in the text?",
        "answer": "The purpose of using the Weka toolkit in the course is to treat it as a black box and focus on how to use the machine learning algorithms to practical effect."
    },
    {
        "question": "What are the two tests mentioned in the text and how much of the total marks do they account for?",
        "answer": "The two tests mentioned are an online test covering units one and two, and a test covering all units from one to six. The first test is worth 20% of the marks, while the second test is worth 30%."
    },
    {
        "question": "What are some key components that EPSRC requires in a research proposal?",
        "answer": "EPSRC requires components such as research hypothesis and objectives, work plan, methodology, background information, contribution to knowledge, novelty of the research, and importance of the research in terms of value and impact."
    },
    {
        "question": "What is one requirement for the research project mentioned in the text?",
        "answer": "To include a work plan diagram, such as a Gantt chart, which shows the stages in the work and visualizes the program developed."
    },
    {
        "question": "According to the text, what is data wrangling?",
        "answer": "Data wrangling is about taking data from various different sources and getting it into the right format so that machine learning can work on it."
    },
    {
        "question": "According to the text, what is a big part of data mining?",
        "answer": "Getting hold of data, getting it into the right format, and understanding what is supposed to be"
    },
    {
        "question": "What are the steps involved in data mining as described in the text?",
        "answer": "The steps involved in data mining as described in the text include checking data quality issues, preparing the data, cleaning the data, running the data through machine learning tools, evaluating the results, and putting the results into practice."
    },
    {
        "question": "What term is often used in industry to refer to the application of data mining to text?",
        "answer": "Text analytics"
    },
    {
        "question": "According to the text, what is the key thing in text analytics?",
        "answer": "The key thing in text analytics is having a way of taking the text, converting the words and sentences into numbers, or vectors of numbers."
    },
    {
        "question": "What is the purpose of the British Council office attached to the embassies?",
        "answer": "The British Council promotes Great Britain, British English, English language teaching, and more."
    },
    {
        "question": "What are some of the key areas of study in linguistics mentioned in the text?",
        "answer": "Some of the key areas of study in linguistics mentioned in the text are phonetics, lexicography, syntax, and semantics."
    },
    {
        "question": "What is the focus of pragmatics and discourse modeling?",
        "answer": "The focus of pragmatics is the analysis of language in practical use, taking into account the context of use. Discourse modeling is about phenomena that range over more than one utterance in a discourse or dialogue."
    },
    {
        "question": "What are some reasons mentioned in the text for developing tech analytics?",
        "answer": "Some reasons mentioned in the text for developing tech analytics are analyzing social media, identifying fake news, wealth creation, and addressing challenges in text analytics."
    },
    {
        "question": "Why do people tend to not tolerate bad performance from text analytics?",
        "answer": "People tend to not tolerate bad performance from text analytics because they expect it to speak proper English and perform perfectly."
    },
    {
        "question": "What type of research projects does the European Union tend to fund?",
        "answer": "The European Union tends to fund research projects requiring several partners, not just a project for one person or one group."
    },
    {
        "question": "According to the text, what is one interesting thing in the language machine mentioned by the author?",
        "answer": "One interesting thing in the language machine is it has a snapshot of what's called the BT technology calendar."
    },
    {
        "question": "According to the text, why have domestic robots not become popular for tasks like cleaning and cooking?",
        "answer": "Domestic duties like cleaning a house or cooking a meal are too complicated for robots to learn and perform effectively."
    },
    {
        "question": "According to the text, what did the soldiers in Bosnia wear on their chests?",
        "answer": "The soldiers in Bosnia wore a small computer on their chests."
    },
    {
        "question": "What was AltaVista known for launching on the internet?",
        "answer": "AltaVista launched a free machine translation service on the internet."
    },
    {
        "question": "According to the text, why are you not supposed to listen to email messages or dictate replies while driving?",
        "answer": "You're not supposed to listen to email messages or dictate replies while driving because it distracts you from driving."
    },
    {
        "question": "What will the speaker provide later in the module?",
        "answer": "More details on what needs to be done to create good research proposals"
    },
    {
        "question": "What is one of the main challenges in text mining compared to data mining?",
        "answer": "One of the main challenges in text mining compared to data mining is the format of the data, as text is a sequence of characters that need to be processed or pre-processed to be understood by computers."
    },
    {
        "question": "What are regular expressions used for?",
        "answer": "Regular expressions are a formal language for specifying patterns of text strings."
    },
    {
        "question": "What does the carat or up arrow symbol mean in the context of the text?",
        "answer": "The carat or up arrow symbol means 'not'."
    },
    {
        "question": "What does the question mark symbol in regular expressions signify?",
        "answer": "The question mark means an optional previous character."
    },
    {
        "question": "What does the carat symbol (^) mean when it is inside a regular expression pattern?",
        "answer": "The carat symbol (^) inside a regular expression pattern means 'not'."
    },
    {
        "question": "What is the purpose of using the dot dollar notation in string processing in Python?",
        "answer": "The dot dollar notation is used to represent the last letter of a string."
    },
    {
        "question": "Why is theology ruled out in the text?",
        "answer": "The word 'theology' is ruled out because it contains the pattern 'the' with a letter after it, which does not match the specific criteria being discussed."
    },
    {
        "question": "What are false positives and false negatives in the context of classification tasks in data mining and machine learning?",
        "answer": "False positives and false negatives are two types of errors that occur in classification tasks. False positives refer to instances where something is incorrectly identified as belonging to a certain class, while false negatives refer to instances where something that belongs to a certain class is incorrectly not identified as such."
    },
    {
        "question": "What are some ways in which regular expressions can be used in conjunction with machine learning classifiers?",
        "answer": "Regular expressions can be used for pre-processing the data, capturing features for the classifiers, capturing generalizations, and substitutions."
    },
    {
        "question": "What does the regular expression open square brackets 0 to 9 close brackets plus represent?",
        "answer": "Any sequence of matching patterns where the pattern is a digit."
    },
    {
        "question": "What is an example of a sophisticated pattern for matching any sequence of characters other than a specific word mentioned in the text?",
        "answer": "A pattern that matches any sequence of characters other than the word 'volcano' by using A to Z, a to z, and specifying that 'volcano' should not be included in the match."
    },
    {
        "question": "According to the text, what is the main belief of a Rogerian psychotherapist?",
        "answer": "A Rogerian psychotherapist believes in talking to the patient and helping them draw out their own problems."
    },
    {
        "question": "What is a corpus in the context of text data?",
        "answer": "A corpus is a text data set that is typically split up into words and used for tasks like counting up how many words there are."
    },
    {
        "question": "What is the difference between word type and word tokens?",
        "answer": "Word type refers to the different forms or variations of a word, while word tokens refer to the total number of individual words in a text."
    },
    {
        "question": "How many tokens are there in the example given in the text?",
        "answer": "There are 15 tokens in the example given in the text."
    },
    {
        "question": "According to the text, what is the relationship between the size of the vocabulary and the number of word tokens?",
        "answer": "The text mentions that the vocabulary size grows as the number of word tokens grows, but the vocabulary size usually grows at a slower rate than the number of tokens."
    },
    {
        "question": "How many word tokens are there in the complete works of Shakespeare compared to the total number of word tokens in the text data set mentioned?",
        "answer": "The complete works of Shakespeare have slightly less than a million word tokens, while the text data set mentioned has 2 and 1/2 million word tokens."
    },
    {
        "question": "What is the difference between the works of Shakespeare and the LOB Corpus and the Brown Corpus mentioned in the text?",
        "answer": "The works of Shakespeare were very creative but limited to just Shakespeare, while the LOB Corpus and the Brown Corpus included writings from various sources, making them more varied."
    },
    {
        "question": "What is a four gram and how many different Ngrams appeared on the web?",
        "answer": "A four gram is a sequence of four words. There were a trillion different Ngrams that appeared on the web, but only 13 million different ones."
    },
    {
        "question": "What factors should be taken into account when considering the vocabulary likely to be found in a text?",
        "answer": "The genre or type of the text, the demographics of the author (such as age, gender, ethnicity), and metadata about the text."
    },
    {
        "question": "What is the purpose of tokenisation when collecting and analyzing data for a classifier?",
        "answer": "The purpose of tokenisation is to segment the text into words and normalise the format of the words, which may also involve segmenting it into sentences."
    },
    {
        "question": "What does the Unix command 'tr' do when used with the options 'sc, A to Z, a to z'?",
        "answer": "The 'tr' command with the specified options changes all the non-alpha characters to new lines."
    },
    {
        "question": "What is the suggested approach for merging uppercase and lowercase letters in a text?",
        "answer": "The suggested approach is to translate all uppercase letters into lowercase letters before sorting the text."
    },
    {
        "question": "Why is the word 'D' on its own in the word frequency list?",
        "answer": "The word 'D' is on its own in the word frequency list because the text counts contractions like 'I'd' as two separate words, 'I' and 'D'."
    },
    {
        "question": "What is the natural language toolkit mentioned in the text used for?",
        "answer": "The natural language toolkit (NLTK) is a collection of Python code used for doing natural language processing."
    },
    {
        "question": "What is the benefit of using regular expressions in Python for tokenization?",
        "answer": "The benefit of using regular expressions in Python for tokenization is that it correctly tokenizes text into words and patterns, allowing for more powerful and complex combinations of patterns to be matched."
    },
    {
        "question": "How does the text explain the challenge of translating Chinese characters into English words?",
        "answer": "The text explains that the challenge lies in the conceptual differences between Chinese and English, as Chinese characters may not directly correspond to English words."
    },
    {
        "question": "What method can be used for machine learning to learn segmentation of Japanese text into words?",
        "answer": "You can use a deep-learning neural sequence model."
    },
    {
        "question": "Why is it important to consider the case of words in text analytics?",
        "answer": "It is important to consider the case of words in text analytics because the case can change the meaning of a word or phrase. For example, 'General Motors' is the name of a company, while 'general motors' in lowercase could refer to any cars. Similarly, 'S-A-I-L' is a piece of software, while 'sail' in lowercase is a different word."
    },
    {
        "question": "What is the core meaning-bearing unit in a word?",
        "answer": "The stem"
    },
    {
        "question": "What is the purpose of stemming in morphological analysis?",
        "answer": "The purpose of stemming is to chop off affixes and leave only the stems of words."
    },
    {
        "question": "What is the purpose of the stemmer available in NLTK and Weka, as described in the text?",
        "answer": "The purpose of the stemmer is to apply rules for patterns which match certain endings in words, such as changing 'ational' to 'ate' or removing 'ing' at the end of a word."
    },
    {
        "question": "Why is determining sentence boundaries in a text complicated?",
        "answer": "Determining sentence boundaries in a text is complicated because a full stop might also mark an abbreviation or be part of a number, making it difficult to distinguish between the end of a sentence and other uses."
    },
    {
        "question": "What can help you identify numbers or percentages in a text?",
        "answer": "Having a dictionary of abbreviations and rules to look for numbers, or percentages, using patterns."
    },
    {
        "question": "According to the text, what is a word embedding in natural language processing?",
        "answer": "A word embedding is a term used to represent the word meaning for text analysis, typically a real valued vector that encodes the meaning of a word such that the uses are closer in vector space are similar in meaning."
    },
    {
        "question": "What are some ways of making word vectors more realistic and better?",
        "answer": "Using TFIDF (term frequency divided by document frequency) or pointwise mutual information as an alternative to raw frequency."
    },
    {
        "question": "According to the text, what did the famous linguist Barbara Partee ask about in the 1960s?",
        "answer": "Barbara Partee asked about the meaning of life."
    },
    {
        "question": "What is the desiderate in the study of lexical semantics?",
        "answer": "Desiderate is a Latin word meaning 'things that are desirable.'"
    },
    {
        "question": "What is the modern sense of the word 'mouse' as mentioned in the text?",
        "answer": "A hand-operated device that controls a cursor"
    },
    {
        "question": "According to the text, why may words that are in the dictionary as synonyms actually be slightly different?",
        "answer": "Words that are in the dictionary as synonyms may be actually slightly different in terms of politeness, slang, genre, topic, or specific meanings in different contexts."
    },
    {
        "question": "What is the difference between synonymy and similarity in terms of word meanings?",
        "answer": "Synonymy means words are exactly the same, while similarity means words share some element of meaning but are not identical."
    },
    {
        "question": "What is the difference between similarity and relatedness in terms of words?",
        "answer": "Similarity refers to words that are near synonyms, while relatedness or association refers to words that are connected in some way, even if they are not synonyms."
    },
    {
        "question": "According to the text, what is the main difference between synonyms and antonyms?",
        "answer": "The main difference between synonyms and antonyms is that antonyms are opposites with respect to only one feature of meaning, while synonyms are very similar in meaning except for minor differences."
    },
    {
        "question": "According to the text, what is the difference between the words 'copy' and 'fake' in terms of sentiment?",
        "answer": "The word 'copy' is considered positive, while 'fake' is considered negative in terms of sentiment."
    },
    {
        "question": "According to Jurafsky and Martin, what does every modern NLP algorithm use as a representation of word meanings?",
        "answer": "embeddings"
    },
    {
        "question": "According to the text, how did Wittgenstein define the meaning of a word?",
        "answer": "Wittgenstein defined the meaning of a word as its use in the language."
    },
    {
        "question": "According to Zelig Harris, when do we say that two words are synonyms?",
        "answer": "We say that two words are synonyms if they have almost identical environments."
    },
    {
        "question": "What does the author suggest you can do to find out more about 'ong choy'?",
        "answer": "The author suggests that you can look for other words which appear in similar contexts, or you could Google it to find a picture of ong choy."
    },
    {
        "question": "What are the three dimensions used to describe the sentiment value of a word?",
        "answer": "The three dimensions used to describe the sentiment value of a word are pleasantness or valence, arousal or intensity, and dominance or control."
    },
    {
        "question": "According to the text, what is the purpose of building a semantic space based on word meanings?",
        "answer": "The purpose is to have similar words, words with similar meanings, be near to each other in the semantic space."
    },
    {
        "question": "What does embedding mean in the context of the text?",
        "answer": "Embedding means taking a large dimensional space and mapping it out into a small dimensional space."
    },
    {
        "question": "What is the standard way to represent meanings in natural language processing?",
        "answer": "Embeddings"
    },
    {
        "question": "Why are vectors useful in sentiment analysis according to the text?",
        "answer": "Vectors are useful in sentiment analysis because they allow for generalization to similar but unseen words, meaning that even if a word like 'terrible' does not appear in the test set, a word with a very similar vector can still be identified as negative."
    },
    {
        "question": "What is the difference between using an unknown value and a similar vector when building vectors for word representations?",
        "answer": "Using an unknown value doesn't provide any information about the sentiment, whereas a similar vector could help in understanding the meaning of the word."
    },
    {
        "question": "According to the text, what analogy is used to explain the relationship between words and meaning?",
        "answer": "The analogy used is 'Net's off a fish, once you get the fish, you can forget the net.'"
    },
    {
        "question": "What are four plays mentioned in the text by the British academic?",
        "answer": "As You Like It, Twelfth Night, Julius Caesar, and Henry V"
    },
    {
        "question": "According to the text, which play has the most battles and the least fools?",
        "answer": "Twelfth Night has the most battles (0) and the least fools (58)."
    },
    {
        "question": "According to the text, which two words are similar in meaning based on their context vectors?",
        "answer": "Wit and fool"
    },
    {
        "question": "How is word similarity measured using vectors according to the text?",
        "answer": "Word similarity is measured using dot product and cosine similarity between the vectors representing the words."
    },
    {
        "question": "What does the dot product of two vectors represent?",
        "answer": "The dot product of two vectors represents the sum of the products of the corresponding entries of the two sequences of numbers."
    },
    {
        "question": "What is the purpose of normalizing the dot product in the context of the text?",
        "answer": "The purpose of normalizing the dot product is to divide the product by the lengths of the two vectors in order to account for vector length and obtain a value that ranges between 1 and -1, indicating the similarity between the words."
    },
    {
        "question": "What is the significance of having non-negative raw frequencies in the context of the cosine metric?",
        "answer": "Having non-negative raw frequencies ensures that the cosine metric will only have positive values, as the cosine metric is calculated using the dot product of two vectors divided by their magnitudes."
    },
    {
        "question": "What is the normalised value overall after calculating the length of the information vector?",
        "answer": "0.017"
    },
    {
        "question": "What is the issue with using raw frequencies as a metric for computing word similarity?",
        "answer": "Raw frequencies tend to over empower very frequent words and do not deal well with comparing low frequency with high frequency words."
    },
    {
        "question": "What is one way to balance the limitation of dealing only with the most frequent words in early experiments in embedding?",
        "answer": "One way to balance this limitation is by using TFIDF, which stands for term frequency multiplied by inverse document frequency."
    },
    {
        "question": "What is one method mentioned in the text to deal with 0 counts when counting how many times a word appears in a corpus?",
        "answer": "Adding one to the count (normalising or smoothing)"
    },
    {
        "question": "Why is it better to take logarithms when dealing with term frequency instead of just using the actual count?",
        "answer": "It is better to take logarithms because probabilities are very small numbers in the range of 0 to 1 or fractions, and multiplying together several fractions results in vanishingly small fractions."
    },
    {
        "question": "What does TFIDF stand for and how is it calculated?",
        "answer": "TFIDF stands for Term Frequency-Inverse Document Frequency. It is calculated by multiplying the term frequency (how many times the term appears in the overall corpus) by the inverse document frequency (or dividing by the document frequency)."
    },
    {
        "question": "What is the pointwise mutual information (PMI) between two words?",
        "answer": "The pointwise mutual information between two words is how many times do the words occur together more often than if they were independent."
    },
    {
        "question": "Why is the straightforward pointwise mutual information sometimes negative?",
        "answer": "The straightforward pointwise mutual information can be negative because the logarithm can be negative, and negative values are not preferred. If the calculated value is less than expected by chance, it is simply rounded down to 0 instead of a negative number."
    },
    {
        "question": "What is the probability of the word 'information' occurring out of all the words mentioned in the text?",
        "answer": "0.3399"
    },
    {
        "question": "What is the PPMI score for 'cherry pie' mentioned in the text?",
        "answer": "4.38"
    },
    {
        "question": "What is one solution mentioned in the text to address the bias towards infrequent events in PMI?",
        "answer": "One solution mentioned is to do add one smoothing, where for very infrequent words, 1 is added to the counts to give smooth probabilities."
    },
    {
        "question": "What is the author's suggestion for making word vectors shorter and easier to compare?",
        "answer": "The author suggests having word vectors with much shorter lengths, maybe 1,000 or even down to 50 values, where all values are non-zero and in the range of 0 to 1."
    },
    {
        "question": "Why are short, dense vectors preferred over long sparse vectors in machine learning?",
        "answer": "Short, dense vectors are preferred over long sparse vectors in machine learning because they can be easier to use as features, require a limited number of features, can generalize better, and are better at capturing synonyms."
    },
    {
        "question": "What are some of the methods mentioned in the text for obtaining short, dense vectors?",
        "answer": "Some of the methods mentioned in the text for obtaining short, dense vectors include neural or deep learning language models like word2vec or GloVe, singular value decomposition, and latent semantic analysis."
    },
    {
        "question": "What is the main difference between word2vec and GloVe in terms of providing word embeddings?",
        "answer": "Word2vec provides embeddings based on the meaning derived from the contexts a word appears in, while GloVe provides fixed embeddings for words."
    },
    {
        "question": "What is the purpose of the deep learning model mentioned in the text?",
        "answer": "The deep learning model tries to predict the best vector for a particular word by seeing how good it is at predicting whether or not a word will actually appear."
    },
    {
        "question": "What is one of the key differences in the approach described in the text compared to traditional methods of word representation?",
        "answer": "One key difference is that instead of counting how many times a word appears near another word, the approach involves training a classifier for binary prediction."
    },
    {
        "question": "What is the advantage of using the words in the text as labels for the target word in supervised learning?",
        "answer": "The advantage is that there is no need for a separate human to be paid to label the data, as the labels are already present in the text."
    },
    {
        "question": "What is the process described for generating positive and negative examples in the context of word embeddings?",
        "answer": "The process involves identifying a target word, such as 'cherry', and finding context words that appear with it as positive examples. Negative examples are generated by randomly sampling words from the lexicon that do not appear in the neighborhood of the target word."
    },
    {
        "question": "What is the goal of training a classifier in the context described in the text?",
        "answer": "The goal is to assign a positive probability for pairs like 'apricot and jam' and a negative probability for pairs like 'apricot and aardvark'."
    },
    {
        "question": "What is the model for word2vec based on?",
        "answer": "The model for word2vec is based on having a set of embeddings for all the target words in the vocabulary and the same set of embeddings for all the vocabulary words in the context."
    },
    {
        "question": "What is the goal of learning mentioned in the text?",
        "answer": "The goal of learning is to adjust the word vectors such that we maximise the similarity of a target word, content word pairs, and minimise the similarity of the negative pairs."
    },
    {
        "question": "What algorithm is mentioned in the text for changing the values of embeddings in a deep learning model?",
        "answer": "stochastic gradient descent"
    },
    {
        "question": "What is the process described in the text for learning word embeddings?",
        "answer": "The process described in the text for learning word embeddings involves starting off with random D dimensional vectors as initial embeddings, training a classifier based on similarity using positive and negative examples of word pairs that co-occur or don't co-occur, and adjusting the embeddings to improve classifier performance."
    },
    {
        "question": "What is the main advantage of using word2vec embeddings over other methods like TFIDF or pointwise mutual information?",
        "answer": "Word2vec embeddings provide small, dense vectors that are much more computable, efficient for computation, and give better results compared to the large, sparse vectors generated by other methods."
    },
    {
        "question": "What information did the computers in 1986 provide when looking at very small windows?",
        "answer": "The computers in 1986 could only look at the word before or after immediately, providing information that words like 'of', 'and', 'in', and 'to' are similar in meaning because they are all prepositions."
    },
    {
        "question": "What is the analogy presented in the text using the terms 'apple', 'tree', 'grape', and 'vine'?",
        "answer": "The analogy presented is: apple is to tree as grape is to vine."
    },
    {
        "question": "According to the text, what can be inferred by comparing the vectors for different cities and countries?",
        "answer": "By comparing the vectors for different cities and countries, it can be inferred that the capital of Paris is Paris, and the capital of Italy is Rome."
    },
    {
        "question": "How has the meaning of the word 'gay' changed over time according to the text?",
        "answer": "The meaning of the word 'gay' has changed from being associated with daunting, flaunting, or daft in the 1900s, to being associated with bright, witty, and frolicsome in the 1950s, and finally to being related to bisexual, homosexual, or lesbian at the end of the century."
    },
    {
        "question": "According to the text, what is an example of cultural bias in the training corpus for computer programmers?",
        "answer": "The equivalent for a woman in the training corpus is often a homemaker, not a computer programmer."
    },
    {
        "question": "Why is the Google Books corpus considered biased according to the text?",
        "answer": "The Google Books corpus is considered biased because it includes ethical bias issues due to the time period it covers and the lack of very modern corpora."
    },
    {
        "question": "What are two different ways mentioned in the text for normalizing frequencies?",
        "answer": "TFIDF and pointwise mutual information"
    },
    {
        "question": "What is the final word used in the text?",
        "answer": "END"
    },
    {
        "question": "What are the two main topics discussed in the lecture by Professor Eric Atwell?",
        "answer": "The two main topics discussed in the lecture by Professor Eric Atwell are part of speech tagging and named entity recognition."
    },
    {
        "question": "According to the text, what are the two broad classes of words mentioned?",
        "answer": "The two broad classes of words mentioned are the closed class and the open class."
    },
    {
        "question": "What are closed class words and how do they differ from open class words?",
        "answer": "Closed class words, also known as function words, are short, very frequent words with grammatical functions and little meaning attached to them. They include pronouns and prepositions. In contrast, open class words are content words that keep on coming up with new words."
    },
    {
        "question": "What are some examples of phatic communication words mentioned in the text?",
        "answer": "Examples of phatic communication words mentioned in the text include 'oh', 'ouch', and 'uh-huh'."
    },
    {
        "question": "What are some examples of closed class words mentioned in the text?",
        "answer": "Some examples of closed class words mentioned in the text are determiners, conjunctions, pronouns, prepositions, and particles."
    },
    {
        "question": "Why is part of speech tagging made more difficult than simply looking up each word in a dictionary?",
        "answer": "Part of speech tagging is made more difficult than simply looking up each word in a dictionary because words can have more than one part of speech, and the context in which the word is used determines its appropriate part of speech."
    },
    {
        "question": "What are the two main classes of words mentioned in the text and how are they categorized?",
        "answer": "The two main classes of words mentioned in the text are open classes and closed classes. Open classes include adjective, adverb, noun, verb, and proper noun. Closed classes include punctuation and other symbols like dollar signs or emojis."
    },
    {
        "question": "What is the purpose of allowing variations in the universal tag set for different languages?",
        "answer": "The purpose is to allow other tags to be added for particular languages or particular types of text."
    },
    {
        "question": "Why did Professor Greenbaum develop the International Corpus of English?",
        "answer": "He decided that the Brown and LOB tag sets weren't quite right and wanted to make a better tag set."
    },
    {
        "question": "Why is it important to identify nouns and verbs when working out the grammatical structure of a sentence?",
        "answer": "It is important to identify nouns and verbs in order to determine the structure of a sentence and to perform tasks like machine translation where word order may need to be adjusted for different languages."
    },
    {
        "question": "What are some examples given in the text to illustrate the different meanings of the word 'lead'?",
        "answer": "The examples given in the text are: buying a lead for a dog, leading the dog into the garden, and the roof having lead on it."
    },
    {
        "question": "What is the percentage of words that fairly sophisticated models can accurately tag in a piece of text?",
        "answer": "Around 97%"
    },
    {
        "question": "What accuracy level did the baseline model achieve?",
        "answer": "92%"
    },
    {
        "question": "What strategy does the speaker suggest for tagging words that are not in the dictionary?",
        "answer": "The speaker suggests that for unknown words not in the dictionary, it is best to tag them as nouns, as most new words are made up of nouns, which leads to a very high accuracy."
    },
    {
        "question": "According to the text, what can be inferred about a word like 'bill' when the word 'the' is before it?",
        "answer": "When the word 'the' is before 'bill', it's almost certainly a noun and not likely to be a verb."
    },
    {
        "question": "What is the basis of the constraint grammar mentioned in the text?",
        "answer": "The basis of the constraint grammar is ruling out certain possibilities based on the appearance of specific words, such as ruling out a word as a verb if the word 'the' appears before it."
    },
    {
        "question": "What models have been mentioned in the text for part-of-speech tagging?",
        "answer": "Maximum entropy Markov models, transformers, large deep-learning models like BERT, and Hidden Markov model"
    },
    {
        "question": "What is the move in current research areas regarding training data sets for part-of-speech tagging and named entity recognition?",
        "answer": "The move in current research areas is to use deep learning or neural network models to learn the models themselves and self-improve in order to automatically spot mistakes and correct them to produce a fully 100% training data set."
    },
    {
        "question": "What are the two steps involved in tagging proper names in a text?",
        "answer": "The two steps involved in tagging proper names in a text are identifying which words are the entity and determining the type of this entity."
    },
    {
        "question": "What is the significance of the New York Street in Leeds mentioned in the text?",
        "answer": "The New York Street in Leeds is actually not related to New York City, but rather to the city of York. It serves as a street connecting Leeds to York, with a new street built next to it called the new York Street."
    },
    {
        "question": "What is one way to address the segmentation problem in part-of-speech tagging?",
        "answer": "One way to address the segmentation problem in part-of-speech tagging is to use BIO tagging, which includes tags like person, organization, location, and specifies whether a word is at the beginning or inside a particular category."
    },
    {
        "question": "What are the three types of tags mentioned in the text for labeling tokens?",
        "answer": "Person, organisation, location"
    },
    {
        "question": "What are some of the variants on the BIO tagging mentioned in the text?",
        "answer": "Some of the variants on the BIO tagging mentioned in the text are having extra tags for begin, inside, and other; having only inside and other tags (I tags and O tags); or having begin, inside, and end tags (B-I-O tags)."
    },
    {
        "question": "Why is it helpful to label each word with its part of speech and each entity with its entity category before performing information extraction or machine translation?",
        "answer": "It helps in adding additional features that can be used later on in machine learning applications like information extraction."
    },
    {
        "question": "What classifier does Professor Eric Atwell mention in the text for text classification?",
        "answer": "Naive Bayes classifier"
    },
    {
        "question": "What are some measures used for evaluating sentiment classification besides accuracy?",
        "answer": "Precision, recall, and F measure"
    },
    {
        "question": "Who were the three politicians mentioned in the text who wrote letters or papers to persuade people to join the United States?",
        "answer": "J. Madison and Hamilton"
    },
    {
        "question": "What is the University of Leeds 2 of 16 medical subject hierarchy used for in medical research?",
        "answer": "The University of Leeds 2 of 16 medical subject hierarchy is used to classify articles according to the MeSH Subject Category Hierarchy in medical research."
    },
    {
        "question": "What is the purpose of sentiment analysis mentioned in the text?",
        "answer": "The purpose of sentiment analysis is to count up positive and negative words in order to determine if a review or text is positive or negative."
    },
    {
        "question": "What is sentiment analysis primarily focused on in computational terms?",
        "answer": "Sentiment analysis is primarily focused on detecting the attitudes of people when they write things."
    },
    {
        "question": "What are some examples of applications for text classification mentioned in the text?",
        "answer": "Some examples of applications for text classification mentioned in the text are sentiment analysis, spam analysis, identifying different languages, and determining the topic of a document."
    },
    {
        "question": "What is the purpose of supervised machine learning in the context of spam detection?",
        "answer": "The purpose of supervised machine learning in the context of spam detection is to train a classifier that can determine whether a given document is spam or not spam based on a training set of hand-labeled documents."
    },
    {
        "question": "What is the 'bag of words' assumption in the Naive Bayes classifier?",
        "answer": "The 'bag of words' assumption in the Naive Bayes classifier treats a document as a set of words without considering the order of the words. It focuses on counting how many times each word occurs in the document."
    },
    {
        "question": "What is the bag of words representation and how is it used in predicting classes?",
        "answer": "The bag of words representation is a list of words in a document along with their frequency of occurrence. It is used to predict classes by determining the probability of a class given a document based on the frequency of words in the document."
    },
    {
        "question": "What is the goal when applying the Bayes equation to a document classification problem?",
        "answer": "The goal is to find the class that maximizes the probability of the document given the class times the probability of the class."
    },
    {
        "question": "What is one of the challenges in estimating the likelihood of a word for a particular class?",
        "answer": "One of the challenges is that you have to have a very large number of training examples, as most words are quite rare even in a large corpus."
    },
    {
        "question": "What assumption does the Naive Bayes classifier make about the probability of each word given a class?",
        "answer": "The Naive Bayes classifier assumes that the probability of each word given a class is independent of the probability of other words, treating them as a 'bag of words'."
    },
    {
        "question": "Why do we use logarithms of probabilities in N-gram modeling instead of multiplying probabilities directly?",
        "answer": "We use logarithms of probabilities to avoid floating point underflow, as multiplying lots of small probabilities can result in very small numbers. Additionally, adding logarithms is faster than multiplying in computer terms and helps in ranking classes based on scores."
    },
    {
        "question": "Why is Naive Bayes called a linear classifier?",
        "answer": "Naive Bayes is called a linear classifier because it is a linear function of the inputs, and it aims to maximize the sum of weights."
    },
    {
        "question": "What is the method described for counting the frequency of a word in documents of a particular topic or sentiment?",
        "answer": "The method described involves creating a mega document by concatenating all documents in the topic or sentiment, and then counting the frequency of the word in this mega document."
    },
    {
        "question": "Why is having a probability of 0 a problem in Naive Bayes modeling?",
        "answer": "Having a probability of 0 is a problem in Naive Bayes modeling because when multiplying together all the probabilities, if one of those probabilities is 0, then the product of all of them will be 0, leading to the overall probability of the text becoming 0."
    },
    {
        "question": "How is the probability of a class calculated in the text?",
        "answer": "The probability of a class is calculated by counting how many documents have that class and dividing it by the total number of documents."
    },
    {
        "question": "What is the formula mentioned for calculating the probability of a word in the given text?",
        "answer": "The formula is (count of the word + 1) / (total number of words + 1 times vocabulary size)."
    },
    {
        "question": "What is one difference between classifiers and N-gram modeling mentioned in the text?",
        "answer": "One difference mentioned is that in Naive Bayes classifiers, unknown words can be ignored, while in N-gram modeling, all words are typically used."
    },
    {
        "question": "How many negative examples are there in the small training corpus mentioned in the text?",
        "answer": "There are three negative examples in the small training corpus."
    },
    {
        "question": "What is the probability of the negative class in the training set based on the given data?",
        "answer": "3 out of 5"
    },
    {
        "question": "How many words are there in the negative set mentioned in the text?",
        "answer": "There are 14 words in the negative set."
    },
    {
        "question": "What is the probability of a sentence being predictable given that it's positive?",
        "answer": "1 divided by 29"
    },
    {
        "question": "What method is mentioned in the text for tasks like sentiment analysis?",
        "answer": "Binary multinominal Naive Bayes or binary NB"
    },
    {
        "question": "What approach is used for binary classification in the given text?",
        "answer": "The approach used for binary classification in the given text is to count whether a word is present or not, rather than calculating all the frequencies of the words."
    },
    {
        "question": "What is the purpose of predocument binarization in the text?",
        "answer": "The purpose of predocument binarization is to get rid of all the duplicates in the text, such as repeated words like 'was' and 'the', in order to simplify the documents and do the counts all over again."
    },
    {
        "question": "Why is the word 'great' considered a good indicator of positive documents in sentiment analysis?",
        "answer": "The word 'great' is considered a good indicator of positive documents because it appears in multiple positive documents, showing its significance across several documents."
    },
    {
        "question": "According to the text, what is the purpose of negation in a sentence?",
        "answer": "The purpose of negation in a sentence is to reverse the meaning of the word or phrase that follows it."
    },
    {
        "question": "What are some examples of prebuilt lexicons mentioned in the text?",
        "answer": "Some examples of prebuilt lexicons mentioned in the text are the MPQA Subjectivity Cues Lexicon and the General Inquirer."
    },
    {
        "question": "What are some limitations of using the method described for sentiment analysis?",
        "answer": "The method is limited to the words in the lexicon and ignores other words that may also affect sentiment in some way."
    },
    {
        "question": "What are some examples of features that can be good indicators of spam according to the text?",
        "answer": "Mentions of millions of dollars, From address starting with numbers, subject in all capitals, and other specific indicators mentioned in the text."
    },
    {
        "question": "What are some advantages of using Naive Bayes over deep learning and other machine learning methods?",
        "answer": "Some advantages of using Naive Bayes over deep learning and other machine learning methods include its speed, low storage requirements, effectiveness with small amounts of training data, and robustness to irrelevant features."
    },
    {
        "question": "What is the significance of Naive Bayes in text classification according to the text?",
        "answer": "Naive Bayes is considered a good and dependable baseline for text classification, meaning it is something that should be tried first before exploring other options."
    },
    {
        "question": "What are some examples of features that Naive Bayes can look at besides words when classifying text?",
        "answer": "Naive Bayes can also look at features such as email address, URL, and other characteristics besides words when classifying text."
    },
    {
        "question": "What is the difference between the isnad and the matn in the context of a hadith?",
        "answer": "The isnad refers to the chain of narrators where the hadith came from, while the matn refers to the text of the hadith itself."
    },
    {
        "question": "According to the text, how is the probability of a whole sentence calculated using a unigram language model?",
        "answer": "The probability of a whole sentence is calculated by multiplying together the probabilities of each of the words given the class."
    },
    {
        "question": "What are some other metrics mentioned in the text besides accuracy, and when might they be more useful?",
        "answer": "Precision, recall, and F measure are other metrics mentioned in the text. They might be more useful in certain circumstances where one class is very small or skewed, and there's a lot of the other class."
    },
    {
        "question": "What is the main challenge with using accuracy as a metric for detecting offensive tweets?",
        "answer": "The main challenge with using accuracy as a metric for detecting offensive tweets is that if there is a very large number of gold negatives, the accuracy will be dominated by true negatives, which would be a very large number."
    },
    {
        "question": "What is the difference between precision and recall in the context of evaluating predictions?",
        "answer": "Precision is the true positives divided by the sum of true positives and false positives, while recall is the true positives divided by the sum of true positives and false negatives."
    },
    {
        "question": "What is the main drawback of using a 0R classifier in this context?",
        "answer": "The main drawback of using a 0R classifier in this context is that it always predicts the most popular class, resulting in high accuracy but failing to identify the specific class of interest (in this case, articles about pie)."
    },
    {
        "question": "What is the F1 score formula mentioned in the text?",
        "answer": "2 times the precision times the recall divided by the precision plus the recall"
    },
    {
        "question": "What is one way to avoid overfitting when training a model?",
        "answer": "One way to avoid overfitting is to take out a portion of the training set and use it as a development test set to fine-tune the model before testing it on a separate test set."
    },
    {
        "question": "What is cross validation and how can it be used in training and testing?",
        "answer": "Cross validation is a way of combining all of a data set for training and testing. It can be used to get a development set and a training set, and then the best model can be used as the overall model for testing on a test set."
    },
    {
        "question": "What is the advantage of having a separate development set in addition to a training model and a test set?",
        "answer": "Having a separate development set allows for further training and evaluation of the model, especially in cases where there are more than two classes, enabling precision and recall calculations for each class separately."
    },
    {
        "question": "What is the difference between micro averaging and macro averaging in terms of computing precision and recall?",
        "answer": "Micro averaging involves collecting decisions for all classes and then computing precision and recall for the overall table, while macro averaging computes precision and recall for the entire dataset without considering individual classes separately."
    },
    {
        "question": "What is a problem with Naive Bayes classifiers mentioned in the text?",
        "answer": "A problem with Naive Bayes classifiers is that they will reproduce whatever is in the training set, including any implicit bias present in the data."
    },
    {
        "question": "What are some of the challenges mentioned in the text regarding censorship and detection of toxicity in discussions about certain groups?",
        "answer": "Some challenges mentioned include the censorship of discussions about gay people due to hate speech, the filtering of sex education information at schools, biases in training data for machine learning systems, and issues with human labels and resources like lexicons."
    },
    {
        "question": "What is the concept of model cards mentioned in the text?",
        "answer": "Model cards are documents that accompany the release of an algorithm and data set, providing information about the training data sources, development process, motivation, preprocessing, evaluation, intended use and users, demographic/environmental group considerations, and other relevant details."
    },
    {
        "question": "What are some metrics that are better than accuracy when evaluating text classifiers, especially in the case of skewed data?",
        "answer": "Precision and recall are better metrics than accuracy, and the F measure, which is a combination of precision and recall into one measure."
    },
    {
        "question": "What does the author hope for regarding the future lectures in this module?",
        "answer": "The author hopes that the future lectures will be a bit shorter and less taxing."
    },
    {
        "question": "According to the text, what is the CHEAT system mentioned by Professor Eric Atwell used for?",
        "answer": "The CHEAT system is used for a text analytic challenge, specifically the Morpho Challenge."
    },
    {
        "question": "What are some features of the Weka toolkit mentioned in the text?",
        "answer": "Some features of the Weka toolkit mentioned in the text include a wide range of classifiers, clustering algorithms, association algorithms, visualization tools, data analysis tools, and a graphical user interface with pull-down menus."
    },
    {
        "question": "According to the text, what is the advice given regarding programming for computer scientists?",
        "answer": "The advice given is that you shouldn't program unless you actually have to, and you should look for existing tools or libraries to solve your problem before writing new code."
    },
    {
        "question": "What is NLTK and what does it provide an easy-to-use interface to?",
        "answer": "NLTK is the leading platform for building Python programs to work with human language data. It provides an easy to use interface to over 50 corpora and lexical resources, such as WordNet."
    },
    {
        "question": "What resources are available on the NLTK discussion forum?",
        "answer": "The NLTK discussion forum offers a hands-on guide, an online guide with API documentation, a textbook on natural language processing using Python, and various tools for text analysis and testing classifiers."
    },
    {
        "question": "What module is being imported in the provided Python code snippet?",
        "answer": "NLTK.corpus"
    },
    {
        "question": "Who will join the board as a non-executive director on November 29th?",
        "answer": "Peter Vinkin"
    },
    {
        "question": "According to the text, what is NLTK particularly good for?",
        "answer": "NLTK is particularly good for learning and university education, according to the text."
    },
    {
        "question": "Why might you consider using Weka or Sketch Engine instead of doing programming?",
        "answer": "You might consider using Weka or Sketch Engine instead of doing programming if you're trying to build some experiment using known techniques and not doing something novel."
    },
    {
        "question": "What is the Morpho Challenge about?",
        "answer": "The Morpho Challenge is about unsupervised machine learning segmentation of words into morphemes."
    },
    {
        "question": "What is the task described in the text and what does it involve?",
        "answer": "The task described in the text involves coming up with an algorithm for identifying morphemes in text, particularly in difficult languages like Finnish and Turkish. The algorithm needs to work unsupervised, meaning it has to analyze a list of words and identify repeated patterns to determine likely morphemes."
    },
    {
        "question": "What is the purpose of the unsupervised machine learning mentioned in the text?",
        "answer": "The purpose of the unsupervised machine learning is for segmentation of words into morphemes."
    },
    {
        "question": "What is the purpose of the CHEAT ensemble classifier mentioned in the text?",
        "answer": "The purpose of the CHEAT ensemble classifier is to combine the results from several different Leeds student entries into a combined system."
    },
    {
        "question": "What was the reason for recording and putting the video of the lecture on the internet back in 2005?",
        "answer": "The reason for recording and putting the video of the lecture on the internet back in 2005 was because a company called YouTube had started and they invited people to upload their own short videos."
    },
    {
        "question": "What technology was used to record the lecture in 2005?",
        "answer": "The lecture was recorded on very early technology, with a green background and crackly sound."
    },
    {
        "question": "What is the title of the talk mentioned in the text?",
        "answer": "Combinatory Hybrid Elementary Analysis of Text, The CHEAT Approach to Morpho Challenge 2005"
    },
    {
        "question": "What inspired the concept of 'triple layer super-sized unsupervised learning' mentioned in the text?",
        "answer": "The concept was inspired by the recent film Super-Size Me."
    },
    {
        "question": "What was the purpose of the Python program cheat.py mentioned in the text?",
        "answer": "The purpose of the Python program cheat.py was to read outputs of other systems line by line, select the majority vote analysis result, and output the final result."
    },
    {
        "question": "What is the CHEAT program mentioned in the text?",
        "answer": "The CHEAT program is actually a committee of unsupervised learners."
    },
    {
        "question": "Who does the author thank in the text?",
        "answer": "The author thanks the Morpho challenge organizers and the audience."
    }
]